<!DOCTYPE html>
<html lang="ko"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Sangmin Blog | GPT-3 â†’ Word2Vec : ë‹¨ì–´ ì˜ë¯¸ í‘œí˜„ ì—¬ì •</title>
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css">
  <link rel="stylesheet" href="../../assets/styles/main.css">
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <link rel="stylesheet"
     href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

</head><body>
  <main class="article">
    <h1>GPT-3 â†’ Word2Vec : ë‹¨ì–´ ì˜ë¯¸ í‘œí˜„ ì—¬ì •</h1>
    <div class="meta">May 18, 2025 Â· 12 min read</div>
    <img class="hero" src="../../images/word2vec_intro/thumbnail.png" alt="cover image">
    <h2>ğŸš€ Intro</h2>
<blockquote>
<p><strong>GPT-3 â€” A first step on the path to universal models</strong><br>â€œì–¸ì–´ ëª¨ë¸ì€ ê²°êµ­ <em>ì˜ë¯¸</em>ë¥¼ ì–´ë–»ê²Œ ë‹´ì•„ë‚¼ê¹Œ?â€</p>
</blockquote>
<hr>
<h2>1ï¸âƒ£ ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ëŠ” ê³ ì „ì  ë°©ì‹</h2>
<h3>â€¢ Webster ì‚¬ì „ ì •ì˜ <em>(ë°œì·Œ)</em></h3>
<ul>
<li>the idea that is represented by a word  </li>
<li>â€¦ <strong>signifier â†” signified</strong> (denotational semantics)</li>
</ul>
<h3>â€¢ NLP ì „í†µ : <strong>ë‹¨ì–´ = ë…ë¦½ ê¸°í˜¸</strong></h3>
<table>
<thead>
<tr>
<th>ë‹¨ì–´</th>
<th>one-hot ë²¡í„° ì˜ˆì‹œ</th>
</tr>
</thead>
<tbody><tr>
<td>motel</td>
<td><code>[0 â€¦ 0 1 0 â€¦]</code></td>
</tr>
<tr>
<td>hotel</td>
<td><code>[0 â€¦ 1 0 0 â€¦]</code></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>ë¬¸ì œ</strong>  </p>
<ol>
<li>ë²¡í„° ì°¨ì› = ì–´íœ˜ í¬ê¸° â†’ ğŸ’¥ ìˆ˜ì‹­ë§Œ ì°¨ì›  </li>
<li>one-hot ì€ í•­ìƒ ì§êµ â†’ <em>ìœ ì‚¬ë„ 0</em><br>  â†’ â€œ<em>Seattle motel</em>â€ ê²€ìƒ‰ ì‹œ <em>hotel</em> ë¬¸ì„œë¥¼ ë†“ì¹œë‹¤</li>
</ol>
</blockquote>
<hr>
<h2>2ï¸âƒ£ ğŸŒ Distributional Semantics</h2>
<blockquote>
<p>â€œ<strong>You shall know a word by the company it keeps</strong>â€</p>
</blockquote>
<ul>
<li><strong>Context window</strong> <em>m</em> : ì¤‘ì‹¬ ë‹¨ì–´ <code>w_t</code> ì£¼ë³€ Â± <em>m</em> í† í°</li>
<li>ë‹¨ì–´ <code>w</code> ì¶œí˜„ âœ ì´ì›ƒ í† í° ì§‘í•©ì´ <strong>ì˜ë¯¸ ë²¡í„°</strong>ë¥¼ ë§Œë“ ë‹¤</li>
</ul>
<pre><code class="language-text">â€¦ government debt problems turning into  banking  crises â€¦
                                 â–²â”€â”€ context
</code></pre>
<hr>
<h2>3ï¸âƒ£ Word Embedding ê°œë…</h2>
<ul>
<li><strong>Dense ì‹¤ìˆ˜ ë²¡í„°</strong> <code>v_w âˆˆ â„â¿</code></li>
<li>â€œë¹„ìŠ·í•œ ë¬¸ë§¥ â†” ë¹„ìŠ·í•œ ë²¡í„°â€ (ë‚´ì  â†‘, cos Î¸ â†‘)</li>
</ul>
<p>ì˜ˆ)  </p>
<pre><code>banking = [ 0.286, 0.792, âˆ’0.177, â€¦ ]áµ€
</code></pre>
<blockquote>
<p><strong>Synonyms</strong> â‰ˆ ê°€ê¹Œìš´ ë²¡í„°<br><strong>king âˆ’ man + woman â†’</strong> <em>queen</em> ê°™ì€ ë²¡í„° ì—°ì‚°ë„ ê°€ëŠ¥</p>
</blockquote>
<hr>
<h2>4ï¸âƒ£ Word2Vec : í•™ìŠµ ì•„ì´ë””ì–´</h2>
<ol>
<li>ë§ë­‰ì¹˜ ì „ì²´ <strong>í† í° ì‹œí€€ìŠ¤</strong> <code>wâ‚â€¦w_T</code></li>
<li>ê° ë‹¨ì–´ì— <strong>ì¤‘ì‹¬ ë²¡í„°</strong> <code>v_c</code> / <strong>ì™¸ë¶€ ë²¡í„°</strong> <code>u_o</code> ìŒ ë°°ì •</li>
<li>ìœ„ì¹˜ <code>t</code> ì—ì„œ ì¤‘ì‹¬ë‹¨ì–´ <code>c=w_t</code><br>â†’ <em>context</em> <code>o âˆˆ window</code> ì˜ <strong>ì˜ˆì¸¡ í™•ë¥ </strong> ìµœëŒ€í™”</li>
</ol>
<h3>4-1. Softmax í™•ë¥ </h3>
<p>[
P(o\mid c)=
\frac{\exp(u_o^{!\top}v_c)}
     {\sum_{w\in V}\exp(u_w^{!\top}v_c)}
]</p>
<h3>4-2. ì „ì²´ ëª©í‘œ (Skip-gram)</h3>
<figure class="eq">

<p>[
\mathcal L(\theta)=
-\frac1T\sum_{t=1}^{T}
      \sum_{\substack{-m\le j\le m\ j\ne0}}
      \log P(w_{t+j}\mid w_t;\theta)
]</p>
</figure>

<p>(ê²½ì‚¬ í•˜ê°• â†’ <code>v, u</code> ì—…ë°ì´íŠ¸)</p>
<hr>
<h2>ğŸ’» Word2Vec (Skip-gram + NegSampling) ì´ˆê°„ë‹¨ ì½”ë“œ</h2>
<pre><code class="language-python">import torch
import torch.nn as nn

class SkipGramNS(nn.Module):
    def __init__(self, vocab_size: int, dim: int):
        super().__init__()
        self.in_embed  = nn.Embedding(vocab_size, dim)   # v_c
        self.out_embed = nn.Embedding(vocab_size, dim)   # u_o

    def forward(self, center, pos, neg):
        v_c   = self.in_embed(center)            # [B, D]
        u_pos = self.out_embed(pos)              # [B, K, D]
        u_neg = self.out_embed(neg)              # [B, Kneg, D]

        # positive &amp; negative loss
        pos_score = (u_pos @ v_c.unsqueeze(2)).squeeze()                         .sigmoid().log()
        neg_score = (u_neg @ (-v_c).unsqueeze(2)).squeeze()                         .sigmoid().log()

        loss = -(pos_score.sum() + neg_score.sum()) / center.size(0)
        return loss
</code></pre>
<hr>
<h2>ğŸ“ í•µì‹¬ ìš”ì•½</h2>
<table>
<thead>
<tr>
<th>Key Idea</th>
<th>í•œ ì¤„ ì„¤ëª…</th>
</tr>
</thead>
<tbody><tr>
<td><strong>One-hot í•œê³„</strong></td>
<td>ì°¨ì›â†‘â†‘ &amp; ìœ ì‚¬ë„ 0</td>
</tr>
<tr>
<td><strong>Distributional Hypothesis</strong></td>
<td>â€œê°™ì€ ë¬¸ë§¥ â†’ ë¹„ìŠ·í•œ ì˜ë¯¸â€</td>
</tr>
<tr>
<td><strong>Word2Vec í•™ìŠµ</strong></td>
<td>ì¤‘ì‹¬-ë¬¸ë§¥ ìŒ í™•ë¥  â†‘ â†’ ë²¡í„°ê°€ ì˜ë¯¸ ê³µê°„ í˜•ì„±</td>
</tr>
<tr>
<td><strong>Embedding íš¨ìš©</strong></td>
<td>ê²€ìƒ‰Â·ì¶”ì²œÂ·ìœ ì¶” <em>king âˆ’ man + woman â‰ˆ queen</em></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>ê²°êµ­</strong>: <em>í…ìŠ¤íŠ¸ ìì²´</em>ê°€ ë‹¨ì–´ ì˜ë¯¸ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤ â†’ GPT-ì‹œë¦¬ì¦ˆ ê°™ì€ ê±°ëŒ€ LMì˜ í† ëŒ€ ğŸš€</p>
</blockquote>

     
  </main>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
<script>
window.MathJax = {
  tex: {
    inlineMath:  [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },

  chtml: {
    linebreaks: { automatic: true, width: "match" }  // ë˜ëŠ” width: 80
  },

  options: {
    renderActions: { addMenu: [] }   // ìš°í´ë¦­ ë©”ë‰´ ì œê±°
  }
};
</script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script src="../../assets/js/main.js" defer></script>
</body></html>