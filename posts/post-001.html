<!DOCTYPE html><html lang="ko"><head>
       <meta charset="utf-8">
       <meta name="viewport" content="width=device-width,initial-scale=1">
       <title>Diffusion Model : DDPM ì •ë¦¬</title>

       <!-- Pretendard Font -->
       <link rel="stylesheet"
             href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css">
       <!-- ë©”ì¸ CSS -->
       <link rel="stylesheet" href="../assets/styles/main.css">
       <!-- ì½”ë“œ í•˜ì´ë¼ì´íŠ¸ -->
       <link rel="stylesheet"
             href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
     </head><body>
       <main class="article">
         <h1>Diffusion Model : DDPM ì •ë¦¬</h1>
         <div class="meta">Feb 17, 2023 Â· 20 min read</div>

         <img class="hero" src="../images/latentdiffusion.webp" alt="cover image">

         <p>Genetraionì€ í¬ê²Œ ë‘ê°€ì§€ frameworkìœ¼ë¡œ ë‚˜ë‰œë‹¤.</p>
<ul>
<li><strong>Likelihood-based</strong><ul>
<li>autoregressive models</li>
<li>variational autoencoders</li>
<li>flow-based models</li>
<li><code>diffusion models</code></li>
</ul>
</li>
<li><strong>Implicit model</strong><ul>
<li>generative adversarial networks(GAN)</li>
</ul>
</li>
</ul>
<p>likelihood based ì¸ diffusion modelì— ê´€í•´ ì•Œì•„ë³´ì</p>
<p><img src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3e5087f5-76a0-438f-bc01-8e3253104282%2FB948109D-1EBD-4BF6-B739-FB8EF24AE95E.png?table=block&id=1163df0e-ec45-81a6-be3d-d2267660b541&spaceId=14f1eeea-15e5-42e2-b9d5-486040ff5c3d&width=2000&userId=df45ccee-de8f-4276-80c1-4933eb8b1e4d&cache=v2" alt="B948109D-1EBD-4BF6-B739-FB8EF24AE95E.png"></p>
<hr>
<h2>Requirements</h2>
<p>Diffusionì— ê´€í•´ ì„¤ëª…í•˜ê¸° ì „ì— ì•Œê³ ìˆì–´ì•¼í•  ì‚¬ì „ì§€ì‹ë“¤ì„ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬í•´ë³´ê² ë‹¤.</p>
<h3>KL-Divergence</h3>
<p><code>ë‘ í™•ë¥  ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€</code>ë¥¼ ê³„ì‚°(ì •ë³´ ì—”íŠ¸ë¡œí”¼ ì°¨ì´ë¥¼ ê³„ì‚°)</p>
<p>$$
D_{KL}(q||p) = \begin{cases} - \sum_i q_ilog\frac{p_i}{q_i} 
&amp;\text{(discrete form)}\
-\int q(x)log \frac{p(x)}{q(x)} &amp;\text{(continuous form)}
\end{cases}
$$</p>
<p>ì´ê³  ì‹ì„ ì „ê°œí•˜ë©´ self-entropy termê³¼ cross entropy(or average of negative log-likelihood in p of samples from q)ìœ¼ë¡œ ë¶„ë¦¬í•  ìˆ˜ ìˆë‹¤.</p>
<p>$$
D_{KL} = -H(q) + H(q,p)
$$</p>
<p>ë§Œì•½ ë¶„í¬ê°€ continuous í•œ ê²½ìš°</p>
<p>$$
\int q(x) log(q(x))dx + \int q(x)(-log(p(x))dx\ = -H(q) + H(q,p)
$$</p>
<p>ë¡œ ì“¸ ìˆ˜ ìˆë‹¤.</p>
<p>ì˜ˆë¥¼ë“¤ì–´, ìš°ë¦¬ê°€ që¥¼ pë¡œ ê·¼ì‚¬ì‹œí‚¤ê¸° ìœ„í•´, KL divergenceë¥¼ minimizeí•˜ë©´ self entropy termì€ qì˜ varianceë¥¼ ì¦ê°€ì‹œí‚¤ì¼œ ë„“ê²Œ í¼ì§„ ë¶„í¬ê°€ ë˜ë ¤ëŠ” ê²½í–¥ì„ ê°–ê²Œí•˜ê³  cross entropy termì€ p ë¶„í¬ì—ì„œ likelihoodê°€ ê°€ì¥ ë†’ì€ ì§€ì ì—ì„œ Dirac -delta functionì²˜ëŸ¼ ë˜ê²Œí•˜ë ¤ëŠ” ê²½í–¥ì„ ê°–ê²Œ í•  ê²ƒì´ë‹¤. </p>
<p>ì´ ë‘ termì„ í†µí•´(ì‹¸ìš°ëŠ” ëŠë‚Œ?) qê°€ pë¡œ ê·¼ì‚¬ê°€ ëœë‹¤.</p>
<p>KL divergenceì˜ íŠ¹ì„±</p>
<p>í•­ìƒ 0 ì´ìƒì´ë‹¤. CEëŠ” ì•„ë¬´ë¦¬ ë‚®ì•„ì ¸ë´¤ì ì¦‰ qì™€ pê°€ ê°™ì€ ë¶„í¬ê°€ ëœë‹¤ í–ˆì„ ë•Œ self-entropyì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìµœì†Ÿê°’ì´ 0ì´ê³  , ì ˆëŒ€ ìŒìˆ˜ê°€ ë  ìˆ˜ ì—†ë‹¤. </p>
<p>ê±°ë¦¬ ê°œë…ì´ ì•„ë‹ˆë‹¤. </p>
<p>ì¼ë°˜ì ìœ¼ë¡œ $D_{KL}(p|q) \neq D_{KL}(q|p)$ì´ë‹¤.</p>
<h3>Bayes Rule</h3>
<p>$$
P(H|E) = \frac {P(H) P(E|H)}{P(E)}
$$</p>
<p>Straight forwardí•˜ë¯€ë¡œ ì‹ì„¤ëª…ì€ ìƒëµí•˜ê³ , ë…¼ë¬¸ì„ ì½ë‹¤ë³´ë©´ í™•ë¥  ìš©ì–´ê°€ ë§ì´ ë‚˜ì™€ í—·ê°ˆë ¸ëŠ”ë° ìš©ì–´ë§Œ ê°„ë‹¨íˆ ì •ë¦¬í•˜ê² ë‹¤.</p>
<p>E : Evidence(<del>sample x), H : Hypothesis(</del>latent z)</p>
<ul>
<li>P(H) : Prior Probability ( ì‚¬ì „ì— ì•Œê³  ìˆëŠ” Hê°€ ë°œìƒí•  í™•ë¥  )</li>
<li>P(E|H) : <strong>Likelihood</strong> of the evidence E if the Hypothesis H is true ( ëª¨ë“  ì‚¬ê±´ Hì— ëŒ€í•œ Eê°€ ë°œìƒí•  likelihood ) â‡’ How well H explains E !</li>
<li>P(E) : Priori probability that the evidence E itself is true ( Eì˜ ì‚¬ì „í™•ë¥ ,ì¦‰ Eê°€ ë°œìƒí•  í™•ë¥ , marginalì´ë¼ê³ ë„ í•¨ )</li>
<li>P(H|E) : <strong>Posterior Probability</strong> of â€˜Hâ€™ given the evidence</li>
</ul>
<h3>Monte Carlo Method</h3>
<p>ëœë¤ í‘œë³¸ì„ ë½‘ì•„(<code>sampling</code>ì„ í†µí•´) í•¨ìˆ˜ê°’ì„ í™•ë¥ ì ìœ¼ë¡œ ê³„ì‚°í•˜ê² ë‹¤ ì´ê³  ê²°êµ­ ê·¼ì‚¬(<code>approximation</code>) ì‹œí‚¤ê² ë‹¤ëŠ” ì´ì•¼ê¸°ë‹¤.</p>
<p>ì˜ˆë¥¼ë“¤ë©´</p>
<p>$$
\int p(x)f(x)dx = E_{x \sim p(x)}[f(x)] \approx \frac 1 K \sum_i^K f(x_i), x_i \sim p(x)
$$</p>
<p>í™•ë¥  ë°€ë„í•¨ìˆ˜ p(x)ë¥¼ ë”°ë¥´ëŠ” xì—ëŒ€í•œ f(x)ì˜ ê¸°ëŒ“ê°’ì„ êµ¬í•˜ê³  ì‹¶ë‹¤í–ˆì„ ë•Œ p(x)ì—ì„œ Kê°œì˜ ìƒ˜í”Œì„ ë½‘ì•„ ì´ë¡œ ê³„ì‚°í•´ë„ ê´œì°®ë‹¤ëŠ” ì´ì•¼ê¸°</p>
<h3>ELBO : Evidence Lower Bound</h3>
<p>ê²°êµ­ <code>Variational Inference</code>ëŠ” ì‚¬í›„í™•ë¥  ë¶„í¬(posterior) <code>p(z|x)</code>ë¥¼ ë‹¤ë£¨ê¸° ì‰¬ìš´ í™•ë¥ ë¶„í¬ <code>q(z)ë¡œ ê·¼ì‚¬</code>í•˜ê³  ì‹¶ì€ ì˜ì§€ì´ë‹¤.</p>
<p>$q^*(z)  = argmin_{q(z) \in Q} D_{KL}(q(z)||p(z|x))$</p>
<p><img src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F98c4b35f-863d-4745-8c79-8c9a250b4771%2FUntitled.png?table=block&id=1163df0e-ec45-8154-83a1-d09295997c34&spaceId=14f1eeea-15e5-42e2-b9d5-486040ff5c3d&width=2000&userId=df45ccee-de8f-4276-80c1-4933eb8b1e4d&cache=v2" alt="Untitled"></p>
<p><img src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F352292aa-6636-477f-8882-03efc1290c91%2FUntitled.png?table=block&id=1163df0e-ec45-810c-b21f-c7cac87b18e3&spaceId=14f1eeea-15e5-42e2-b9d5-486040ff5c3d&width=2000&userId=df45ccee-de8f-4276-80c1-4933eb8b1e4d&cache=v2" alt="Untitled"></p>
<p>ê·¸ëŸ¬ë¯€ë¡œ KL divergenceë¥¼ ì´ìš©í•´ ì´ë¥¼ ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´</p>
<p>$$
D_{KL} (q(z) || p(z|x)) = \int q(z) \log \frac{q(z)}{p(z|x)}dz\ \text{by Bayes Rule} \= \int q(z)\log\frac{q(z)p(x)}{p(x|z)p(z)}dz \= \int q(z)\log\frac{q(z)}{p(z)}dz +\int q(z)\log p(x)dz -\int q(z)\log p(x|z)dz \ = D_{KL}(q(z)||p(z)) + \log p(x) - E_{z \sim q(z)} [ \log p(x|z)]
$$</p>
<p>ë¡œ ì •ë¦¬ëœë‹¤. </p>
<p>bayes ruleì— ì˜í•´ ìš°ë¦¬ëŠ” ì‚¬í›„í™•ë¥  p(z|x)ë¥¼ p(z), p(x), p(x|z)ë¡œ ê°€ì ¸ê°”ë‹¤. </p>
<p>ì—¬ê¸°ì„œ latent variable z ì˜ì‚¬ì „í™•ë¥  ë¶„í¬ p(z)ëŠ” xì™€ ë¬´ê´€í•´ any kind of distributionì´ì—¬ë„ ê´œì°®ìœ¼ë¯€ë¡œ ê°€ì¥ ê°„ë‹¨í•˜ê³  ì˜ˆìœ Gaussianì´ë¼ í•˜ì. ê·¸ë¦¬ê³  që¥¼ $\phi$, pë¥¼ $\theta$ë¡œ parameterizeí•˜ì. </p>
<p>ê·¸ëŸ°ë° ì—¬ê¸°ì„œ $\log p_{\theta}(x)$ëŠ” intractableí•˜ë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” tractableí•œ lower bound(ELBO)ë¥¼ ì¡ê³  ì´ë¥¼ maximizeí•˜ëŠ” ë°©ì‹ì„ ì·¨í•œë‹¤.</p>
<p>Expectation : $D_{KL} (q_{\phi}(z|x) || p(z|x))$ë¥¼ minimizeí•˜ëŠ” $\phi$ë¥¼ ì°¾ì</p>
<p>Maximization :  $\phi$ë¥¼ ê³ ì •í•˜ê³  $\log p_{\theta}(x)$ì˜ lower boundë¥¼ maximizeí•˜ëŠ” $\theta$ë¥¼ ì°¾ì</p>
<p><a href="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4e8e6c6e-5d0f-4c41-a457-ad6fa203a2a1%2FUntitled.png?table=block&id=1163df0e-ec45-812f-9a80-c4068024117e&spaceId=14f1eeea-15e5-42e2-b9d5-486040ff5c3d&width=2000&userId=df45ccee-de8f-4276-80c1-4933eb8b1e4d&cache=v2">Expectation-Maximization</a></p>
<aside>
ğŸ’¡ **log p(x) ë¥¼ evidenceë¼ê³  í•œë‹¤.**

<p> $\theta$ë¡œ parameterizedëœ ìš°ë¦¬ì˜ modelì´ observed data xì— ëŒ€í•´ marginal probabilityë¥¼ ê³„ì‚°í–ˆì„ ë•Œ ë§Œì•½ ìš°ë¦¬ ëª¨ë¸ì´ ì˜ í•™ìŠµì´ ë˜ì—ˆë‹¤ë©´ ë†’ì€ ê°’ì„ ë‚´ë†“ì„ ê²ƒì´ë‹¤. ì¦‰, í•™ìŠµ ì¤‘ì— $\theta$ë¥¼ ì ì‹œ fixí•´ë†“ê³  evaluationì„ í–ˆì„ ë•Œ ë†’ì€ ê°’ì„ ë‚´ë†“ê³  ìˆë‹¤ë©´ ìš°ë¦¬ëŠ” ì˜ ê°€ê³  ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ $logp(x;\theta)$ë¥¼ ìš°ë¦¬ê°€ ì˜ ê°€ê³  ìˆë‹¤ëŠ” ì˜ë¯¸ì—ì„œ  evidenceë¼ í•œë‹¤. </p>
</aside>

<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4e8e6c6e-5d0f-4c41-a457-ad6fa203a2a1/Untitled.png" alt="Untitled"></p>
<p>$D_{KL} (q_{\phi}(z|x) || p(z|x))\ge 0$ ì´ë¯€ë¡œ </p>
<p>$$
\log p_{\theta}(x) \ge E_{z \sim q(z)}[\log p_{\theta}(x|z)] - D_{KL}(q_{\phi}(z|x)||p(z)) 
$$</p>
<p>ê°€ ëœë‹¤. </p>
<p><strong>ELBO</strong> </p>
<p>$$
â
$$</p>
<p>1st term</p>
<p>$E_{z \sim q(z|x)}[\log p_{\theta}(x|z)]$</p>
<p><code>Reconstruction Error</code> â†’ generative model( Decoder in VAE )</p>
<p>2nd term</p>
<p>$D_{KL}(q(z|x)||p(z)) \text{ or } E_{q(z|x)} [\log \frac {q(z|x) }{p(z)}]$</p>
<p><code>Regularization term</code> â†’ inference model( Encoder in VAE )</p>
<h3>SDE : Stochastic Differential Equation</h3>
<p>TODO</p>
<hr>
<h1><strong>Denoising Diffusion Probabilistic Models</strong></h1>
<h2>DDPM</h2>
<p><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></p>
<ul>
<li>parameterized Markov chain</li>
<li>trained using variational inference</li>
<li><code>learn to reverse a diffusion process</code></li>
</ul>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1b107a88-b5c8-4fea-80df-50b6b903fdc2/Untitled.png" alt="Untitled"></p>
<p>Diffusion modelì€ latent variable ëª¨ë¸ì´ë‹¤.</p>
<p><strong>latent : a hidden continuous feature space</strong></p>
<p><strong>GOAL</strong></p>
<p>$$
p_{\theta} (x_0) = \int p_{\theta}(x_{0:T})dx_{1:T}
$$</p>
<p>$x_1,..x_T$ ëŠ” latentë“¤ì´ê³  data $x_0 \sim q(x_0)$ì™€ ê°™ì€ dimensionì„ ê°–ëŠ”ë‹¤.</p>
<blockquote>
<p>dataì— <code>noise</code>ë¥¼ ë”í•´ê°€ëŠ” ê²ƒì„ <code>forward process</code>
noiseë¡œ ë¶€í„° <code>de-noise</code>í•´ë‚˜ê°€ëŠ” <code>reverse process</code>ë¼í•œë‹¤.</p>
</blockquote>
<p>ìš°ë¦¬ modelì€ ì´ reverse processë¥¼ í•™ìŠµí•˜ê³  ìƒˆë¡œìš´ dataë¥¼ generationí•œë‹¤.</p>
<h3>Forward Process (diffusion process)</h3>
<p><code>forward processëŠ” Gaussian noiseë¥¼ ë”í•´ê°€ëŠ” ê³¼ì •</code></p>
<p>ì´ëŠ” Markov chainìœ¼ë¡œ formulateí•  ìˆ˜ ìˆë‹¤. ì¦‰, </p>
<p>$$
q(x_{1:T}|x_0) = \Pi_{t=1}^Tq(x_t|x_{t-1}) 
$$</p>
<p>by Markov chain with step T</p>
<p>Markov chainì€ ê° stepì€ ì˜¤ì§ ì§ì „ stepì—ë§Œ ì˜ì¡´í•¨ì„ ì˜ë¯¸í•œë‹¤.</p>
<p>$q(x_{1:T})$ëŠ” që¥¼ timestep 1ë¶€í„° Tê¹Œì§€ ë°˜ë³µí•´ì„œ ê°€í•¨ì„ ì˜ë¯¸í•˜ëŠ” notation</p>
<p>$\text {where}$</p>
<p>$$
â 
$$</p>
<p>ê° ìŠ¤í…ì—ì„œ Gaussian noiseë¥¼ ë”í•œë‹¤</p>
<p>ì—¬ê¸°ì„œ $\beta_t$ëŠ” variance scheduleì´ê³  Iê°€ identityì´ë¯€ë¡œ ê° dimensionì€ ê°™ì€ stdë¥¼ ê°–ëŠ”ë‹¤. ìƒìˆ˜ë¡œ ë‘¬ë„ ë˜ê³ , ì‹œê°„ì— ë”°ë¥¸ ë³€ìˆ˜ë¡œ ë‘ì–´ë„ ëœë‹¤. ë…¼ë¬¸ì—ì„  higer të¡œ ê°ˆìˆ˜ë¡ ì»¤ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ linearí•˜ê²Œ ë‘ì—ˆëŠ”ë° ë‹¤ë¥¸ ë…¼ë¬¸ì—ì„  cosine shcedule ì´ ì˜ ëë‹¤ê³  í•œë‹¤.</p>
<p>$\beta_t$ë¥¼ ì´ìš©í•´ scalingí•œ í›„ ë”í•´ì£¼ëŠ” ì´ìœ ëŠ” varianceê°€ divergeí•˜ëŠ” ê²ƒì„ ë§‰ê¸°ìœ„í•¨ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. </p>
<aside>
ğŸ’¡ Gaussian noiseë¥¼ ë”í•´ê°€ë©´, ìµœì¢… step (time T)ì—ì„œëŠ”standard normal prior $N(x_T;0,I)$ ê°€ë˜ê³  ê·¸ë˜ì„œ diffusionì´ë¼ í•œ ê²ƒê°™ë‹¤.

</aside>

<p>ê·¸ëŸ°ë° ì—¬ê¸°ì„œ ì–´ë–¤ ìˆœê°„ t ( $0 \le t \le T)$ì—ì„œ $x_t$ë¥¼ ì•Œê³  ì‹¶ë‹¤ê³ í•œë‹¤ë©´, ìœ„ì˜ ì‹ì„ ì´ìš©í•´ ë°˜ë³µì ì¸ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ë©´ ëœë‹¤. ê·¸ëŸ¬ë‚˜ t ê°€ í¬ë‹¤ë©´ ì´ëŠ” ì¢‹ì€ ë°©ë²•ì´ ì•„ë‹ ê²ƒì´ë‹¤.</p>
<p><strong>Reparameterization trick (for sampling</strong> $x_t$ <strong>at once)</strong></p>
<p>ë§Œì•½ ìš°ë¦¬ê°€</p>
<p>$\alpha_t = 1-\beta_t , \bar \alpha_t = \Pi_{s=0}^t \alpha_s$ ë¼ê³  ì¡ëŠ”ë‹¤ë©´, tì—ì„œ $x_t$ë¥¼ samplingí•˜ëŠ” ê²ƒì„ closed formìœ¼ë¡œ ì“¸ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>
<p>â­â­â­</p>
<p>$$
q(x_t|x_0) = N(x_t;\sqrt {\bar \alpha_t} x_0, (1- {\bar \alpha_t})I)
$$</p>
<p>sample = mean + (var**0.5)*epsilon</p>
<p>â­â­â­</p>
<ul>
<li><p><strong>Proof )</strong></p>
<p>  let $\epsilon_0, \cdots \epsilon_{t-2},\epsilon_{t-1} \sim N(0,I)$</p>
<p>  $x_ t = \sqrt{(1-\beta_t)}x_{t-1} + \sqrt {\beta_t} \epsilon_{t-1} \ =\cdots  \=\sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon_0$</p>
<p>  ì¦ëª…ì˜ í•µì‹¬ë…¼ë¦¬ëŠ” ë‹¤ë¥¸ varianceë¥¼ ê°–ëŠ” ë‘ê°œ($\sigma_1^2,\sigma_2^2$)ì˜ Gaussiansì„ mergeí•´ ìƒˆë¡œìš´ distribution(with variance $\sigma_1^2+\sigma_2^2$ )ì„ ë§Œë“œëŠ” ê²ƒ</p>
</li>
</ul>
<p>ì¢‹ë‹¤. </p>
<p>í™•ì¸ì„ í•´ë³´ë©´ t â†’ $\infty$ë¡œ ê°ˆë•Œ $q(x_t|x_0)$ê°€ $N(x_t;0,I)$ë¡œ ê°ë„ ë³¼ ìˆ˜ ìˆë‹¤</p>
<p>ì´ì œ ìš°ë¦¬ëŠ” any timestep tì—ì„œ noiseë¥¼ samplingí•  ìˆ˜ ìˆê²Œëê³ , ì´ë¥¼ í†µí•´ $x_t$ë¥¼ $x_0$ì™€ $\epsilon$ì˜ í•¨ìˆ˜ë¡œ ë³¼ìˆ˜ ìˆê²Œ ë˜ì–´ forward processì—ì„œ $x_0$ë§Œ ì•Œë©´ ë°”ë¡œ  <strong>$x_t$ë¥¼ ì–»ì„ ìˆ˜ ìˆê²Œëë‹¤.</strong></p>
<p>$$
x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon_0
$$</p>
<p>or</p>
<p>$$
x_0 = \frac{1}{\sqrt{\bar \alpha_t}}(x_t - \sqrt{1-\bar \alpha_t}\epsilon)
$$</p>
<h3>Reverse Process (denoising process)</h3>
<p><code>reverse processëŠ” neural networkì´ í•™ìŠµí•  ê³¼ì •</code></p>
<p>$q(x_{t-1}|x_{t})$ë¥¼ ì›í•˜ë‚˜ ì–´ë ¤ì›Œì„œ neural networkë¥¼ ì´ìš©</p>
<p>forward processì˜ Gaussian noiseê°€ ì¶©ë¶„íˆì‘ì„ ë•Œ reverse process ë˜í•œ Gaussianì´ ë˜ê³  ì´ëŠ” neural networkë¥¼ ì´ìš©í•´ ê·¼ì‚¬ì‹œì¼œ meanê³¼ varianceë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.</p>
<p>$$
p_{\theta}(x_{t-1}|x_t) = N(x_{t-1};\mu_{\theta}(x_t,t), \Sigma_{\theta}(x_t,t))
$$</p>
<p>Reverse processëŠ” $p(x_T) = N(x_T;0,I)$ë¶€í„° ì¶œë°œí•´ (learned) Gaussian trainsitionì„ í•˜ëŠ” Markov chainì´ë‹¤. ì¦‰, trajectoryëŠ”</p>
<p>$$
â
$$</p>
<p>ë¡œ fomulateí•  ìˆ˜ ìˆë‹¤.</p>
<aside>
ğŸ’¡ neural networkì— timestep të¥¼ conditioningí•˜ë©´ modelì€ ê° time stepì˜ Gaussianì˜ meanê³¼ varianceë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œëœë‹¤.

</aside>

<h3>Training</h3>
<p><code>ELBO</code>on the negative log likelihoodë¥¼ optimize</p>
<p>$$
E[-\log p_{\theta}(x_0)] \le E_q[-\log \frac {p_\theta (x_{0:T})}{q(x_{1:T}|x_0)}] \ = L
$$</p>
<p>ì´ê³  ìœ„ ì‹ì„ ì •ë¦¬í•˜ë©´</p>
<p>$$
L = E_q[D_{KL}(q(x_T|x_0)||p(x_T))+\sum_{t&gt;1}D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\log p_{\theta}(x_0|x_1)]
$$</p>
<p>ì´ ëœë‹¤.</p>
<aside>
ğŸ’¡ $q(x_{t-1}|x_t)$ëŠ” intractableí•˜ì§€ë§Œ $x_0$ì˜ conditioningì„ ì£¼ë©´ tractableí•˜ë‹¤ê³  í•¨ â†’ generative modelì´ reverse diffusion stepìœ¼ë¡œ generationì„ í•˜ê¸° ìœ„í•´ì„  reference image $x_0$ê°€ í•„ìš”í•˜ë‹¤

</aside>

<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d411bb47-dcf7-4b5a-afb1-89eb1e580dcc/Untitled.png" alt="Untitled"></p>
<blockquote>
<p>ë™í•˜ ì£¼)</p>
<ul>
<li><p>ìœ„ ì‹ ì¦ëª… ê³¼ì •</p>
<p>  $q(x_{t-1}|x_t)$ëŠ” ë¬´ì‹œê°€ëŠ¥í•´ì§. (Markov Processì´ê¸° ë•Œë¬¸)</p>
<p>  <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c052df61-f676-421a-b570-5f8733d6d4d0/Untitled.png" alt="Untitled"></p>
</li>
</ul>
</blockquote>
<p>ê²°êµ­ í•˜ê³  ì‹¶ì€ ê²ƒì€ <code>$p_{\theta}(x_{t-1}|x_t)$ì™€ forward process posteriors $q(x_{t-1}|x_t,x_0)$ë¥¼ ë¹„êµ</code>í•˜ëŠ” ê²ƒì´ê³ </p>
<p>$L_T = D_{KL}(q(x_T|x_0)||p(x_T))$ = const</p>
<p>$X_T$ê°€ ì–¼ë§ˆë‚˜ standard Gaussianì¸ì§€, ê·¸ëŸ°ë° ìš°ë¦¬ëŠ” $\beta_t$ë¥¼ ì‹œê°„ì— ë”°ë¥¸ constant ë¡œ ë‘ì—ˆìœ¼ë¯€ë¡œ ì´ termë„ constantì´ê³  í•™ìŠµí•  ë•Œ <code>ë¬´ì‹œ</code>í•´ë„ ëœë‹¤</p>
<p>$L_{T-1} = \sum_{t&gt;1}D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))$</p>
<p>denoising step $p_{\theta}(x_{t-1}|x_t)$ ê³¼ approximated denoising step $q(x_{t-1}|x_t,x_0)$ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•¨ì„ ë³¼ ìˆ˜ ìˆìŒ</p>
<p>modelì´ <code>noiseë¥¼ ì˜ˆì¸¡</code>í•˜ë„ë¡ <code>Reparam</code></p>
<ol>
<li>$\Sigma_{\theta}(x_t,t) = \sigma_t^2I$, $\sigma$ëŠ” $\beta$ì— ê´€í•œ time dependent constants</li>
<li>$\mu_\theta(x_t,t)$ for $p_{\theta}(x_{t-1}|x_t)$ using  $x_t(x_0,\epsilon)$</li>
</ol>
<p>â­â­â­</p>
<p>$$
p_\theta(x_{t-1}|x_t) = N(x_{t-1};\mu_{\theta}(x_t,t),\sigma_t^2I)
$$</p>
<p>$$
\mu_{\theta}(x_t,t) = \frac 1 {\sqrt {\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar \alpha_t}}\epsilon_{\theta}(x_t,t))
$$</p>
<p><strong>sample</strong> $x_{t-1}$ = $\frac 1 {\sqrt {\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar \alpha_t}}\epsilon_{\theta}(x_t,t))$ + $\sigma_t$<strong>z</strong>,      <strong>z</strong> $\sim N(0,I)$</p>
<p>â­â­â­</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b1b82858-adcf-4e8f-8ee0-b4fb748bd636/Untitled.png" alt="Untitled"></p>
<p>â‡’<code>$\epsilon_{\theta}$ ëŠ” $x_t$ì™€ të¥¼ ë°›ê³  noiseë¥¼ ì˜ˆì¸¡</code>í•œë‹¤.</p>
<p>$x_t$ëŠ” $x_0$ë¡œ ë¶€í„° samplingì´ ê°€ëŠ¥í•˜ë„ë¡ ìœ„ì—ì„œ reparamí–ˆë‹¤.</p>
<p>$L_0 = -\log p_{\theta}(x_0|x_1)$</p>
<p><code>Reconstruction term</code></p>
<p>â­â­â­</p>
<p><strong>Simplified Loss</strong></p>
<p>$$
L_{simple}(\theta) = \mathbb E_{t,x_0,\epsilon} [||\epsilon -\epsilon_{\theta}(\sqrt{\bar \alpha_t}x_0  + \sqrt{1-\bar\alpha_t}\epsilon,t) ||^2]
$$</p>
<p>â­â­â­</p>
<p>t = 1 ì¼ ë•Œ</p>
<p>$L_0$ ì¦‰, $-\log p_{\theta}(x_0|x_1)$ë¥¼ minimize</p>
<p>t&gt; 1ì¼ ë•Œ</p>
<p>$L_{t-1}$ì—ì„œ ì‹ ì •ë¦¬í•˜ë©´ ë‚˜ì˜¤ëŠ” coefficient $\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar\alpha_t)}$ë¥¼ ë²„ë ¤ì„œ higher noise level(higer t)ì— ì „(coefficient ê°€ ìˆì„ ë•Œ)ë³´ë‹¤ ë” í° weightë¥¼ ì£¼ê³  small tì— ëŒ€í•´ì„  ë” ì‘ì€ weightë¥¼ ì¤˜ì„œ ë” ì¢‹ì€ sample qualityë¥¼ ì–»ì—ˆë‹¤. (small tì—ì„  modelì´ ì‘ì€ ì–‘ì˜ noiseë§Œ denoiseí•˜ë„ë¡ í•™ìŠµì„ ì‹œí‚¤ê¸° ë•Œë¬¸, ê·¸ë˜ì„œ ë” ì–´ë ¤ìš´ large tì— ì§‘ì¤‘í•˜ë„ë¡ ë§Œë“¦)</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9f379455-4bf7-4088-a3fa-49fa0bda4539/Untitled.png" alt="Untitled"></p>
<aside>
ğŸ’¡ randomí•˜ê²Œ timesteps të¥¼ ë½‘ê³ , $x_0$ì™€ të¥¼ ì´ìš©í•´ $q(x_t|x_0)$ ë¡œë¶€í„° $x_t$ë¥¼ êµ¬í•¨
ì´ $x_t$ì™€ të¥¼ ìš°ë¦¬ ëª¨ë¸ì— ë„£ê³  epsilonì„ ë½‘ìŒ
ì´ epsilonê³¼ ($x_0$ì™€ ì •í™•íˆ ê°™ì€ dimensionì„ ê°–ëŠ”) noiseë¥¼ ë½‘ê³  MSE loss ë•Œë¦¬ë©´ ë¨

</aside>

<h3>Model Architectue</h3>
<p>modelì˜ inputê³¼ outputì˜ dimensionì´ ê°™ì•„ì•¼í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„  U-Netì„ ì‚¬ìš©í–ˆë‹¤. U-Netì€ Residual Block, self-attention blockì´ ìˆë‹¤.</p>
<p>diffusionì˜ timestep tê°€ position embeddingì„ í•œ í›„residual blockì— ì „ë‹¬ë˜ëŠ” ì‹ìœ¼ë¡œ ëª¨ë¸ì— tê°€ ì…ë ¥ëœë‹¤.</p>
<p><strong>U-Net</strong></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/22fdc1ea-d205-47fe-9d1f-a9715d5f4c05/Untitled.png" alt="Untitled"></p>
<p>$\epsilon_{\theta}$ <strong>model using U-Net</strong></p>
<p>input <code>$(x_t, t)$</code></p>
<p>output <code>noise</code></p>
<p>ì œê±°í•´ì•¼í•  noise</p>
<p><strong>Implementation Code</strong></p>
<pre><code class="language-python">TODO
Skeleton code ì‘ì„±í•´ë³´ê¸°
</code></pre>

       </main>

       <!-- Highlight.js -->
       <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
       <script>hljs.highlightAll();</script>

       <!-- MathJax (ìë™ ì¤„ë°”ê¿ˆ) -->
       <script>
         window.MathJax = {
           tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
           options: { linebreaks: { automatic: true, width: 'container' } }
         };
       </script>
       <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
     </body></html>