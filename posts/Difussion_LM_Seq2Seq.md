---
title: "Difussion : Difussion LM & Seq2Seq"
date: 2023-02-26
readingTime: 20 
thumbnail: /images/Difussion_LM_Seq2Seq/thumb.png
tags: [Generative AI, Diffusion, DDPM]
category : [Tech Review]
---

# Diffusion LM Improves Controllable Text Generation

## Diffusion LM

[Diffusion-LM Improves Controllable Text Generation](https://arxiv.org/abs/2205.14217)

https://github.com/xiangli1999/diffusion-lm

- non-autoregressive language model based on continuous diffusions
- embedding from discrete space to continuous space
- continuous diffusion model to discrete text
- Transformer based

`Gaussian noise ë²¡í„°ë“¤ì˜ sequenceì—ì„œ ì‹œì‘`í•´ (inference) denoisingì„í•´ì„œ wrodsì— correspondingí•˜ëŠ” vectorsë“¤ì„ ì–»ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤.

*a sequence of Gaussian noise vecotrs -- `denoise` â€”> vectors corresponding to words*

ì´ëŸ° ì‹ìœ¼ë¡œ ì§„í–‰í•˜ë©´ ì¤‘ê°„ì— hierarchyê°€ ìˆëŠ” contiuous latent variableë“¤ì´ ìƒê¸´ë‹¤. ë•ë¶„ì— ìš°ë¦¬ê°€ í•˜ë“¯ gradient ë¥¼ ë•Œë ¤ì„œ ìš°ë¦¬ê°€ í•˜ë˜ ëŒ€ë¡œ í•˜ë©´ ëœë‹¤.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/269add55-95f8-4bed-9ca2-e838de76ebb4/Untitled.png)

diffusion ëª¨ë¸ì´ textì— ì ìš©í•˜ê¸° í˜ë“ ì´ìœ ëŠ” diffusion modelì€ continuous domainì—ì„œ ë§ì€ ë°œì „ì„ ì´ë¤„ì™”ëŠ”ë° ë¹„í•´ textê°€ ìì²´ì ìœ¼ë¡œ dicreteí•œ íŠ¹ì„±ì„ ê°–ê³  ìˆê¸° ë•Œë¬¸. ê·¸ë˜ì„œ ì´ëŸ° standard diffusionì— textë¥¼ ì ìš©ì‹œí‚¤ê¸° ìœ„í•´ discreteí•œ textë¥¼ continuousí•œ domainìœ¼ë¡œ ì˜ mappingì„ í•´ì•¼í•œë‹¤. NLPì— word to vectorê°€ ë– ì˜¤ë¥¸ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ìœ„í•´ 2ê°€ì§€ stepì„ ì¶”ê°€ì ìœ¼ë¡œ ì ìš©í•œë‹¤.

1. **`embedding step`** : ê°™ì´ í•™ìŠµì‹œì¼œë²„ë¦¼
2. **`rounding step`** with softmax

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e320eedb-ae89-41f4-964c-36be65ca8e6c/Untitled.png)

**Diffusion Models for Text** ëŠ” í¬ê²Œ ë‘ê°€ì§€ê°€ ìˆë‹¤ ë³¼ ìˆ˜ ìˆë‹¤.

1. text diffusion models on discrete state spaces
    
    discrete data(ê° í† í°)ì— corruptionì„ ì£¼ëŠ” ì‹ìœ¼ë¡œ forward process ì§„í–‰
    
2. `continuousí•œ latent` variablesì„ ë½‘ì•„ì„œ ì§„í–‰

ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ atuoregressiveí•˜ë„ë¡ generationì„ í•˜ëŠ” transformer ê°™ì€ ë°©ì‹ì˜  language ëª¨ë¸ë“¤ì´ ë§ë‹¤.

$p_{lm}(w) = p_{lm}(w_1)\Pi_{i=2}^np_{lm}(x_i|x_{<i})$ â‡’ ì§€ê¸ˆê¹Œì§€ ë§Œë“¤ì–´ë‚¸ tokenë“¤ì˜ sequenceë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ tokenì„ ì˜ˆì¸¡í•˜ê³  ì´ë¥¼ eosê¹Œì§€ ë°˜ë³µí•œë‹¤. ì´ë ‡ê²Œ generationì˜ ìˆœì„œë¥¼ ê³ ì •ì‹œí‚¤ëŠ” ê²ƒì€ ëª¨ë¸ì„ ì»¨íŠ¸ë¡¤í•˜ê¸°ê°€ ì–´ë µë‹¤. 

 Plug and Play Controllable Generation ì€ ëª¨ë¸ ìì²´ëŠ” frozenì‹œí‚¤ê³  ì™¸ë¶€ì— classifierë¥¼ ë”°ë¡œë‘ëŠ” ë°©ì‹ì´ë¼ ë³´ë©´ëœë‹¤.

Text Generationì€ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ëª¨ë¸ì´ discrete words $w_i$ë“¤ì˜ sequence ***w***ë¥¼ ë½‘ì•„ë‚´ëŠ” ê²ƒ $p_{lm}(w)$ ì´ê³  ì´ê²ƒì´ controllableí•˜ë‹¤ëŠ” ê²ƒì€ ê²°êµ­ conditional distribution $p(w|c)$ ì„ ì˜ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì´ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„  bayes ruleë¡œ ì‹ì„ ë‹¤ì‹œì¨ ì™¸ë¶€ì— classifier $p(c|w)$ ë¥¼ ë”°ë¡œë‘ëŠ” ë°©ì‹ì„ íƒí–ˆë‹¤.

### Continuous Diffusion Langauge Modeling

1. **End-To-End Training**

ìš°ì„  ê°€ì¥ ë¨¼ì €í•´ì•¼ë  ê²ƒì€ ê°ê°ì˜ `ë‹¨ì–´ë“¤ì„ dì°¨ì›ì˜ ë²¡í„°ë¡œ embedding`ì„ ì‹œì¼œì•¼í•œë‹¤. ê·¸ë˜ì„œ d x nì˜ í…ì„œë¥¼ ì–»ëŠ”ë‹¤.

$EMB(w_i)$ â†’ $EMB(w) = [EMB(w_1),\cdots, EMB(w_n)] \in \mathbb R^{nd}$ 

ì—¬ê¸°ì„œ diffusion modelì˜ parameterë“¤ê³¼ word embeddingì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” full objectiveë¥¼ ì œì‹œí•œë‹¤.( Gaussian embeddingì´ë‚˜ pre-trained word embeddingë„ ì‹¤í—˜ì„ í•´ë´¤ëŠ”ë°, Diffusion LMì— ëŒ€í•´ì„œ `fixed embeddingsì€ end-to-end trainingì— ë¹„í•´ suboptimal`ì— ë„ë‹¬í–ˆë‹¤ê³  í•œë‹¤.)

ì´ë¥¼ ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ $q_{\phi}(x_0|w) = N(EMB(w),\sigma_0 I)$ì´ê³  standard diffusion processë¥¼ ëŒë¦¬ê¸° ì „ì— Markov chainí•˜ë‚˜ë¥¼ ì¶”ê°€í•´ $wâ†’ x_0$ ë¡œ ê°€ê²Œ ë§Œë“ ë‹¤. ê·¸ëŸ¼ ì—­ìœ¼ë¡œ $x_0 â†’ w$ ë¡œ ê°€ëŠ” stepë„ í•„ìš”í• í…Œê³  ì´ë¥¼ `rounding step`ì´ë¼ í•˜ê³  ì‹ìœ¼ë¡œ ì“°ë©´ $p_{\theta} (w|x_0) = \Pi_{i = 1}^n p_\theta (w_i|x_i)$ ì´ê³  ê° ë‹¨ì–´ì˜ ëŒ€ì‘í•˜ëŠ” ë²¡í„°ë“¤ì˜ `softmaxë¥¼ ì´ìš©`í•´ ì ì ˆí•œ í† í°ì„ ë³µêµ¬í•˜ëŠ” ì‹ì´ë‹¤.

ì´ ê³¼ì •ì˜ objectiveë¥¼ ì‹ìœ¼ë¡œì“°ê³  simple formìœ¼ë¡œ ì“°ë©´

$$
L^{e2e}_{simple}(w) = \mathbb E_{q_{\phi}(x_{0:T}|w)} [L_{simple}(x_0) + ||EMB(w) - \mu_\theta(x_1,1)||^2 - \log p_\theta(w|x_0)] 
$$

ì´ ëœë‹¤.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e6f4dc33-e9f4-4f1c-9c52-ac01d7c0a573/Untitled.png)

1. **Reducing Rounding Errors**

$x_0 â†’ discrete \;\;text$ ì€ $x_0$ê°€ wordë“¤ì˜ embeddingì— ì •í™•íˆ ìˆë‹¤ë©´ softmaxë¥¼ ì‚¬ìš©í•´ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í°ìœ¼ë¡œ ë³µêµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° ì‹¤ì œ í•´ë³´ë‹ˆ ì˜ ì•ˆëê³ , ì´ìœ ëŠ” $L_{simple}(x_0)$ê°€  $x_0$ì™€ wordì‚¬ì´ì˜ ê´€ê³„ë¥¼ tê°€ 0ê·¼ì²˜ì¼ë•Œë§Œ ê´€ì‹¬ì„ ê°€ì ¸ì„œ $x_0$ì˜ structureë¥¼ ì˜ ëª¨ë¸ë§í•˜ëŠ”ë° ì¶©ë¶„í•˜ì§€ ì•Šì•„ì„œ ì´ë‹¤.ê·¸ë˜ì„œ ëª¨ë“  tì—ëŒ€í•´ì„œ  $x_0$ë¥¼ modelingí•  ìˆ˜ ìˆë„ë¡ lossë¥¼ ìˆ˜ì •í–ˆë‹¤.

$$
L_{x_0 simple}^{e2e} (x_0) = \sum_{t=1}^T \mathbb E _{x_t}||f_{\theta}(x_t,t)-x_0||^2
$$

`$f_\theta(x,t)$  predicts $x_0$ directly`

ê·¸ë¦¬ê³  decodingí•  ë•Œë„ ì´ë¥¼ ì‚¬ìš© ( clamping trick : fê°€ ì˜ˆì¸¡í•œ $x_0$ë¥¼ $x_0$ì™€ ê°€ì¥ ê°€ê¹Œìš´ word embedding sequenceë¡œ mapping, ì¦‰ w_pred)

$x_{t-1} = \sqrt {\bar \alpha}f_\theta (x_t,t) + \sqrt{(1-\bar \alpha)}\epsilon$ â€” `clamping trick` -â†’ $x_{t-1} = \sqrt {\bar \alpha}Clamp(f_\theta (x_t,t)) + \sqrt{(1-\bar \alpha)}\epsilon$

### Decoding and Controllable Generation with Diffusion-LM

1. Controllable Text **Generation**

$$
\nabla _{x_{t-1}} \log p(x_{t-1}|x_-t,c) = \nabla _{x_{t-1}} \log p (x_{t-1}|x_t) + \nabla _{x_{t-1}} \log p(c|x_{t-1})
$$

ì¦‰ ì™¸ë¶€ì— classifierë¥¼ ë”°ë¡œ ë‘”ì±„ë¡œ í•™ìŠµí•˜ê² ë‹¨ ì´ì•¼ê¸°ì´ê³ , ê°ê°ì˜ stepë§ˆë‹¤ gradient stepì„ ë°Ÿê² ë‹¨ ì–˜ê¸°. performanceì™€ speedë¥¼ ìœ„í•´ ë‘ê°€ì§€ë¥¼ ìˆ˜ì •í–ˆëŠ”ë°

1. Fluency Regularization
    
    $\lambda$ë¼ëŠ” hyperparamì„ ë„ì…í•´ fluencyì™€ controlê°„ì˜ tradeoffë¥¼ ì¡°ì ˆ
    
2. multiple gradient steps
    
    control qualityë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í•œë²ˆì˜ diffusion stepë‹¹ 3ë²ˆì˜ Adagrad updateë¥¼ ì ìš©
    
1. Minimum Bayes Risk **Decoding**
    
    negative BELU (Bilingual Evaluation Understudy)ë¥¼ ì´ìš©í•´ ê°€ì¥ ë‚®ì€ BELU scoreë¥¼ ê¸°ë¡í•œ sampleì„ íƒí•œë‹¤.
    

---

# DiffuSeq **: Sequence to Sequence Text Generation with Diffusion Models**

## DiffuSeq

[DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models](https://arxiv.org/abs/2210.08933)

https://github.com/Shark-NLP/DiffuSeq

- Conditioning
- Classifier free guidance
- Seq2Seq

Transformerë¥¼ modelë¡œ ì‚¬ìš© â‡’ `Attention`ì„ í†µí•´ classifier free guide ë°©ì‹ì„ ì œì‹œ

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/35ff4bdc-8eee-4374-8755-3694e88498df/Untitled.png)

Notation)

**source sequence $w^x =[w^x_1,w^x_2,...w_m^x]$**

**target sequence $w^y = [w_1^y,w_2^y,...,w_n^y]$**

ëª¨ë¸ì€ source sequenceì˜ conditioningìœ¼ë¡œ target seqeunceë¥¼ ìƒì„±

### Forward Process with Partially Noising

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/314b0108-6a1c-42f1-9666-7a285cbadda5/Untitled.png)

Diffusion-LMì—ì„œ ì²˜ëŸ¼ $EMB(w)$ì„ ì´ìš©í•´ discrete text w ë¥¼ continuous spaceë¡œ mapping ( `embedding`, `concatenation`)

- sequence $w^x$ ì™€ $w^y$ì˜ pairê°€ ì£¼ì–´ì§€ê³ , DiffuSeqëŠ” unified feature spaceë¥¼ í•™ìŠµí•¨

original Markov chainì— $q_{\phi}(z_0|w^{x\bigoplus y})= N(EMB(w^{x\bigoplus y}),\beta_0I)$ë¥¼ ì¶”ê°€í•œ í›„ diffusion process ì§„í–‰

ê·¸ëŸ°ë° ì „ì²´ $z_t$ ($x_t$ì™€ $y_t$ ë‘˜ë‹¤) ê°€ ì•„ë‹Œ íƒ€ê²Ÿ ì†ŒìŠ¤ì¸ $y_t$ì—ë§Œ noiseë¥¼ ì¶”ê°€í•˜ëŠ” ì‹ìœ¼ë¡œ forward process â†’ `partially noising`

<aside>
ğŸª§ implementation ) $x_t$ê¹Œì§€ ê°™ì´ corruput ì‹œí‚¤ê³  ì´ë¥¼ $x_0$ë¡œ replace
trainingê³¼ inferenceí•  ë•Œ ë‘˜ë‹¤ ì´ ë°©ì‹ì„ ì‚¬ìš©

</aside>

### Reverse Process with Conditional Denoising

**Goal :** $z_t$ â€” denoise â€”> $z_0$

$p_{\theta}(z_{0:T}) = p(z_T)\Pi_{t=1}^T p_\theta(z_{t-1}|z_t)$ 

**model :** $f_\theta (z_t,t)$ ; Transformer â‡’ $x_t$ì™€ $y_t$ ê°„ì˜ semantic relationì„ í•™ìŠµ

$p_{\theta}(z_{t-1}|z_t) = N(z_{t-1};\mu_\theta(z_t,t),\sigma_\theta(z_t,t))$

**Loss**

$$
min_\theta[\sum_{t=2}^T||y_0-\tilde f_\theta(z_t,t)||^2 + ||EMB(w^y)-\tilde f_{\theta}(z_1,1)||^2 + R(||z_0||^2)]
$$

í‹¸ë‹¤ëŠ” f ê°€ ì¶”ë¡ í•œ zì¤‘ yë§Œ ì“°ê² ë‹¤ëŠ” í‘œê¸°

**First term**

$y_0$ì— ê´€í•´ì„œ lossë¥¼ ê³„ì‚°í–ˆê¸°ì— $x_0$ì™€ ë¬´ê´€í•´ ë³´ì´ëŠ”ë° transformerì˜ attention mechanism ë•Œë¬¸ì— $x_0$ë„ ê³ ë ¤ëœ termì„

**Third term**

embedding learningì„ regularize

<aside>
ğŸ’¡ embedding functionì„ sourceì™€ targetì´ ê³µìœ í•˜ë‹ˆê¹Œ ë‘ê°œì˜ feature spaceë¥¼ í•¨ê»˜ ë°°ì›€

</aside>

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/238b4eab-8c28-4b55-bd3c-d14d78cc6ce3/Untitled.png)

decodingí•  ë•Œ Diffusion LM ì²˜ëŸ¼ BELU score ì‚¬ìš©