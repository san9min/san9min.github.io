---
title: "Diffusion : Kick-off"
date: 2023-01-15
readingTime: 15 
thumbnail: /images/diffusion_kickoff/thumbnail.png
tags: [Generative AI, Diffusion]
category : [Tech Review]
---
### Genetrative Model Framework
Genetraionì€ í¬ê²Œ ë‘ê°€ì§€ frameworkìœ¼ë¡œ ë‚˜ë‰œë‹¤.
- **Likelihood-based**
    - Autoregressive Models
    - Variational Autoencoders
    - Flow-based Models
    - **`Diffusion models`**
- **Implicit model**
    - Generative Adversarial Networks(GAN)

![B948109D-1EBD-4BF6-B739-FB8EF24AE95E.png](/images/diffusion_kickoff/01.png)

diffusion modelì€ likelihood basedì´ë‹¤.

---

### ğŸ“š Background  

Diffusionì€ í™•ë¥  ê¸°ë°˜ì˜ processì´ë‹¤. ë¨¼ì € í•µì‹¬ í™•ë¥  ê°œë… 3ê°€ì§€ë¥¼ ì •ë¦¬í•˜ì.

#### (1) KL-Divergence

> `ë‘ í™•ë¥  ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€`ë¥¼ ê³„ì‚°, minimizeë¥¼ í†µí•´ ê·¼ì‚¬


$$
D_{KL}(q\||p)=
\begin{cases}
-\displaystyle\sum_i q_i \log\frac{p_i}{q_i}, & \text{(discrete form)} \\\\
-\displaystyle\int q(x)\log\frac{p(x)}{q(x)}, & \text{(continuous form)}
\end{cases}
$$

ì‹ì„ ì „ê°œí•˜ë©´ Self Entropy termê³¼ Cross Entropyìœ¼ë¡œ ë¶„ë¦¬í•  ìˆ˜ ìˆë‹¤.

$$
D_{KL} = -H(q) + H(q,p)
$$

ì˜ˆë¥¼ ë“¤ì–´, continuous formì— ëŒ€í•´

$$
\int q(x) log(q(x))dx + \int q(x)(-log(p(x))dx\\ = -H(q) + H(q,p)
$$

ë¡œ ì“¸ ìˆ˜ ìˆë‹¤.


ì‹ì„ ìì„¸íˆ ë³´ì.
ìš°ë¦¬ê°€ që¥¼ pë¡œ ê·¼ì‚¬ì‹œí‚¤ê¸° ìœ„í•´, KL divergenceë¥¼ minimizeí•œë‹¤ê³  í–ˆì„ ë•Œ
Self Entropy termì€ qì˜ varianceë¥¼ ì¦ê°€ì‹œí‚¤ì¼œ ë„“ê²Œ í¼ì§„ ë¶„í¬ê°€ ë˜ë ¤ëŠ” ê²½í–¥ì„ ê°–ê²Œí•˜ê³  
Cross Entropy termì€ p ë¶„í¬ì—ì„œ likelihoodê°€ ê°€ì¥ ë†’ì€ ì§€ì ì—ì„œ Delta functionì²˜ëŸ¼ ë˜ê²Œí•˜ë ¤ëŠ” ê²½í–¥ì„ ê°–ê²Œ í•  ê²ƒì´ë‹¤.  

ì´ ë‘ termì„ í†µí•´ (ì‹¸ìš°ëŠ” ëŠë‚Œ?) qê°€ pë¡œ ê·¼ì‚¬ê°€ ëœë‹¤.

**KL divergenceì˜ íŠ¹ì„±** 

* **í•­ìƒ 0 ì´ìƒ**ì´ë‹¤. CEëŠ” ì•„ë¬´ë¦¬ ë‚®ì•„ì ¸ë´¤ì (ì¦‰, qì™€ pê°€ ê°™ì€ ë¶„í¬ê°€ ëœë‹¤ í–ˆì„ ë•Œ) self-entropyì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìµœì†Ÿê°’ì´ 0ì´ê³ , ì ˆëŒ€ ìŒìˆ˜ê°€ ë  ìˆ˜ ì—†ë‹¤. 

* ì—„ë°€íˆëŠ” ê±°ë¦¬ ê°œë…ì´ ì•„ë‹ˆë‹¤. 

* ì¼ë°˜ì ìœ¼ë¡œ $D_{KL}(p|q) \neq D_{KL}(q|p)$ì´ë‹¤.

#### (2) Bayes Rule  
> ë³µì¡í•œ posterior p(zâˆ£x)ë¥¼ priorÂ·likelihoodÂ·evidence í•­ìœ¼ë¡œ ë¶„í•´, ELBOì™€ KL ì‹ ë„ì¶œ

$$
P(H \mid E) \=\ \frac{P(H)\,P(E \mid H)}{P(E)}
$$

*E : Evidence(sample x, ì¦ê±°Â·ê´€ì¸¡ ë°ì´í„°), H : Hypothesis(latent z ,ê°€ì„¤)*

| ê¸°í˜¸            | ìš©ì–´                              |ì˜ë¯¸                                                      |
| ------------- | ---------------------------------- | ----------------------------------------------------------- |
| $P(H)$        | **Prior probability**              | ê´€ì¸¡ ì „ì— ê°€ì„¤ $H$ê°€ ì°¸ì¼ ì‚¬ì „í™•ë¥                                        |
| $P(E \mid H)$ | **Likelihood**                     | $H$ê°€ ì°¸ì¼ ë•Œ ì¦ê±° $E$ê°€ ë‚˜íƒ€ë‚  ê°€ëŠ¥ë„ <br>â†’ ê°€ì„¤ $H$ê°€ ì¦ê±° $E$ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ |
| $P(E)$        | **Evidence / Marginal likelihood** | ê°€ì„¤ì„ êµ¬ë¶„í•˜ì§€ ì•Šê³  $E$ê°€ ê´€ì¸¡ë  ì „ì²´ í™•ë¥                                   |
| $P(H \mid E)$ | **Posterior probability**          | ì¦ê±° $E$ë¥¼ ë³¸ ë’¤ ê°€ì„¤ $H$ê°€ ì°¸ì¼ ì‚¬í›„í™•ë¥                                  |


#### (3) Monte Carlo Method
> ì ë¶„ ëŒ€ì‹  **ìƒ˜í”Œ í‰ê· **ìœ¼ë¡œ ê·¼ì‚¬

ëœë¤ í‘œë³¸ì„ ë½‘ì•„ (`sampling`ì„ í†µí•´) í•¨ìˆ˜ê°’ì„ í™•ë¥ ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ê²°êµ­ ê·¼ì‚¬(`approximation`) ì´ì•¼ê¸°ë‹¤.


ì˜ˆë¥¼ë“¤ë©´

$$
\int p(x)f(x)dx = E_{x \sim p(x)}[f(x)] \approx \frac 1 K \sum_i^K f(x_i), x_i \sim p(x)
$$

í™•ë¥  ë°€ë„í•¨ìˆ˜ p(x)ë¥¼ ë”°ë¥´ëŠ” xì—ëŒ€í•œ f(x)ì˜ ê¸°ëŒ“ê°’ì„ êµ¬í•˜ê³  ì‹¶ë‹¤í–ˆì„ ë•Œ p(x)ì—ì„œ Kê°œì˜ ìƒ˜í”Œì„ ë½‘ì•„ ì´ë¡œ ê³„ì‚°í•´ë„ ê´œì°®ë‹¤ëŠ” ì´ì•¼ê¸°ì´ë‹¤.

---


### ğŸ¤” ì™œ Variational Inference(VI)ê°€ í•„ìš”í• ê¹Œ?  
> ì§ì ‘ ì ë¶„ì´ ë¶ˆê°€ëŠ¥í•œ posterior $p_\theta(z\mid x)$ ëŒ€ì‹ , tractable ê·¼ì‚¬ë¶„í¬ $q_\phi(z\mid x)$ë¡œ ë¬¸ì œë¥¼ í’€ì–´ $\log p_\theta(x)$ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•´  


**ë¨¼ì €, Likelihood-based ëª¨ë¸ì´ ìµœì í™”í•˜ë ¤ëŠ” í•µì‹¬ ê°’ ë‘ ê°€ì§€**

1. **Likelihood**  
   $$
   \log p_\theta(x)
   $$
   â†’ ëª¨ë¸ì´ ê´€ì¸¡ $x$ ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ì˜ ì²™ë„

2. **Posterior**  
   $$
   p_\theta(z \mid x)
   $$
   â†’ ì£¼ì–´ì§„ $x$ ì—ì„œ ì ì¬ ë³€ìˆ˜ $z$ ê°€ ì·¨í•  ë¶„í¬

Bayes Ruleë¡œ posterior $p_\theta(z\mid x)$ë¥¼ ì •ì˜í•  ìˆ˜ëŠ” ìˆì§€ë§Œ  **ì •ê·œí™” ìƒìˆ˜** $p_\theta(x)$ ê³„ì‚°ì´ ì–´ë µê³  **ê³ ì°¨ì› ì ë¶„** ë•Œë¬¸ì— ì‹¤ì œ ê°’ì„ êµ¬í•˜ê¸°ê°€ ì‚¬ì‹¤ìƒ ë¶ˆê°€ëŠ¥í•˜ë‹¤.  


$$
p_\theta(z\mid x) = \frac{p_\theta(x\mid z) p_\theta(z)}{p_\theta(x)}
\quad\text{where}\quad p_\theta(x)=\int p_\theta(x\mid z)p_\theta(z) dz
$$


ë”°ë¼ì„œ **Variational Inference**ëŠ” ë‹¤ë£¨ê¸° ì‰¬ìš´ ë¶„í¬ $q_\phi(z\mid x)$ë¡œ $p_\theta(z\mid x)$ë¥¼ **ê·¼ì‚¬**í•˜ê³ , ë‘ ë¶„í¬ì˜ ì°¨ì´ë¥¼ **KL Divergence**ë¡œ ìµœì†Œí™”í•œë‹¤.  


ì´ ê³¼ì •ì˜ ìµœì í™” ëª©í‘œê°€ **ELBO(Evidence Lower Bound)** ì´ë‹¤.


---


### ğŸ“ ELBO : Evidence Lower Bound  


ì •ë¦¬í•˜ë©´, Variational Inferenceì˜ ê¶ê·¹ì  ëª©ì ì€ ë³µì¡í•œ posterior $p(z\mid x)\$ ë¥¼ ë‹¤ë£¨ê¸° ì‰¬ìš´ $q_\phi(z\mid x)\$ ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒ.  
ì—¬ê¸°ì„œ latent variable z ì˜ ì‚¬ì „í™•ë¥  ë¶„í¬ p(z)ëŠ” xì™€ ë¬´ê´€í•´ ê°€ì¥ ê°„ë‹¨í•˜ê³  ì˜ˆìœ Gaussianì´ë¼ í•˜ì. ê·¸ë¦¬ê³  që¥¼ $\phi$, pë¥¼ $\theta$ë¡œ parameterizeí•˜ì. 


$$q_\phi^{*} = \underset{q_\phi \in \mathcal Q}{\arg\min} D_{KL}\(q_\phi(z \mid x) || p_\theta(z \mid x))
$$

#### (1) KL ë¶„í•´
Bayes Ruleì— ì˜í•´ ìš°ë¦¬ëŠ” posterior p(z|x)ë¥¼ p(z), p(x), p(x|z)ë¡œ ì“¸ ìˆ˜ ìˆë‹¤.
ê·¸ëŸ¬ë¯€ë¡œ Bayes Ruleì„ ì´ìš©í•´ KL divergenceë¥¼ í‘œí˜„í•˜ë©´


$$
\begin{aligned}
D_{KL}\\bigl(q_\phi(z\mid x)\|p_\theta(z\mid x)\bigr)
  &= \int q_\phi(z\mid x)\
       \log\frac{q_\phi(z\mid x)}{p_\theta(z\mid x)}\
       \mathrm dz \\\\[6pt]
  &= \int q_\phi(z\mid x)\
       \log\frac{q_\phi(z\mid x)p_\theta(x)}
                {p_\theta(x\mid z)p_\theta(z)}\
       \mathrm dz \\\\[6pt]
  &= \int q_\phi(z\mid x)\
       \log\frac{q_\phi(z\mid x)}{p_\theta(z)}\
       \mathrm dz
     + \log p_\theta(x)
     - \int q_\phi(z\mid x)\
         \log p_\theta(x\mid z)\
         \mathrm dz \\\\[6pt]
  &= D_{KL}\bigl(q_\phi(z\mid x)\|p_\theta(z)\bigr)
     + \log p_\theta(x)
     - \mathbb E_{z \sim q_\phi(z\mid x)}
       \bigl[\log p_\theta(x\mid z)\bigr]
\end{aligned}
$$


ë¡œ ì •ë¦¬ëœë‹¤. 


ê·¸ëŸ°ë° ì—¬ê¸°ì„œ **$\log p_{\theta}(x)$ëŠ” intractableí•˜ë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” tractableí•œ lower bound(ELBO)ë¥¼ ì¡ê³  ì´ë¥¼ maximizeí•˜ëŠ” ë°©ì‹ì„ ì·¨í•œë‹¤.**


Expectation : $D_{KL} (q_{\phi}(z|x) || p(z|x))$ë¥¼ minimizeí•˜ëŠ” $\phi$ë¥¼ ì°¾ì

Maximization :  $\phi$ë¥¼ ê³ ì •í•˜ê³  $\log p_{\theta}(x)$ì˜ lower boundë¥¼ maximizeí•˜ëŠ” $\theta$ë¥¼ ì°¾ì

<aside>
<bold>log p(x) ë¥¼ evidence, likelihoodë¼ê³  í•œë‹¤</bold>

 $\theta$ë¡œ parameterizedëœ ìš°ë¦¬ì˜ modelì´ observed data xì— ëŒ€í•´ marginal probabilityë¥¼ ê³„ì‚°í–ˆì„ ë•Œ ë§Œì•½ ìš°ë¦¬ ëª¨ë¸ì´ ì˜ í•™ìŠµì´ ë˜ì—ˆë‹¤ë©´ ë†’ì€ ê°’ì„ ë‚´ë†“ì„ ê²ƒì´ë‹¤. ì¦‰, í•™ìŠµ ì¤‘ì— $\theta$ë¥¼ ì ì‹œ fixí•´ë†“ê³  evaluationì„ í–ˆì„ ë•Œ ë†’ì€ ê°’ì„ ë‚´ë†“ê³  ìˆë‹¤ë©´ ìš°ë¦¬ëŠ” ì˜ ê°€ê³  ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ $\log p(x;\theta)$ë¥¼ ìš°ë¦¬ê°€ ì˜ ê°€ê³  ìˆë‹¤ëŠ” ì˜ë¯¸ì—ì„œ  evidenceë¼ í•œë‹¤. 
</aside>


$D_{KL} (q_{\phi}(z|x) || p(z|x))\ge 0$ ì´ë¯€ë¡œ 

$$
\log p_{\theta}(x) \ge E_{z \sim q(z)}[\log p_{\theta}(x|z)] - D_{KL}(q_{\phi}(z|x)||p(z)) 
$$

ê°€ ëœë‹¤.

#### (2) ELBO ì •ì˜

$$
{\text{ELBO}} = E_{z \sim q_{\phi}(z|x)}[\log p_{\theta}(x|z)] - D_{KL}(q_{\phi}(z|x)||p(z)) 
$$


$E_{z \sim q(z|x)}[\log p_{\theta}(x|z)]$

#### 1st term : Reconstruction Error
* generative model(Decoder in VAE)
* Decoderê°€ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë³µì›í•˜ëŠ”ê°€



$D_{KL}(q(z|x)||p(z)) \text{ or } E_{q(z|x)} [\log \frac {q(z|x) }{p(z)}]$

#### 2nd term : Regularization term`
* inference model(Encoder in VAE)
* $q(z\mid x)$ ê°€ prior $p(z)$ ì™€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œê°€


ë¥¼ ë‚˜íƒ€ëŒ„ë‹¤.

$$
\log p_{\theta}(x) \ge {\text{ELBO}}
$$
ì´ë¯€ë¡œ ELBOë¥¼ ìµœëŒ€í™”í•˜ë©´ ê³§ KLë„ ì¤„ì´ë©´ì„œ $\log p_\theta(x)$ë¥¼ ëŒì–´ì˜¬ë¦¬ëŠ” íš¨ê³¼ë¥¼ ì–»ëŠ”ë‹¤.


---

### ğŸ ë‚˜ê°€ë©°

#### â‘  ELBOê°€ í•˜ëŠ” ì¼  
* **ELBO**ëŠ” ì§ì ‘ ê³„ì‚°ì´ ì–´ë ¤ìš´ **likelihood $p_\theta(x)$** ì˜ *ì•ˆì „í•œ í•˜í•œ(lower bound)*.  
* ì´ í•˜í•œì„ **ìµœëŒ€í™”**í•˜ë©´ ì‹¤ì œ $\log p_\theta(x)$ ë„ í•¨ê»˜ ëŒì–´ì˜¬ë¦´ ìˆ˜ ìˆë‹¤.  


#### â‘¡ DiffusionÂ·VAE ë“± ëª¨ë¸ì˜ í•™ìŠµ ë²•ì¹™  
* **Diffusion** ëª¨ë¸ì€ likelihood-based ê³„ì—´ â†’ í•™ìŠµ ë‹¨ê³„ì—ì„œ **ELBO ìµœëŒ€í™”**ë¡œ íŒŒë¼ë¯¸í„° ìµœì í™”  
* VAEÂ·Flow ë“± ëª¨ë“  **Variational ëª¨ë¸**ë„ ë™ì¼í•œ ëª©í‘œë¥¼ ë”°ë¥¸ë‹¤.  


#### â‘¢ Variational Inference ê´€ì   
* ë³µì¡í•œ ì‚¬í›„ë¶„í¬ $p_\theta(z\mid x)$ ëŒ€ì‹  **ê·¼ì‚¬ ë¶„í¬ $q_\phi(z\mid x)$** ì‚¬ìš©  
* ë‘ ë¶„í¬ì˜ ì°¨ì´ë¥¼ **KL Divergence**ë¡œ ì¸¡ì •Â·ìµœì†Œí™”  
* ê²°ë¡ ì ìœ¼ë¡œ ë‚˜ì˜¨ ELBOëŠ” *Reconstruction Error* ì™€ *Regularization term* ìœ¼ë¡œ êµ¬ì„± 

> *Training* ë‹¨ê³„ì—ì„œëŠ” **ELBO** ë¥¼ ìµœëŒ€í™”í•´ $\log p_\theta(x)$ ë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ë†’ì¸ë‹¤.
> *Inference* ë‹¨ê³„ì—ì„œëŠ” $q_\phi(z\mid x)$ ë¡œ posteriorë¥¼ ê·¼ì‚¬í•œë‹¤.  


