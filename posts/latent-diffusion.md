
---
title: "latent diffusion"
date: 2023-02-17
readingTime: 20 
thumbnail: images/post-001-thumb.jpg
tags: [Generative AI, Diffusion, DDPM]
---



# **High-Resolution Image Synthesis with Latent Diffusion Models**

## Stable Diffusion

[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)

https://github.com/CompVis/stable-diffusion

- `Latent Diffusion Model`
- `Cross-Attention based Conditioning`
- `text to image`

pixcelì´ ì•„ë‹Œ latent spaceì—ì„œ diffusion modelì„ ëŒë ¤ì„œ computational costë¥¼ ë§ì´ ì¤„ì˜€ë‹¤. ê·¸ëŸ°ë° ë‚œ ì´ ëª¨ë¸ì´ ë” ë§¤ë ¥ì ì¸ ë¶€ë¶„ì€ cross attentionìœ¼ë¡œ generalí•œ conditioning inputì„ ë°›ëŠ”ë‹¤ëŠ” ì ì´ë‹¤. ì´ë¡œì¨ text ë¡œë„ gudienceë¥¼ ì¤„ ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ë§¤ìš° ê°ë™ì´ì—ˆë‹¤..

### Guided Diffusion

Stable Diffusionì„ ë³´ê¸°ì „ì— Guided Diffusionì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ì•¼í•œë‹¤. 

Sampling processì— conditionì„ ì¤˜ì„œ generateë˜ëŠ” sampleë“¤ì„ ì¡°ì‘í•  ìˆ˜ ìˆë‹¤. ì¦‰ generationì„ guideí•œë‹¤. ì¦‰ prior data distribution p(x)ì— condition yë¥¼ ì¤˜ì„œ p(x|y)ê°€ ë˜ëŠ”ê²ƒì´ê³  conditional diffusionì€ ê° diffusion stepì— condition infoë¥¼ ë”í•´ì¤€ë‹¤ ìƒê°í•˜ë©´ ë˜ê³  ì´ë¡œì¨ imageë‚˜ textë¥¼ ë„£ì„ ìˆ˜ ìˆê²Œ ëœë‹¤. 

$$
p_{\theta}(x_{0:T}|y) = p_{\theta}(x_T)\Pi_{t=1}^Tp_{\theta}(x_{t-1}|x_t,y)
$$

ì´ë ‡ê²Œ í•˜ë©´ ìš°ë¦¬ê°€ text yë¥¼ inputìœ¼ë¡œ ëª¨ë¸ì— ì¤„ ìˆ˜ ìˆë‹¤.

Diffusion modelì€ SDEë¡œ í‘œí˜„í•  ìˆ˜ ìˆê³  ê·¸ë˜ì„œ guided diffusion modelì€ $\nabla \log p_{\theta}(x_t|y)$ë¥¼ ë°°ìš°ê³ ì í•˜ê³  Bayes Ruleì— ì˜í•´

$$
\nabla_{x_t} \log p_{\theta}(x_t|y) = \nabla_{x_t} \log (\frac{p_{\theta}(y|x_t)p(x_t)}{p_{\theta}(y)}) \\ = \nabla_{x_t} \log p_{\theta}(x_t) + \nabla_{x_t} \log p_{\theta}(y|x_t)
$$

ë¡œ ì •ë¦¬ ëœë‹¤. ì—¬ê¸°ì„œ guidance scalar termì„ ì¶”ê°€í•´ ì£¼ë©´

**Classifier guidance**

$$

\nabla_{x_t} \log p_{\theta}(x_t|y) =\nabla_{x_t} \log p_{\theta}(x_t) + \gamma \cdot\nabla_{x_t} \log p_{\theta}(y|x_t)
$$

ì´ ì‹ì˜ ë‘ë²ˆì§¸ í…€ì„ ë‹¤ì‹œ Bayes Ruleì„ ì¨ì„œ ì •ë¦¬í•˜ë©´

**Classifier Free guidance**

[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)

$$
\nabla_{x_t} \log p_{\theta}(x_t|y) = (1-\gamma)\nabla_{x_t} \log p_{\theta}(x_t) + \gamma \cdot\nabla_{x_t} \log p_{\theta}(x_t|y)
$$

â‡’  $\tilde{\epsilon}(z_\lambda,c) = (1+w)\epsilon_\theta(z_\lambda,c) - w\epsilon_\theta(z_\lambda)$

<aside>
ğŸ’¡ unconditional termì€ 0ì„ embedding í•˜ëŠ” ë°©ì‹ ë“±ìœ¼ë¡œ ì²˜ë¦¬í•´ì„œ, `conditionalê³¼ unconditional ì„ ë™ì‹œì— í•™ìŠµ` (`í•˜ë‚˜ì˜ ëª¨ë¸`ë¡œ ì²˜ë¦¬ ê°€ëŠ¥)

</aside>

Latent Diffusionì˜ conditioningì€  `Classifier free guidance`

â­
$\epsilon_\theta(x_t,c) = s\epsilon_{cond}(x_t,c) + (1-s)\epsilon_{cond}(x_t,c_u)$

â­

ìœ¼ë¡œ formulateí•˜ê³  ì—¬ê¸°ì„œ $c_u$ëŠ” empty promptì— ëŒ€í•œ conditional embedding

### Architecture

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ba36e87a-4856-43c6-96fc-b307cc9aa560/Untitled.png)

`CLIP Text`

- Text understanding component
- for Text Encoding
- Transformer language model

`U-Net` + `Scheduler`

- Information creator
- latent space â‡’ faster

`Autoencoder Decoder`

- Image Decoder