<!DOCTYPE html>
<html lang="ko"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Sangmin Blog | Diffusion : DDIM</title>
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css">
  
  <link rel="stylesheet"
     href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
<link rel="stylesheet" href="../../assets/styles/main.css">
  
<link rel="icon" type="image/png" href="/assets/favicon.png" sizes="32x32">
</head><body>
  <main class="article">
    <h1>Diffusion : DDIM</h1>
    <div class="meta">Jan 31, 2023 Â· 20 min read</div>
    <img class="hero" src="../../images/diffusion_ddim/thumbnail.png" alt="cover image">
    <h2>ğŸš€ <strong>Denoising Diffusion Implicit Models</strong></h2>
<p><a href="https://arxiv.org/abs/2010.02502">Denoising Diffusion Implicit Models</a></p>
<ul>
<li>Deterministic sampling</li>
<li><code>Fast sampling</code></li>
</ul>
<aside>
DDPMìœ¼ë¡œ ì—¬ëŸ¬ stepí•˜ê³ , DDIMìœ¼ë¡œ ë¹ ë¥´ê²Œ ìƒ˜í”Œë§í•œë‹¤.
â‡’ ë†’ì€ í’ˆì§ˆê³¼ ë¹ ë¥¸ ìƒ˜í”Œë§
</aside>

<p>DDPMì˜ Markovianì´ì—ˆë˜ forward processë¥¼ <code>non-Markcovian</code> formìœ¼ë¡œ ì¼ë°˜í™”í•˜ê³ , reverese processëŠ” ì§§ì€ Markov chainìœ¼ë¡œ ì„¤ê³„í•´ì„œ ë” ë¹ ë¥´ê²Œ samplingì„ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ ì£¼ìš” contributionì´ë‹¤.</p>
<p>DDIMì—ì„œëŠ” DDPMì˜ $\bar\alpha$ë¥¼ $\alpha$ë¼ê³  ì¼ë‹¤.<br>ì—¬ê¸°ì„  DDPMì˜ notationì„ ë”°ë¼ ì •ë¦¬í–ˆë‹¤.</p>
<h3>ğŸ“ DDPM Remind</h3>
<p>DDPMì„ remindí•´ë³´ì.
DDPMì€ Variational lower boundë¥¼ maximizeí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.  </p>
<p>Gaussian transitionì„ í•˜ëŠ” Markov chainì„ ìƒê°í•´ forward processë¥¼ formulateí–ˆê³  ì´ ì—­ê³¼ì •ì€ intractableí•´ì„œ neural networkë¥¼ ì‚¬ìš©í–ˆë‹¤.</p>
<p>ê·¸ë¦¬ê³  ì‹ì„ Reparmeterizeí•´ì„œ $x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$ ë¡œ $x_t$ë¥¼ $x_0$ ì™€ noiseì˜ linear combinationìœ¼ë¡œ ì“¸ ìˆ˜ ìˆì—ˆë‹¤. </p>
<p>neural networkëŠ” noiseë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí–ˆì—ˆë‹¤.</p>
<p>$$
L_\gamma (\epsilon_{\theta}) = \sum_{t=1}^T \gamma_t  \mathbb E_{x_0 \sim q(x_0), \epsilon_t \sim N(0,I) }[||\epsilon_\theta ^{(t)}(\sqrt{\bar \alpha_t}x_0 +\sqrt{1-\bar{\alpha_t}}\epsilon_t)-\epsilon_t||^2_2]
$$</p>
<p>ì´ ì‹ì˜ $\gamma$ =1 ì¼ ë•Œì˜€ë‹¤.( <em>where</em> $\epsilon_\theta = {\epsilon_\theta^{(t)}}^{T}_{t=1}$ , $\gamma = [\gamma _1,\cdots,\gamma _T]$ )</p>
<p>ê·¸ëŸ° T ê°€ ì¶©ë¶„íˆ ì»¤ì•¼í•˜ê³  sequentialí•˜ê²Œ ê³„ì† ê³„ì‚°(iterations)ì„ ë–„ë ¤ì•¼í•˜ê¸° ë•Œë¬¸ì— computaitional costê°€ ë§¤ìš° ë†’ë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤.</p>
<h3>ğŸ§© Variational inference for Non-Markovian Forward Processes</h3>
<p>ìš°ë¦¬ì˜ generative modelì€ reverse ë¥¼ approximateì„ ì˜í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤.</p>
<p>ê·¸ë˜ì„œ iterationì˜ ìˆ˜ë¥¼ ì¤„ì´ê³ ìí•˜ëŠ” ì˜ì§€ì™€ í•¨ê»˜ ì‹ì„ ëœ¯ì–´ ê³ ì³ë³´ì. ìœ„ì˜ objective functionì„ ë³´ë©´ ìš°ë¦¬ê°€ ddpmì—ì„œ reparametrizationì„ í†µí•´ ë³¼ ìˆ˜ ìˆë“¯ joint distribution $q(x_{1:T}|x_0)$ ê°€ ì•„ë‹ˆë¼ $q(x_t|x_0)$ì—ë§Œ ì§ì ‘ì ì¸ dependencyê°€ ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤.  </p>
<p>ê°™ì€ obejctiveë¥¼ ê°–ê²Œ í•˜ê¸° ìœ„í•´, $q(x_t|x_0)$ë§Œ ë§Œì¡±í•˜ë©´ë˜ê³  ì´ë¥¼ ë§Œì¡±í•˜ëŠ” jointëŠ” ë§ê¸°ì— forward processë¥¼ non-Markovian ìœ¼ë¡œ ë°”ê¿” ì¼ë°˜í™”í•´ë³´ì.</p>
<h4>1ï¸âƒ£ <strong>Non-Markovian Forward Processes</strong></h4>
<p>inference distributionë“¤ì„ ëª¨ì•„ë…¼ Që¥¼ ìƒê°í•´ë³´ì. ì´ë¥¼ real vector $\sigma$ë¡œ inference distbì„ indexingì„ í•´ì„œ forward processë¥¼ inference distbë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>
<p>$$q_\sigma (x_{1:T}|x_0)=q_{\sigma}(x_T|x_0) \Pi_{t=2}^Tq_{\sigma}(x_{t-1}|x_t,x_0)$$</p>
<p>ì—¬ê¸°ì„œ $q(x_T|x_0) = N(\sqrt{\bar \alpha_T}x_0,(1-\bar{\alpha_T})I)$ë¼í•˜ê³  t &gt; 1ë³´ë‹¤ í´ ë•Œë¥¼ ìƒê°í•´ë³´ì.</p>
<p><strong>Reverse Conditional Distribution</strong></p>
<figure class="eq">
$$
q_\sigma\!\left(x_{t-1}\,\middle|\,x_t, x_0\right)
  = \mathcal{N}\!\Bigl(
      \sqrt{\alpha_{t-1}}\,x_0
      + \sqrt{\,1-\alpha_{t-1}-\sigma_t^{2}}\;
        \frac{x_t-\sqrt{\alpha_t}\,x_0}{\sqrt{1-\alpha_t}},
      \;\sigma_t^{2}\mathbf I
    \Bigr)
$$
</figure>

<p>ë¡œ ì¨ì„œ  ëª¨ë“  tì— ëŒ€í•´ì„œ $q(x_t|x_0) = N(\sqrt{\bar \alpha_t}x_0, (1-\bar{\alpha_t})I)$ë¥¼ ë§Œì¡±í•˜ë„ë¡ (DDPMê³¼ ê°™ë„ë¡) formulateí–ˆë‹¤ëŠ” ê²ƒ.</p>
<p>ì´ forward processë¥¼ Bayes ruleë¡œ ë‹¤ì‹œ ì“°ë©´</p>
<p>$$
q_{\sigma}(x_t|x_{t-1},x_0) = \frac {q_{\sigma}(x_t,x_0)q_\sigma(x_t|x_0)}{q_{\sigma}(x_{t-1}|x_0)}
$$</p>
<p>ì´ ì‹ì„ ë³´ë©´  $x_t$ê°€ $x_{t-1}$ ë¿ë§Œì•„ë‹ˆë¼ $x_0$ì—ë„ ì˜ì¡´í•˜ë¯€ë¡œ ë”ì´ìƒ Markovianì´ ì•„ë‹ˆë‹¤. </p>
<aside>
$\sigma$ì˜ í¬ê¸°ê°€ forward processê°€ ì–¼ë§ˆë‚˜ stochasticí•œì§€ë¥¼ ê²°ì •í•œë‹¤.
`$\sigma$ â†’ 0`ì´ë©´ $x_0$ ì™€ $x_t$ê°€ ì£¼ì–´ì§€ë©´ ë°”ë¡œ `$x_{t-1}$ì´ determine` ëœë‹¤. 
ì¦‰ $\sigma$ê°€ 0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ deterministicí•´ì§
</aside>

<p>ì •ë¦¬í•˜ë©´ DDPMê³¼ DDIMì€ forward processì—ì„œ  <code>$q(x_t|x_0)$ ëŠ” ê°™ê²Œ</code> ë‘ê³  ë‹¤ë§Œ  <code>joint distributionì„ ë‹¤ë¥´ê²Œ</code> ê°€ì ¸ê°”ë‹¤.</p>
<p><img src="/images/diffusion_ddim/01.png" alt="Untitled"></p>
<h4>2ï¸âƒ£ <strong>Generative Process &amp;&amp; Unified Variational Inference Objective</strong></h4>
<blockquote>
<p><strong>Goal</strong> : $p_\theta (x_{0:T})$</p>
</blockquote>
<p>Generation ì¸¡ë©´ì—ì„œ <code>$x_t$â†’ $x_{t-1}$</code>ë¡œ ê°€ëŠ” processê°€ ê¶ê¸ˆí•˜ê³ , $q_{\sigma}(x_{t-1}|x_t,x_0)$ë¥¼ ì´ìš©í•´ $p_\theta ^t (x_{t-1}|x_t)$ë¥¼ defineí•´ë³´ì</p>
<aside>

<ol>
<li><p>$x_t$ ê°€ ì£¼ì–´ì§€ë©´ $x_0$ë¥¼ ì˜ˆì¸¡  by $f_\theta$  </p>
</li>
<li><p>$q_{\sigma}(x_{t-1}|x_t,x_0)$ë¥¼ ì´ìš©í•´ $x_{t-1}$ obtain</p>
</aside></li>
</ol>
<p>$x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$ ë¥¼ ì´ìš©í•´ ëª¨ë¸ì´ epsilon <code>noise</code>ì„ ì˜ˆì¸¡í•´, <code>$x_0$</code> ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ì˜ˆì¸¡í•˜ëŠ” f ë„ì…</p>
<p>$$
f_{\theta}^{(t)}(x_t) = (x_t -\sqrt{1-\bar{\alpha_t}}\epsilon_{\theta}^{(t)}(x_t))/\sqrt {\alpha_t} \approx x_0
$$</p>
<p>ê°€ ë˜ê³  ìš°ë¦¬ ëª¨ë¸ì€</p>
<figure class="eq">
$$
p^{(t)}_{\theta}\!\bigl(x_{t-1}\mid x_t\bigr)
  = 
  \begin{cases}
    \mathcal{N}\!\bigl(f^{(1)}_{\theta}(x_1),\,\sigma_1^{2}\mathbf I\bigr), & \text{if } t = 1,\\[6pt]
    q_{\sigma}\!\bigl(x_{t-1}\mid x_t,\,f^{(t)}_{\theta}(x_t)\bigr), & \text{otherwise}.
  \end{cases}
$$
</figure>

<p>ì´ê³  objective $J_\sigma(\epsilon_\theta)$ ëŠ” $\epsilon_{\theta}$ì˜ í•¨ìˆ˜ê°€ ëœë‹¤. </p>
<p>ë˜í•œ objectiveê°€ $\sigma$ì— ëŒ€í•œ dependencyê°€ ìˆìœ¼ë¯€ë¡œ ê° $\sigma$ì—ëŒ€í•´ ë”°ë¡œ í•™ìŠµì„ í•´ì£¼ì–´ì•¼í•œë‹¤.  </p>
<p>ê·¸ëŸ°ë° $J_\sigma$ëŠ” ì–´ë–¤ $\gamma$ ì—ëŒ€í•´ $L_\gamma$ì™€ ê°™ë‹¤ê³  í•œë‹¤.</p>
<p>$$
\text{Theorem 1)} \forall \sigma&gt;0, there \ exists \ \gamma \in \mathbb R^T_{&gt;0} ; and ; C \in \mathbb R \quad s.t.; J_\sigma = L_\gamma + C
$$</p>
<p>ì—¬ê¸°ì„œ $L_\gamma (\epsilon_{\theta}) = \sum_{t=1}^T \gamma_t \mathbb E_{x_0 \sim q(x_0), \epsilon_t \sim N(0,I) }[||\epsilon_\theta ^{(t)}(\sqrt{\bar \alpha_t}x_0 +\sqrt{1-\bar{\alpha_t}}\epsilon_t)-\epsilon_t||^2_2]$ë¥¼ ë‹¤ì‹œ ë³´ì. </p>
<p>ë§Œì•½ $\epsilon_{\theta}^t$ê°€ ì„œë¡œë‹¤ë¥¸ t ë¼ë¦¬ parameterë¥¼ ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ì „ì²´ë¥¼ maximizeí•˜ê¸° ìœ„í•´ì„  ìš°ë¦¬ëŠ” ê° tì— ëŒ€í•œ termë“¤ì„ ê°ê° maximizeí•´ì•¼ëœë‹¤, </p>
<p>ì¦‰ weight factor $\gamma$ì™€ ë¬´ê´€í•˜ê²Œ optimizationì´ ì§„í–‰ëœë‹¤ëŠ” ê²ƒì´ë‹¤. </p>
<p>ê·¸ëŸ¬ë¯€ë¡œ objectiveê´€ì ì—ì„œ $\gamma$ëŠ” arbitraryí•˜ê²Œ ì¡ì•„ë„ ë˜ê³ , ì´ë¥¼ 1ë¡œ ì¡ì•„ë„ Okì´ë‹¤. </p>
<p>ê·¸ëŸ°ë° theorem 1ì— ì˜í•˜ë©´ ì–´ë–¤ $L_\gamma$ ëŠ” $J_\sigma$ì™€ ê°™ì€ objectiveë¥¼ ê°–ìœ¼ë¯€ë¡œ $L_1$ì„  $J_\sigma$ ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p>
<aside>
ë§Œì•½ ëª¨ë¸ $\epsilon_\theta$ì˜ paramterê°€ ì„œë¡œë‹¤ë¥¸ të¼ë¦¬ ê³µìœ í•˜ì§€ ì•ŠëŠ” êµ¬ì¡°ë©´
 $J_\sigma$ì˜ objectiveë¡œ $L_1$ì„ ì¨ë„ Ok.
</aside>

<h3>âš™ï¸ Sampling From Generalized Generative Processes</h3>
<p>ìš°ë¦¬ëŠ” generalí•œ objectiveë¥¼ $L_1$ì´ ëŒ€ì²´ê°€ëŠ¥í•¨ì„ ë³´ì˜€ê³ , ê·¸ë˜ì„œ markovianì˜ forward processì™€ non markovianì˜ forward process ëª¨ë‘ì˜ objectiveì´ë¯€ë¡œ pretrained DDPMì„ ì‚¬ìš©í•´ë„ ì¢‹ë‹¤.</p>
<p>ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” $\sigma$ì— ë”°ë¥¸ sampling ì— ì§‘ì¤‘í•´ë³¼ê²ƒ </p>
<h4>1ï¸âƒ£ <strong>Denoising Diffusion Implicit Models (DDIM ì—…ë°ì´íŠ¸ ì‹)</strong></h4>
<p>ìš°ë¦¬ëŠ” ìœ„ì˜ $p_\theta$ë¡œ $x_t$ë¡œë¶€í„° $x_{t-1}$ì„ generateí•  ìˆ˜ ìˆê²Œ ëë‹¤.</p>
<figure class="eq">

<p>$$
x_{t-1} = \sqrt{\bar \alpha_{t-1}}(\frac{x_t-\sqrt{1-\bar \alpha_{t}}\epsilon_{\theta}^{(t)}(x_t)}{\sqrt{\bar \alpha_{t}}}) +\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot \epsilon_{\theta}^{(t)}(x_t) + \sigma_t\epsilon_t
$$</p>
</figure>

<p><strong>1st term</strong>  </p>
<p>$(\frac{x_t-\sqrt{1-\bar \alpha_{t}}\epsilon_{\theta}^{(t)}(x_t)}{\sqrt{\bar \alpha_{t}}})$  : predicted $x_0$ </p>
<p><strong>2nd term</strong>  </p>
<p>$\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot \epsilon_{\theta}^{(t)}(x_t)$ : direction pointing to $x_t$</p>
<p><strong>3rd term</strong></p>
<p>random noise independent of $x_t$</p>
<p>ì—¬ê¸°ì„œ $\sigma_t$ë¥¼ ì–´ë–»ê²Œ ì¡ëŠëƒì— ë”°ë¼ ë§¤ìš° í¥ë¯¸ë¡œìš´ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.</p>
<p><strong>case1</strong>  </p>
<p>$\sigma_t = \sqrt{(1-\bar\alpha_{t-1})/(1-\bar\alpha_t)}\sqrt{1-\bar\alpha_t/\alpha_{t-1}}$</p>
<p> forward processê°€ Markovianì´ ë˜ê³   DDPMìœ¼ë¡œ reduceëœë‹¤.</p>
<p><strong>case2</strong>  </p>
<p>$\sigma _t$ = 0 for all t</p>
<p>forward processê°€ <code>deterministic</code>í•´ì§„ë‹¤(t=1 ì¼ë•Œ ì œì™¸), ë˜í•œ generative processì—ì„œë„ noiseì˜ ê³„ìˆ˜ê°€ 0ì´ ë˜ì–´ë²„ë ¤ ë§ˆì°¬ê°€ì§€</p>
<p>â‡’ <code>DDIM</code></p>
<p>forward processê°€ ë”ì´ìƒ diffusionì´ ì•„ë‹ˆì§€ë§Œ, <code>DDPMì˜ objectiveë¡œ í•™ìŠµì´ëœ implicit model</code></p>
<p>implicit probablisticì´ë¼ í•˜ëŠ” ì´ìœ ëŠ” sampleë“¤ì´ latent variableì¸ $x_t$ì— ì˜í•´ ìƒì„±ë˜ì—ˆê¸° ë•Œë¬¸</p>
<h4>2ï¸âƒ£ <strong>Accelerated Generation Processes</strong></h4>
<p>ë‹¤ì‹œ ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ë…¼ë¦¬ë¥¼ ì´ì•¼ê¸°í•˜ë©´ $L_1$ì˜ objectiveê°€ ì–´ë– í•œ íŠ¹ì • forward process(joint distb)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  $q(x_t|x_0)$ë§Œ ë§Œì¡±í•˜ë©´ ëë‹¤.</p>
<p>ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ê¸°ì¡´ ì „ì²´ forward processì˜ length of time step Të³´ë‹¤ ë” ì‘ê²Œ  ì¼ë¶€ ëª‡ ê°œì˜ forward stepì— ëŒ€í•´ì„œë§Œ forward processë¥¼ ì§„í–‰í•˜ê³  ì´ë“¤ì˜ subsetì„ ê°–ê³  generative processë¥¼ ì§„í–‰í•´ë„ ì¢‹ë‹¤.</p>
<p>â‡’ pretrainëœ DDPMì„ í™œìš©í•´ generative processë¥¼ ëŒë ¤ë„ ë˜ê³  ì˜¤íˆë ¤ ì¢‹ë‹¤ëŠ” ì´ì•¼ê¸°</p>
<p><img src="/images/diffusion_ddim/02.png" alt="Untitled"></p>
<p>$\tau = [1,3]$, $\tau$ëŠ” (sampling) trajectoryì´ê³  [1,2,3,â€¦,T]ì˜ subsequenceì´ë‹¤. lengthë¥¼ Së¼ê³  denoteí•˜ê³ , ì´ëŠ” DDIMì˜ sampling stepë“¤ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸</p>
<h3>ğŸ”¬ <strong>Relevance to Nueral ODE</strong></h3>
<p>DDIM(with sigma = 0)ì„ ODEë¡œ rewriteí•´ë³´ì.</p>
<p>$$
d\bar x(t) = \epsilon_\theta^{(t)}(\frac{\bar x(t)}{\sqrt{\sigma^2+1}})d\sigma(t)
$$</p>
<p>where I.C : $x(T) \sim N(0,\sigma (T))$</p>
<p> ì¶©ë¶„íˆ discretization stepì„ ê±°ì¹˜ë©´ ì´ ODEë¥¼ reverseí•´ì„œ generation processì˜ reverse, <code>encoding</code>ì´ ê°€ëŠ¥í•´ì§„ë‹¤.</p>
<aside>

<p><strong>DDIM</strong> sampleì˜ high level featureë“¤ì€ $x_T$ì— encodingëœë‹¤.</p>
</aside>

<h3>ğŸ§ª ì‹¤í—˜ ê²°ê³¼ ìš”ì•½</h3>
<p>DDIMì€ DDPMë³´ë‹¤ í›¨ì”¬ ë” ì ì€ iterationìœ¼ë¡œ image generationì´ ê°€ëŠ¥í•˜ê³ , DDPMê³¼ëŠ” ë‹¬ë¦¬ initial latent $x_T$ê°€ fixë˜ë©´ generation trajectoryì™€ ë¬´ê´€í•˜ê²Œ high level image featuresë“¤ì´ ìœ ì§€ëœë‹¤.  </p>
<p>ê·¸ë˜ì„œ latent spaceìƒì—ì„œ ë°”ë¡œ interpolationì´ ê°€ëŠ¥í•˜ë‹¤. </p>
<p>ë˜í•œ sampleë“¤ì„ encodingí•  ìˆ˜ ìˆì–´ latent codeì—ì„œ  sampleì„ reconstructí•  ìˆ˜ ìˆë‹¤.(DDIMì˜ deterministicí•œ ì„±ì§ˆ)</p>
<p>ë‹¤ë¥¸ ì¡°ê±´ì€ ë‹¤ ê°™ê²Œ ë‘ê³  $\tau$( (sampling) trajectory â†’ how fast samples are obtained) ì™€  $\sigma$ (DDIM = 0)ë§Œì„ ì¡°ì ˆí•˜ë©° samplingì— ì§‘ì¤‘í–ˆê³ ,  $\sigma$ë¥¼ í¸í•˜ê²Œ controlí•˜ê¸° ìœ„í•´ $\eta$ ë„ì…í–ˆë‹¤.</p>
<figure class='eq'>

<p>$\eta$ = 1.0 DDPM</p>
<p>$\eta$ = 0.0 DDIM</p>
<p>DDPMê³¼ DDIMì„ interpolate</p>
<p>$$
\sigma_{\tau_i} =\eta \sqrt{\frac{1-\bar\alpha_{\tau_{i-1}}}{1-\bar\alpha_{\tau_{i}}}}\sqrt{1-\frac{\bar\alpha_{\tau_i}}{\bar\alpha_{\tau_{i-1}}}}
$$</p>
</figure>

<ol>
<li><strong>Sample Quality and Efficiency</strong></li>
</ol>
<p><img src="/images/diffusion_ddim/03.png" alt="Untitled"></p>
<ol start="2">
<li><strong>Sample <code>Consistency</code> in DDIMs</strong><aside>
DDIMì˜ generative processëŠ” deterministicí•˜ê³  $x_0$ëŠ” ì˜¤ì§ initial state $x_T$ì—ë§Œ ì˜ì¡´í•œë‹¤
</aside></li>
</ol>
<p><img src="/images/diffusion_ddim/04.png" alt="Untitled"></p>
<ol start="3">
<li><strong>Interpolation in Deterministic Generative Processes</strong></li>
</ol>
<p>$x_0$ì˜ high level featureê°€ $x_T$ë¡œ encodingì´ ë˜ì–´ interpolationë„ ê°€ëŠ¥</p>
<ol start="4">
<li><strong>Reconstruction From Latent Space</strong></li>
</ol>
<h3>ğŸŒ í•œëˆˆì— ì •ë¦¬</h3>
<table>
<thead>
<tr>
<th>ë°©ë²•</th>
<th align="right">ìŠ¤í… ìˆ˜</th>
<th align="right">ê²°ì •ì„±</th>
<th>íŠ¹ì§•</th>
</tr>
</thead>
<tbody><tr>
<td>DDPM</td>
<td align="right">ë§ìŒ</td>
<td align="right">í™•ë¥ ì </td>
<td>ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦¼</td>
</tr>
<tr>
<td>DDIM</td>
<td align="right">ì ìŒ</td>
<td align="right">ê²°ì •ì </td>
<td>ë¹ ë¥´ê³  ì¼ê´€ì„± ìš°ìˆ˜</td>
</tr>
<tr>
<td>í˜¼í•©</td>
<td align="right">ê°€ë³€</td>
<td align="right">ë¶€ë¶„ ê²°ì •</td>
<td>í’ˆì§ˆ-ì†ë„ ì ˆì¶©</td>
</tr>
</tbody></table>
<blockquote>
<p>DDIMì€ <strong>í•™ìŠµì€ ê·¸ëŒ€ë¡œ</strong> ë‘ê³  <strong>ìƒ˜í”Œë§ë§Œ</strong> êµì²´í•´,<br>í’ˆì§ˆ Ã— ì†ë„ë¥¼ ëª¨ë‘ ì±™ê¸¸ ìˆ˜ ìˆëŠ” ì‹¤ìš©ì  ì ‘ê·¼ì´ë‹¤.</p>
</blockquote>

    <section class="related">
        <div class="related-head">
          <h2>Related posts</h2>
          <a class="all-link" href="../../">Browse all articles&nbsp;â†’</a>
        </div>
        <div class="related-list">
      <a class="card mini" href="../../posts/difussion_latent_diffusion/">
        <img class="thumb" src="../..//images/difussion_latent_diffusion/thumbnail.png" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Difussion : Latent Diffusion</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Feb 7, 2023 Â· 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/Diffusion_DDPM/">
        <img class="thumb" src="../..//images/diffusion_ddpm/thumbnail.jpg" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Diffusion : DDPM</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Jan 22, 2023 Â· 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/Diffusion_kickoff/">
        <img class="thumb" src="../..//images/diffusion_kickoff/thumbnail.png" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Diffusion : Kick-off</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Jan 15, 2023 Â· 15 min</div>
        </div>
      </a></div>
      </section> 
  </main>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
<script>
window.MathJax = {
  tex: {
    inlineMath:  [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },

  chtml: {
    linebreaks: { automatic: true, width: "match" }  // ë˜ëŠ” width: 80
  },

  options: {
    renderActions: { addMenu: [] }   // ìš°í´ë¦­ ë©”ë‰´ ì œê±°
  }
};
</script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script src="../../assets/js/main.js" defer></script>
</body></html>