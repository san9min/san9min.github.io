<!DOCTYPE html>
<html lang="ko"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Sangmin Blog | Diffusion : DDIM</title>
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css">
  <link rel="stylesheet" href="../../assets/styles/main.css">
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <link rel="stylesheet"
     href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
<link rel="icon" type="image/png" href="/assets/favicon.png" sizes="32x32">
</head><body>
  <main class="article">
    <h1>Diffusion : DDIM</h1>
    <div class="meta">Jan 31, 2023 Â· 20 min read</div>
    <img class="hero" src="../../images/diffusion_ddim/thumbnail.png" alt="cover image">
    <h2><strong>Denoising Diffusion Implicit Models</strong></h2>
<p><a href="https://arxiv.org/abs/2010.02502">Denoising Diffusion Implicit Models</a></p>
<ul>
<li>Deterministic sampling</li>
<li><code>Fast sampling</code></li>
</ul>
<aside>
ğŸ’¡ í•™ìŠµì€ DDPMìœ¼ë¡œ(for ì—¬ëŸ¬ step), ìƒ˜í”Œë§ì€ DDIMìœ¼ë¡œ
â‡’ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸, ë¹ ë¥¸ ìƒ˜í”Œë§

</aside>

<p>DDPMì˜ Markovianì´ì—ˆë˜ forward diffusion processë¥¼ <code>non-Markcovian</code> formìœ¼ë¡œ <code>generalize</code>í•˜ê³ , reverese processëŠ” ì§§ì€ Markov chainìœ¼ë¡œ ì„¤ê³„í•´ì„œ ë” ë¹ ë¥´ê²Œ samplingì„ í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤. </p>
<aside>
âš ï¸ Notation) DDIMì—ì„œëŠ” DDPMì˜ $\bar\alpha$ë¥¼ $\alpha$ë¼ê³  ì”€, ë‚œ DDPMì˜ notationì„ ë”°ë¥¼ê²ƒ

</aside>

<p>DDPMì„ remindí•´ë³´ë©´ variational lower boundë¥¼ maximizeí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí–ˆì—ˆê³ , Gaussian transitionì„ í•˜ëŠ” Markov chainì„ ìƒê°í•´ forward processë¥¼ formulateí–ˆê³  ì´ ì—­ê³¼ì •ì€ intractableí•´ì„œ neural networkë¥¼ ì‚¬ìš©í–ˆì—ˆë‹¤.</p>
<p>ê·¸ë¦¬ê³  ì‹ì„ Reparmeterizeí•´ì„œ $x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$ ë¡œ $x_t$ë¥¼ $x_0$ ì™€ noiseì˜ linear combinationìœ¼ë¡œ ì“¸ ìˆ˜ ìˆì—ˆê³  objective ë˜í•œ noiseë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ë§Œë“¤ì—ˆì—ˆë‹¤.</p>
<p>$$
L_\gamma (\epsilon_{\theta}) = \sum_{t=1}^T \gamma_t  \mathbb E_{x_0 \sim q(x_0), \epsilon_t \sim N(0,I) }[||\epsilon_\theta ^{(t)}(\sqrt{\bar \alpha_t}x_0 +\sqrt{1-\bar{\alpha_t}}\epsilon_t)-\epsilon_t||^2_2]
$$</p>
<p>ì´ ì‹ì˜ $\gamma$ =1 ì¼ ë•Œì˜€ë‹¤.( <em>where</em> $\epsilon_\theta = {\epsilon_\theta^{(t)}}^{T}_{t=1}$ , $\gamma = [\gamma _1,\cdots,\gamma _T]$ )</p>
<p>ê·¸ëŸ° T ê°€ ì¶©ë¶„íˆ ì»¤ì•¼í•˜ê³  sequentialí•˜ê²Œ ê³„ì† ê³„ì‚°(iterations)ì„ ë–„ë ¤ì•¼í•˜ê¸° ë•Œë¬¸ì— computaitional costê°€ ë§¤ìš° ë†’ë‹¤.</p>
<h3>Variational inference for Non-Markovian Forward Processes</h3>
<p>ê²°êµ­ ìš°ë¦¬ì˜ generative modelì€ reverse ë¥¼ approximateì„ ì˜í•´ë³´ìë¼ëŠ” ê²ƒ.</p>
<p>ê·¸ë˜ì„œ iterationì˜ ìˆ˜ë¥¼ ì¤„ì´ê³ ìí•˜ëŠ” ì˜ì§€ì™€ í•¨ê»˜ ì‹ì„ ëœ¯ì–´ ê³ ì³ë³´ì. ìœ„ì˜ objective functionì„ ë³´ë©´ ìš°ë¦¬ê°€ ddpmì—ì„œ reparametrizationì„ í†µí•´ ë³¼ ìˆ˜ ìˆë“¯ joint distribution $q(x_{1:T}|x_0)$ ê°€ ì•„ë‹ˆë¼ $q(x_t|x_0)$ì—ë§Œ ì§ì ‘ì ì¸ dependencyê°€ ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê°™ì€ obejctiveë¥¼ ê°–ê²Œ í•˜ê¸° ìœ„í•´,â­â­ <code>$q(x_t|x_0)$ë§Œ ë§Œì¡±</code>â­â­í•˜ë©´ë˜ê³  ì´ë¥¼ ë§Œì¡±í•˜ëŠ” jointëŠ” ë§ê¸°ì— forward processë¥¼ non-Markovian ìœ¼ë¡œ ë°”ê¿”ë³´ì(í˜¹ì€ ì¼ë°˜í™”í•´ë³´ì).</p>
<ol>
<li><strong>Non-Markovian Forward Processes</strong></li>
</ol>
<p>inference distributionë“¤ì„ ëª¨ì•„ë…¼ Që¥¼ ìƒê°í•´ë³´ì. ì´ë¥¼ real vector $\sigma$ë¡œ inference distbì„ indexingì„ í•´ì„œ forward processë¥¼ inference distbë¡œ í‘œí˜„í•˜ë©´</p>
<p>$$
q_\sigma (x_{1:T}|x_0) = q_{\sigma}(x_T|x_0) \Pi <em>{t=2}^T q</em>{\sigma}(x_{t-1}|x_t,x_0)
$$</p>
<p>ì´ê³  ì—¬ê¸°ì„œ $q(x_T|x_0) = N(\sqrt{\bar \alpha_T}x_0, (1-\bar{\alpha_T})I)$ë¼í•˜ê³  t &gt; 1ë³´ë‹¤ í´ ë•Œ</p>
<p>â­â­â­</p>
<p><strong>Def ) Reverse Conditional Distribution</strong></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/75f54a8d-91c0-48aa-9dfb-955c0a5c1e26/Untitled.png" alt="Untitled"></p>
<p>â­â­â­</p>
<p>ë¡œ ì¨ì„œ  <code>ëª¨ë“  tì— ëŒ€í•´ì„œ $q(x_t|x_0) = N(\sqrt{\bar \alpha_t}x_0, (1-\bar{\alpha_t})I)$ë¥¼ ë§Œì¡±í•˜ë„ë¡ formulate</code>í–ˆë‹¤ëŠ” ê²ƒ( DDPMê³¼ ê°™ë„ë¡).</p>
<p>ì´ forward processë¥¼ Bayes ruleë¡œ ë‹¤ì‹œ ì“°ë©´</p>
<p>$$
q_{\sigma}(x_t|x_{t-1},x_0) = \frac {q_{\sigma}(x_t,x_0)q_\sigma(x_t|x_0)}{q_{\sigma}(x_{t-1}|x_0)}
$$</p>
<p>ì´ ì‹ì„ ë³´ë©´  $x_t$ê°€ $x_{t-1}$ ë¿ë§Œì•„ë‹ˆë¼ $x_0$ì—ë„ ì˜ì¡´í•˜ë¯€ë¡œ ë”ì´ìƒ Markovianì´ ì•„ë‹ˆë‹¤. </p>
<aside>
ğŸ’¡ $\sigma$ì˜ í¬ê¸°ê°€ forward processê°€ ì–¼ë§ˆë‚˜ stochasticí•œì§€ë¥¼ ê²°ì •í•œë‹¤.
`$\sigma$ â†’ 0`ì´ë©´ $x_0$ ì™€ $x_t$ê°€ ì£¼ì–´ì§€ë©´ ë°”ë¡œ `$x_{t-1}$ì´ determine` ëœë‹¤. 
ì¦‰ $\sigma$ê°€ 0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ deterministicí•´ì§

</aside>

<p>ì •ë¦¬í•˜ë©´ DDPMê³¼ DDIMì€ forward processì—ì„œ  <code>$q(x_t|x_0)$ ëŠ” ê°™ê²Œ</code> ë‘ê³  ë‹¤ë§Œ  <code>joint distributionì„ ë‹¤ë¥´ê²Œ</code> ê°€ì ¸ê°”ë‹¤.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8bacd29c-2ea0-40d4-b082-4ad1a808e100/Untitled.png" alt="Untitled"></p>
<ol>
<li><strong>Generative Process And Unified Variational Inference Objective</strong></li>
</ol>
<p><strong>Goal</strong> $p_\theta (x_{0:T})$</p>
<p>Generation ì¸¡ë©´ì—ì„œ <code>$x_t$â†’ $x_{t-1}$</code>ë¡œ ê°€ëŠ” processê°€ ê¶ê¸ˆí•˜ê³ , $q_{\sigma}(x_{t-1}|x_t,x_0)$ë¥¼ ì´ìš©í•´ $p_\theta ^t (x_{t-1}|x_t)$ë¥¼ defineí•´ë³´ì</p>
<aside>
ğŸª§ 1. $x_t$ ê°€ ì£¼ì–´ì§€ë©´ $x_0$ë¥¼ ì˜ˆì¸¡  by $f_\theta$
2. $q_{\sigma}(x_{t-1}|x_t,x_0)$ë¥¼ ì´ìš©í•´ $x_{t-1}$ obtain

</aside>

<p>$x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$ ë¥¼ ì´ìš©í•´ ëª¨ë¸ì´ epsilon <code>noise</code>ì„ ì˜ˆì¸¡í•´, <code>$x_0$</code> ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ì˜ˆì¸¡í•˜ëŠ” f ë„ì…</p>
<p>$$
f_{\theta}^{(t)}(x_t) = (x_t -\sqrt{1-\bar{\alpha_t}}\epsilon_{\theta}^{(t)}(x_t))/\sqrt {\alpha_t} \approx x_0
$$</p>
<p>ê°€ ë˜ê³  ìš°ë¦¬ ëª¨ë¸ì€</p>
<p>â­â­â­</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/49d32062-0fc9-468e-b667-9c6cc2c084c2/Untitled.png" alt="Untitled"></p>
<p>â­â­â­</p>
<p>ì´ê³  objective $J_\sigma(\epsilon_\theta)$ ëŠ” $\epsilon_{\theta}$ì˜ í•¨ìˆ˜ê°€ ëœë‹¤. ë˜í•œ objectiveê°€ $\sigma$ì— ëŒ€í•œ dependencyê°€ ìˆìœ¼ë¯€ë¡œ ê° $\sigma$ì—ëŒ€í•´ ë”°ë¡œ í•™ìŠµì„ í•´ì£¼ì–´ì•¼í•œë‹¤. ê·¸ëŸ°ë° $J_\sigma$ëŠ” ì–´ë–¤ $\gamma$ì—ëŒ€í•´ $L_\gamma$ì™€ ê°™ë‹¤ê³  í•œë‹¤.</p>
<p>$$
\text{Theorem 1)} \forall \sigma&gt;0, there ; exists ; \gamma \in \mathbb R^T_{&gt;0} ; and ; C \in \mathbb R \quad s.t.; J_\sigma = L_\gamma + C
$$</p>
<p>ì—¬ê¸°ì„œ $L_\gamma (\epsilon_{\theta}) = \sum_{t=1}^T \gamma_t \mathbb E_{x_0 \sim q(x_0), \epsilon_t \sim N(0,I) }[||\epsilon_\theta ^{(t)}(\sqrt{\bar \alpha_t}x_0 +\sqrt{1-\bar{\alpha_t}}\epsilon_t)-\epsilon_t||^2_2]$ë¥¼ ë‹¤ì‹œ ë³´ì. </p>
<p>ë§Œì•½ $\epsilon_{\theta}^t$ê°€ ì„œë¡œë‹¤ë¥¸ t ë¼ë¦¬ parameterë¥¼ ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´!! , ì „ì²´ë¥¼ maximizeí•˜ê¸° ìœ„í•´ì„  ìš°ë¦¬ëŠ” ê° tì— ëŒ€í•œ termë“¤ì„ ê°ê° maximizeí•´ì•¼ë˜ê³  ê·¸ ì–˜ê¸°ëŠ” weight factor <code>$\gamma$ì™€ ë¬´ê´€</code>í•˜ê²Œ optimizationì´ ì§„í–‰ëœë‹¤ëŠ” ê²ƒ. ê·¸ëŸ¬ë¯€ë¡œ optimize(objective)ê´€ì ì—ì„œ $\gamma$ëŠ” arbitraryí•˜ê²Œ ì¡ì•„ë„ ë˜ê³ , ì´ë¥¼ 1ë¡œ ì¡ì•„ë„ Okì´ë‹¤. ê·¸ëŸ°ë° theorem 1ì— ì˜í•˜ë©´ ì–´ë–¤ $L_\gamma$ ëŠ” $J_\sigma$ì™€ ê°™ì€ objectiveë¥¼ ê°–ìœ¼ë¯€ë¡œ $L_1$ì„  $J_\sigma$ ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p>
<aside>
ğŸ’¡ ë§Œì•½ ëª¨ë¸ $\epsilon_\theta$ì˜ paramterê°€ ì„œë¡œë‹¤ë¥¸ të¼ë¦¬ ê³µìœ í•˜ì§€ ì•ŠëŠ” êµ¬ì¡°ë©´
 `$J_\sigma$ì˜ objectiveë¡œ $L_1$ì„ ì¨ë„ Ok`.

</aside>

<h3>Sampling From Generalized Generative Processes</h3>
<p>ìš°ë¦¬ëŠ” generalí•œ objectiveë¥¼ $L_1$ì´ ëŒ€ì²´ê°€ëŠ¥í•¨ì„ ë³´ì˜€ê³ , ê·¸ë˜ì„œ markovianì˜ forward processì™€ non markovianì˜ forward process ëª¨ë‘ì˜ objectiveì´ë¯€ë¡œ pretrained DDPMì„ ì‚¬ìš©í•´ë„ ì¢‹ë‹¤.</p>
<p>ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” $\sigma$ì— ë”°ë¥¸ sampling ì— ì§‘ì¤‘í•´ë³¼ê²ƒ </p>
<ol>
<li><strong>Denoising Diffusion Implicit Models</strong></li>
</ol>
<p>ìš°ë¦¬ëŠ” ìœ„ì˜ $p_\theta$ë¡œ $x_t$ë¡œë¶€í„° $x_{t-1}$ì„ generateí•  ìˆ˜ ìˆê²Œ ëë‹¤.</p>
<p>â­â­â­â­â­</p>
<p>$$
x_{t-1} = \sqrt{\bar \alpha_{t-1}}(\frac{x_t-\sqrt{1-\bar \alpha_{t}}\epsilon_{\theta}^{(t)}(x_t)}{\sqrt{\bar \alpha_{t}}}) +\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot \epsilon_{\theta}^{(t)}(x_t) + \sigma_t\epsilon_t
$$</p>
<p>1st term</p>
<p>$(\frac{x_t-\sqrt{1-\bar \alpha_{t}}\epsilon_{\theta}^{(t)}(x_t)}{\sqrt{\bar \alpha_{t}}})$  : predicted $x_0$ </p>
<p>2nd term</p>
<p>$\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot \epsilon_{\theta}^{(t)}(x_t)$ : direction pointing to $x_t$</p>
<p>3rd term</p>
<p>random noise independent of $x_t$</p>
<p>â­â­â­â­â­</p>
<p>ì—¬ê¸°ì„œ $\sigma_t$ë¥¼ ì–´ë–»ê²Œ ì¡ëŠëƒì— ë”°ë¼ ë§¤ìš° í¥ë¯¸ë¡œìš´ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.</p>
<p>case1) $\sigma_t = \sqrt{(1-\bar\alpha_{t-1})/(1-\bar\alpha_t)}\sqrt{1-\bar\alpha_t/\alpha_{t-1}}$</p>
<p> forward processê°€ Markovianì´ ë˜ê³   DDPMìœ¼ë¡œ reduceëœë‹¤.</p>
<p>case2) <code>$\sigma_t$ = 0 for all t</code></p>
<p>forward processê°€ <code>deterministic</code>í•´ì§„ë‹¤(t=1 ì¼ë•Œ ì œì™¸), ë˜í•œ generative processì—ì„œë„ noiseì˜ ê³„ìˆ˜ê°€ 0ì´ ë˜ì–´ë²„ë ¤ ë§ˆì°¬ê°€ì§€</p>
<p>â‡’ <code>DDIM</code></p>
<p>forward processê°€ ë”ì´ìƒ diffusionì´ ì•„ë‹ˆì§€ë§Œ, <code>DDPMì˜ objectiveë¡œ í•™ìŠµì´ëœ implicit model</code></p>
<p>implicit probablisticì´ë¼ í•˜ëŠ” ì´ìœ ëŠ” sampleë“¤ì´ latent variableì¸ $x_t$ì— ì˜í•´ ìƒì„±ë˜ì—ˆê¸° ë•Œë¬¸</p>
<ol>
<li><strong>Accelerated Generation Processes</strong></li>
</ol>
<p>ë‹¤ì‹œ ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ë…¼ë¦¬ë¥¼ ì´ì•¼ê¸°í•˜ë©´ $L_1$ì˜ objectiveê°€ ì–´ë– í•œ íŠ¹ì • forward process(joint distb)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  â­â­ <code>$q(x_t|x_0)$ë§Œ ë§Œì¡±</code>â­â­í•˜ë©´ ëë‹¤.</p>
<p>ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ê¸°ì¡´ ì „ì²´ forward processì˜ length of time step Të³´ë‹¤ ë” ì‘ê²Œ  ì¼ë¶€ ëª‡ ê°œì˜ forward stepì— ëŒ€í•´ì„œë§Œ forward processë¥¼ ì§„í–‰í•˜ê³  ì´ë“¤ì˜ subsetì„ ê°–ê³  generative processë¥¼ ì§„í–‰í•´ë„ ì¢‹ë‹¤.</p>
<p>â‡’ pretrainëœ DDPMì„ í™œìš©í•´ generative processë¥¼ ëŒë ¤ë„ ë˜ê³  ì˜¤íˆë ¤ ì¢‹ë‹¤ëŠ” ì´ì•¼ê¸°</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/190ddad9-a7bc-452c-bd2e-a1b005402c8a/Untitled.png" alt="Untitled"></p>
<p>$\tau = [1,3]$, $\tau$ëŠ” (sampling) trajectoryì´ê³  [1,2,3,â€¦,T]ì˜ subsequenceì´ë‹¤. lengthë¥¼ Së¼ê³  denoteí•˜ê³ , ì´ëŠ” DDIMì˜ sampling stepë“¤ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸</p>
<ol>
<li><strong>Relevance to Nueral ODE</strong></li>
</ol>
<p>DDIM(with sigma = 0)ì„ ODEë¡œ rewriteí•´ë³´ì.</p>
<p>$$
d\bar x(t) = \epsilon_\theta^{(t)}(\frac{\bar x(t)}{\sqrt{\sigma^2+1}})d\sigma(t)
$$</p>
<p>where I.C : $x(T) \sim N(0,\sigma (T))$</p>
<p> ì¶©ë¶„íˆ discretization stepì„ ê±°ì¹˜ë©´ ì´ ODEë¥¼ reverseí•´ì„œ generation processì˜ reverse, <code>encoding</code>ì´ ê°€ëŠ¥í•´ì§„ë‹¤.</p>
<aside>
â­ **`DDIM** sampleì˜ high level featureë“¤ì€ $x_T$ì— encodingëœë‹¤.`

</aside>

<h3>Experiments</h3>
<p>DDIMì€ DDPMë³´ë‹¤ í›¨ì”¬ ë” ì ì€ iterationìœ¼ë¡œ image generationì´ ê°€ëŠ¥í•˜ê³ , DDPMê³¼ëŠ” ë‹¬ë¦¬ <code>initial latent $x_T$ê°€ fixë˜ë©´ generation trajectoryì™€ ë¬´ê´€í•˜ê²Œ high level image featuresë“¤ì´ ìœ ì§€</code>ëœë‹¤.ê·¸ë˜ì„œ latent spaceìƒì—ì„œ ë°”ë¡œ interpolationì´ ê°€ëŠ¥í•˜ë‹¤. ë˜í•œ sampleë“¤ì„ encodingí•  ìˆ˜ ìˆì–´ latent codeì—ì„œ  sampleì„ reconstructí•  ìˆ˜ ìˆë‹¤.(DDIMì˜ deterministicí•œ ì„±ì§ˆ)</p>
<p>ë‹¤ë¥¸ ì¡°ê±´ì€ ë‹¤ ê°™ê²Œ ë‘ê³  $\tau$( (sampling) trajectory â†’ how fast samples are obtained) ì™€  $\sigma$ (DDIM = 0)ë§Œì„ ì¡°ì ˆí•˜ë©° samplingì— ì§‘ì¤‘í–ˆê³ ,  $\sigma$ë¥¼ í¸í•˜ê²Œ controlí•˜ê¸° ìœ„í•´ $\eta$ ë„ì…</p>
<p>â­â­â­</p>
<p>$\eta$ = 1.0 DDPM</p>
<p>$\eta$ = 0.0 DDIM</p>
<p>DDPMê³¼ DDIMì„ interpolate</p>
<p>$$
\sigma_{\tau_i} =\eta \sqrt{\frac{1-\bar\alpha_{\tau_{i-1}}}{1-\bar\alpha_{\tau_{i}}}}\sqrt{1-\frac{\bar\alpha_{\tau_i}}{\bar\alpha_{\tau_{i-1}}}}
$$</p>
<p>â­â­â­</p>
<ol>
<li><strong>Sample Quality and Efficiency</strong></li>
</ol>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3da138f4-0ecc-4d9a-838f-16719c426d44/Untitled.png" alt="Untitled"></p>
<ol>
<li><strong>Sample <code>Consistency</code> in DDIMs</strong></li>
</ol>
<p>â­â­â­DDIMì˜ generative processëŠ” deterministicí•˜ê³  $x_0$ëŠ” ì˜¤ì§ initial state $x_T$ì—ë§Œ ì˜ì¡´í•œë‹¤â­â­â­</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2dedb5eb-1aa6-4fe1-b6a9-44b84708ec15/Untitled.png" alt="Untitled"></p>
<ol>
<li><strong>Interpolation in Deterministic Generative Processes</strong></li>
</ol>
<p>$x_0$ì˜ high level featureê°€ $x_T$ë¡œ encodingì´ ë˜ì–´ interpolationë„ ê°€ëŠ¥</p>
<ol>
<li><strong>Reconstruction From Latent Space</strong></li>
</ol>
<hr>

    <section class="related">
        <div class="related-head">
          <h2>Related posts</h2>
          <a class="all-link" href="../../">Browse all articles&nbsp;â†’</a>
        </div>
        <div class="related-list">
      <a class="card mini" href="../../posts/difussion_latent_diffusion/">
        <img class="thumb" src="../..//images/difussion_latent_diffusion/thumbnail.png" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Difussion : Latent Diffusion</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Feb 7, 2023 Â· 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/Diffusion_DDPM/">
        <img class="thumb" src="../..//images/diffusion_ddpm/thumbnail.jpg" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Diffusion : DDPM</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Jan 22, 2023 Â· 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/Diffusion_kickoff/">
        <img class="thumb" src="../..//images/diffusion_kickoff/thumbnail.png" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Diffusion : Kick-off</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Jan 15, 2023 Â· 15 min</div>
        </div>
      </a></div>
      </section> 
  </main>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
<script>
window.MathJax = {
  tex: {
    inlineMath:  [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },

  chtml: {
    linebreaks: { automatic: true, width: "match" }  // ë˜ëŠ” width: 80
  },

  options: {
    renderActions: { addMenu: [] }   // ìš°í´ë¦­ ë©”ë‰´ ì œê±°
  }
};
</script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script src="../../assets/js/main.js" defer></script>
</body></html>