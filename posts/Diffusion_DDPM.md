---
title: "Diffusion : DDPM"
date: 2023-01-15
readingTime: 20 
thumbnail: images/Diffusion_DDPM/thumb.jpg
tags: [Generative AI, Diffusion, DDPM]
category : [Tech Papers Reivew]
---



## **Denoising Diffusion Probabilistic Models**

### DDPM

[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)

- parameterized Markov chain
- trained using variational inference
- `learn to reverse a diffusion process`

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1b107a88-b5c8-4fea-80df-50b6b903fdc2/Untitled.png)

Diffusion modelì€ latent variable ëª¨ë¸ì´ë‹¤.

**latent : a hidden continuous feature space**

**GOAL**

$$
p_{\theta} (x_0) = \int p_{\theta}(x_{0:T})dx_{1:T}
$$

$x_1,..x_T$ ëŠ” latentë“¤ì´ê³  data $x_0 \sim q(x_0)$ì™€ ê°™ì€ dimensionì„ ê°–ëŠ”ë‹¤.

> dataì— `noise`ë¥¼ ë”í•´ê°€ëŠ” ê²ƒì„ `forward process`
noiseë¡œ ë¶€í„° `de-noise`í•´ë‚˜ê°€ëŠ” `reverse process`ë¼í•œë‹¤.
> 

ìš°ë¦¬ modelì€ ì´ reverse processë¥¼ í•™ìŠµí•˜ê³  ìƒˆë¡œìš´ dataë¥¼ generationí•œë‹¤.

### Forward Process (diffusion process)

`forward processëŠ” Gaussian noiseë¥¼ ë”í•´ê°€ëŠ” ê³¼ì •`

ì´ëŠ” Markov chainìœ¼ë¡œ formulateí•  ìˆ˜ ìˆë‹¤. ì¦‰, 

$$
q(x_{1:T}|x_0) = \Pi_{t=1}^Tq(x_t|x_{t-1}) 
$$

by Markov chain with step T

Markov chainì€ ê° stepì€ ì˜¤ì§ ì§ì „ stepì—ë§Œ ì˜ì¡´í•¨ì„ ì˜ë¯¸í•œë‹¤.

$q(x_{1:T})$ëŠ” që¥¼ timestep 1ë¶€í„° Tê¹Œì§€ ë°˜ë³µí•´ì„œ ê°€í•¨ì„ ì˜ë¯¸í•˜ëŠ” notation

$\text {where}$

$$
â 
$$

ê° ìŠ¤í…ì—ì„œ Gaussian noiseë¥¼ ë”í•œë‹¤

ì—¬ê¸°ì„œ $\beta_t$ëŠ” variance scheduleì´ê³  Iê°€ identityì´ë¯€ë¡œ ê° dimensionì€ ê°™ì€ stdë¥¼ ê°–ëŠ”ë‹¤. ìƒìˆ˜ë¡œ ë‘¬ë„ ë˜ê³ , ì‹œê°„ì— ë”°ë¥¸ ë³€ìˆ˜ë¡œ ë‘ì–´ë„ ëœë‹¤. ë…¼ë¬¸ì—ì„  higer të¡œ ê°ˆìˆ˜ë¡ ì»¤ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ linearí•˜ê²Œ ë‘ì—ˆëŠ”ë° ë‹¤ë¥¸ ë…¼ë¬¸ì—ì„  cosine shcedule ì´ ì˜ ëë‹¤ê³  í•œë‹¤.

$\beta_t$ë¥¼ ì´ìš©í•´ scalingí•œ í›„ ë”í•´ì£¼ëŠ” ì´ìœ ëŠ” varianceê°€ divergeí•˜ëŠ” ê²ƒì„ ë§‰ê¸°ìœ„í•¨ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. 

<aside>
ğŸ’¡ Gaussian noiseë¥¼ ë”í•´ê°€ë©´, ìµœì¢… step (time T)ì—ì„œëŠ”standard normal prior $N(x_T;0,I)$ ê°€ë˜ê³  ê·¸ë˜ì„œ diffusionì´ë¼ í•œ ê²ƒê°™ë‹¤.

</aside>

ê·¸ëŸ°ë° ì—¬ê¸°ì„œ ì–´ë–¤ ìˆœê°„ t ( $0 \le t \le T)$ì—ì„œ $x_t$ë¥¼ ì•Œê³  ì‹¶ë‹¤ê³ í•œë‹¤ë©´, ìœ„ì˜ ì‹ì„ ì´ìš©í•´ ë°˜ë³µì ì¸ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ë©´ ëœë‹¤. ê·¸ëŸ¬ë‚˜ t ê°€ í¬ë‹¤ë©´ ì´ëŠ” ì¢‹ì€ ë°©ë²•ì´ ì•„ë‹ ê²ƒì´ë‹¤.

**Reparameterization trick (for sampling** $x_t$ **at once)**

ë§Œì•½ ìš°ë¦¬ê°€

$\alpha_t = 1-\beta_t , \bar \alpha_t = \Pi_{s=0}^t \alpha_s$ ë¼ê³  ì¡ëŠ”ë‹¤ë©´, tì—ì„œ $x_t$ë¥¼ samplingí•˜ëŠ” ê²ƒì„ closed formìœ¼ë¡œ ì“¸ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

â­â­â­

$$
q(x_t|x_0) = N(x_t;\sqrt {\bar \alpha_t} x_0, (1- {\bar \alpha_t})I)
$$

sample = mean + (var**0.5)*epsilon

â­â­â­

- **Proof )**
    
    let $\epsilon_0, \cdots \epsilon_{t-2},\epsilon_{t-1} \sim N(0,I)$
    
    $x_ t = \sqrt{(1-\beta_t)}x_{t-1} + \sqrt {\beta_t} \epsilon_{t-1} \\ =\cdots  \\=\sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon_0$
    
    ì¦ëª…ì˜ í•µì‹¬ë…¼ë¦¬ëŠ” ë‹¤ë¥¸ varianceë¥¼ ê°–ëŠ” ë‘ê°œ($\sigma_1^2,\sigma_2^2$)ì˜ Gaussiansì„ mergeí•´ ìƒˆë¡œìš´ distribution(with variance $\sigma_1^2+\sigma_2^2$ )ì„ ë§Œë“œëŠ” ê²ƒ
    

ì¢‹ë‹¤. 

í™•ì¸ì„ í•´ë³´ë©´ t â†’ $\infty$ë¡œ ê°ˆë•Œ $q(x_t|x_0)$ê°€ $N(x_t;0,I)$ë¡œ ê°ë„ ë³¼ ìˆ˜ ìˆë‹¤

ì´ì œ ìš°ë¦¬ëŠ” any timestep tì—ì„œ noiseë¥¼ samplingí•  ìˆ˜ ìˆê²Œëê³ , ì´ë¥¼ í†µí•´ $x_t$ë¥¼ $x_0$ì™€ $\epsilon$ì˜ í•¨ìˆ˜ë¡œ ë³¼ìˆ˜ ìˆê²Œ ë˜ì–´ forward processì—ì„œ $x_0$ë§Œ ì•Œë©´ ë°”ë¡œ  **$x_t$ë¥¼ ì–»ì„ ìˆ˜ ìˆê²Œëë‹¤.**

$$
x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon_0
$$

or

$$
x_0 = \frac{1}{\sqrt{\bar \alpha_t}}(x_t - \sqrt{1-\bar \alpha_t}\epsilon)
$$

### Reverse Process (denoising process)

`reverse processëŠ” neural networkì´ í•™ìŠµí•  ê³¼ì •`

$q(x_{t-1}|x_{t})$ë¥¼ ì›í•˜ë‚˜ ì–´ë ¤ì›Œì„œ neural networkë¥¼ ì´ìš©

forward processì˜ Gaussian noiseê°€ ì¶©ë¶„íˆì‘ì„ ë•Œ reverse process ë˜í•œ Gaussianì´ ë˜ê³  ì´ëŠ” neural networkë¥¼ ì´ìš©í•´ ê·¼ì‚¬ì‹œì¼œ meanê³¼ varianceë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

$$
p_{\theta}(x_{t-1}|x_t) = N(x_{t-1};\mu_{\theta}(x_t,t), \Sigma_{\theta}(x_t,t))
$$

Reverse processëŠ” $p(x_T) = N(x_T;0,I)$ë¶€í„° ì¶œë°œí•´ (learned) Gaussian trainsitionì„ í•˜ëŠ” Markov chainì´ë‹¤. ì¦‰, trajectoryëŠ”

$$
â
$$

ë¡œ fomulateí•  ìˆ˜ ìˆë‹¤.

<aside>
ğŸ’¡ neural networkì— timestep të¥¼ conditioningí•˜ë©´ modelì€ ê° time stepì˜ Gaussianì˜ meanê³¼ varianceë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œëœë‹¤.

</aside>

### Training

`ELBO`on the negative log likelihoodë¥¼ optimize

$$
E[-\log p_{\theta}(x_0)] \le E_q[-\log \frac {p_\theta (x_{0:T})}{q(x_{1:T}|x_0)}] \\ = L
$$

ì´ê³  ìœ„ ì‹ì„ ì •ë¦¬í•˜ë©´

$$
L = E_q[D_{KL}(q(x_T|x_0)||p(x_T))+\sum_{t>1}D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\log p_{\theta}(x_0|x_1)]
$$

ì´ ëœë‹¤.

<aside>
ğŸ’¡ $q(x_{t-1}|x_t)$ëŠ” intractableí•˜ì§€ë§Œ $x_0$ì˜ conditioningì„ ì£¼ë©´ tractableí•˜ë‹¤ê³  í•¨ â†’ generative modelì´ reverse diffusion stepìœ¼ë¡œ generationì„ í•˜ê¸° ìœ„í•´ì„  reference image $x_0$ê°€ í•„ìš”í•˜ë‹¤

</aside>

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d411bb47-dcf7-4b5a-afb1-89eb1e580dcc/Untitled.png)

> ë™í•˜ ì£¼)
> 
> - ìœ„ ì‹ ì¦ëª… ê³¼ì •
>     
>     $q(x_{t-1}|x_t)$ëŠ” ë¬´ì‹œê°€ëŠ¥í•´ì§. (Markov Processì´ê¸° ë•Œë¬¸)
>     
>     ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c052df61-f676-421a-b570-5f8733d6d4d0/Untitled.png)
>     

ê²°êµ­ í•˜ê³  ì‹¶ì€ ê²ƒì€ `$p_{\theta}(x_{t-1}|x_t)$ì™€ forward process posteriors $q(x_{t-1}|x_t,x_0)$ë¥¼ ë¹„êµ`í•˜ëŠ” ê²ƒì´ê³ 

$L_T = D_{KL}(q(x_T|x_0)||p(x_T))$ = const

$X_T$ê°€ ì–¼ë§ˆë‚˜ standard Gaussianì¸ì§€, ê·¸ëŸ°ë° ìš°ë¦¬ëŠ” $\beta_t$ë¥¼ ì‹œê°„ì— ë”°ë¥¸ constant ë¡œ ë‘ì—ˆìœ¼ë¯€ë¡œ ì´ termë„ constantì´ê³  í•™ìŠµí•  ë•Œ `ë¬´ì‹œ`í•´ë„ ëœë‹¤

$L_{T-1} = \sum_{t>1}D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))$

denoising step $p_{\theta}(x_{t-1}|x_t)$ ê³¼ approximated denoising step $q(x_{t-1}|x_t,x_0)$ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•¨ì„ ë³¼ ìˆ˜ ìˆìŒ

modelì´ `noiseë¥¼ ì˜ˆì¸¡`í•˜ë„ë¡ `Reparam`

1. $\Sigma_{\theta}(x_t,t) = \sigma_t^2I$, $\sigma$ëŠ” $\beta$ì— ê´€í•œ time dependent constants
2. $\mu_\theta(x_t,t)$ for $p_{\theta}(x_{t-1}|x_t)$ using  $x_t(x_0,\epsilon)$

â­â­â­

$$
p_\theta(x_{t-1}|x_t) = N(x_{t-1};\mu_{\theta}(x_t,t),\sigma_t^2I)
$$

$$
\mu_{\theta}(x_t,t) = \frac 1 {\sqrt {\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar \alpha_t}}\epsilon_{\theta}(x_t,t))
$$

**sample** $x_{t-1}$ = $\frac 1 {\sqrt {\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar \alpha_t}}\epsilon_{\theta}(x_t,t))$ + $\sigma_t$**z**,      **z** $\sim N(0,I)$

â­â­â­

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b1b82858-adcf-4e8f-8ee0-b4fb748bd636/Untitled.png)

â‡’`$\epsilon_{\theta}$ ëŠ” $x_t$ì™€ të¥¼ ë°›ê³  noiseë¥¼ ì˜ˆì¸¡`í•œë‹¤.

$x_t$ëŠ” $x_0$ë¡œ ë¶€í„° samplingì´ ê°€ëŠ¥í•˜ë„ë¡ ìœ„ì—ì„œ reparamí–ˆë‹¤.

$L_0 = -\log p_{\theta}(x_0|x_1)$

`Reconstruction term`

â­â­â­

**Simplified Loss**

$$
L_{simple}(\theta) = \mathbb E_{t,x_0,\epsilon} [||\epsilon -\epsilon_{\theta}(\sqrt{\bar \alpha_t}x_0  + \sqrt{1-\bar\alpha_t}\epsilon,t) ||^2]
$$

â­â­â­

t = 1 ì¼ ë•Œ

$L_0$ ì¦‰, $-\log p_{\theta}(x_0|x_1)$ë¥¼ minimize

t> 1ì¼ ë•Œ

$L_{t-1}$ì—ì„œ ì‹ ì •ë¦¬í•˜ë©´ ë‚˜ì˜¤ëŠ” coefficient $\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar\alpha_t)}$ë¥¼ ë²„ë ¤ì„œ higher noise level(higer t)ì— ì „(coefficient ê°€ ìˆì„ ë•Œ)ë³´ë‹¤ ë” í° weightë¥¼ ì£¼ê³  small tì— ëŒ€í•´ì„  ë” ì‘ì€ weightë¥¼ ì¤˜ì„œ ë” ì¢‹ì€ sample qualityë¥¼ ì–»ì—ˆë‹¤. (small tì—ì„  modelì´ ì‘ì€ ì–‘ì˜ noiseë§Œ denoiseí•˜ë„ë¡ í•™ìŠµì„ ì‹œí‚¤ê¸° ë•Œë¬¸, ê·¸ë˜ì„œ ë” ì–´ë ¤ìš´ large tì— ì§‘ì¤‘í•˜ë„ë¡ ë§Œë“¦)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9f379455-4bf7-4088-a3fa-49fa0bda4539/Untitled.png)

<aside>
ğŸ’¡ randomí•˜ê²Œ timesteps të¥¼ ë½‘ê³ , $x_0$ì™€ të¥¼ ì´ìš©í•´ $q(x_t|x_0)$ ë¡œë¶€í„° $x_t$ë¥¼ êµ¬í•¨
ì´ $x_t$ì™€ të¥¼ ìš°ë¦¬ ëª¨ë¸ì— ë„£ê³  epsilonì„ ë½‘ìŒ
ì´ epsilonê³¼ ($x_0$ì™€ ì •í™•íˆ ê°™ì€ dimensionì„ ê°–ëŠ”) noiseë¥¼ ë½‘ê³  MSE loss ë•Œë¦¬ë©´ ë¨

</aside>

### Model Architectue

modelì˜ inputê³¼ outputì˜ dimensionì´ ê°™ì•„ì•¼í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„  U-Netì„ ì‚¬ìš©í–ˆë‹¤. U-Netì€ Residual Block, self-attention blockì´ ìˆë‹¤.

diffusionì˜ timestep tê°€ position embeddingì„ í•œ í›„residual blockì— ì „ë‹¬ë˜ëŠ” ì‹ìœ¼ë¡œ ëª¨ë¸ì— tê°€ ì…ë ¥ëœë‹¤.

**U-Net**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/22fdc1ea-d205-47fe-9d1f-a9715d5f4c05/Untitled.png)

$\epsilon_{\theta}$ **model using U-Net**

input `$(x_t, t)$`

output `noise`

ì œê±°í•´ì•¼í•  noise

**Implementation Code**

```python
TODO
Skeleton code ì‘ì„±í•´ë³´ê¸°
```
