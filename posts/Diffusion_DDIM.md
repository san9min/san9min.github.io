---
title: "Diffusion : DDIM"
date: 2025-02-17
readingTime: 20 
thumbnail: images/post-002-thumb.jpg
tags: [Generative AI, Diffusion, DDPM]
category : [Tech Papers Reivew]
---

# **Denoising Diffusion Implicit Models**

## DDIM

[Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)

- Deterministic sampling
- `Fast sampling`

<aside>
ğŸ’¡ í•™ìŠµì€ DDPMìœ¼ë¡œ(for ì—¬ëŸ¬ step), ìƒ˜í”Œë§ì€ DDIMìœ¼ë¡œ
â‡’ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸, ë¹ ë¥¸ ìƒ˜í”Œë§

</aside>

DDPMì˜ Markovianì´ì—ˆë˜ forward diffusion processë¥¼ `non-Markcovian` formìœ¼ë¡œ `generalize`í•˜ê³ , reverese processëŠ” ì§§ì€ Markov chainìœ¼ë¡œ ì„¤ê³„í•´ì„œ ë” ë¹ ë¥´ê²Œ samplingì„ í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤. 

<aside>
âš ï¸ Notation) DDIMì—ì„œëŠ” DDPMì˜ $\bar\alpha$ë¥¼ $\alpha$ë¼ê³  ì”€, ë‚œ DDPMì˜ notationì„ ë”°ë¥¼ê²ƒ

</aside>

DDPMì„ remindí•´ë³´ë©´ variational lower boundë¥¼ maximizeí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí–ˆì—ˆê³ , Gaussian transitionì„ í•˜ëŠ” Markov chainì„ ìƒê°í•´ forward processë¥¼ formulateí–ˆê³  ì´ ì—­ê³¼ì •ì€ intractableí•´ì„œ neural networkë¥¼ ì‚¬ìš©í–ˆì—ˆë‹¤.

ê·¸ë¦¬ê³  ì‹ì„ Reparmeterizeí•´ì„œ $x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$ ë¡œ $x_t$ë¥¼ $x_0$ ì™€ noiseì˜ linear combinationìœ¼ë¡œ ì“¸ ìˆ˜ ìˆì—ˆê³  objective ë˜í•œ noiseë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ë§Œë“¤ì—ˆì—ˆë‹¤.

$$
L_\gamma (\epsilon_{\theta}) = \sum_{t=1}^T \gamma_t  \mathbb E_{x_0 \sim q(x_0), \epsilon_t \sim N(0,I) }[||\epsilon_\theta ^{(t)}(\sqrt{\bar \alpha_t}x_0 +\sqrt{1-\bar{\alpha_t}}\epsilon_t)-\epsilon_t||^2_2]
$$

ì´ ì‹ì˜ $\gamma$ =1 ì¼ ë•Œì˜€ë‹¤.( *where* $\epsilon_\theta = {\epsilon_\theta^{(t)}}^{T}_{t=1}$ , $\gamma = [\gamma _1,\cdots,\gamma _T]$ )

ê·¸ëŸ° T ê°€ ì¶©ë¶„íˆ ì»¤ì•¼í•˜ê³  sequentialí•˜ê²Œ ê³„ì† ê³„ì‚°(iterations)ì„ ë–„ë ¤ì•¼í•˜ê¸° ë•Œë¬¸ì— computaitional costê°€ ë§¤ìš° ë†’ë‹¤.

### Variational inference for Non-Markovian Forward Processes

ê²°êµ­ ìš°ë¦¬ì˜ generative modelì€ reverse ë¥¼ approximateì„ ì˜í•´ë³´ìë¼ëŠ” ê²ƒ.

ê·¸ë˜ì„œ iterationì˜ ìˆ˜ë¥¼ ì¤„ì´ê³ ìí•˜ëŠ” ì˜ì§€ì™€ í•¨ê»˜ ì‹ì„ ëœ¯ì–´ ê³ ì³ë³´ì. ìœ„ì˜ objective functionì„ ë³´ë©´ ìš°ë¦¬ê°€ ddpmì—ì„œ reparametrizationì„ í†µí•´ ë³¼ ìˆ˜ ìˆë“¯ joint distribution $q(x_{1:T}|x_0)$ ê°€ ì•„ë‹ˆë¼ $q(x_t|x_0)$ì—ë§Œ ì§ì ‘ì ì¸ dependencyê°€ ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê°™ì€ obejctiveë¥¼ ê°–ê²Œ í•˜ê¸° ìœ„í•´,â­â­ `$q(x_t|x_0)$ë§Œ ë§Œì¡±`â­â­í•˜ë©´ë˜ê³  ì´ë¥¼ ë§Œì¡±í•˜ëŠ” jointëŠ” ë§ê¸°ì— forward processë¥¼ non-Markovian ìœ¼ë¡œ ë°”ê¿”ë³´ì(í˜¹ì€ ì¼ë°˜í™”í•´ë³´ì).

1.  **Non-Markovian Forward Processes**

inference distributionë“¤ì„ ëª¨ì•„ë…¼ Që¥¼ ìƒê°í•´ë³´ì. ì´ë¥¼ real vector $\sigma$ë¡œ inference distbì„ indexingì„ í•´ì„œ forward processë¥¼ inference distbë¡œ í‘œí˜„í•˜ë©´

$$
q_\sigma (x_{1:T}|x_0) = q_{\sigma}(x_T|x_0) \Pi _{t=2}^T q_{\sigma}(x_{t-1}|x_t,x_0)
$$

ì´ê³  ì—¬ê¸°ì„œ $q(x_T|x_0) = N(\sqrt{\bar \alpha_T}x_0, (1-\bar{\alpha_T})I)$ë¼í•˜ê³  t > 1ë³´ë‹¤ í´ ë•Œ

â­â­â­

**Def ) Reverse Conditional Distribution**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/75f54a8d-91c0-48aa-9dfb-955c0a5c1e26/Untitled.png)

â­â­â­

ë¡œ ì¨ì„œ  `ëª¨ë“  tì— ëŒ€í•´ì„œ $q(x_t|x_0) = N(\sqrt{\bar \alpha_t}x_0, (1-\bar{\alpha_t})I)$ë¥¼ ë§Œì¡±í•˜ë„ë¡ formulate`í–ˆë‹¤ëŠ” ê²ƒ( DDPMê³¼ ê°™ë„ë¡).

ì´ forward processë¥¼ Bayes ruleë¡œ ë‹¤ì‹œ ì“°ë©´

$$
q_{\sigma}(x_t|x_{t-1},x_0) = \frac {q_{\sigma}(x_t,x_0)q_\sigma(x_t|x_0)}{q_{\sigma}(x_{t-1}|x_0)}
$$

ì´ ì‹ì„ ë³´ë©´  $x_t$ê°€ $x_{t-1}$ ë¿ë§Œì•„ë‹ˆë¼ $x_0$ì—ë„ ì˜ì¡´í•˜ë¯€ë¡œ ë”ì´ìƒ Markovianì´ ì•„ë‹ˆë‹¤. 

<aside>
ğŸ’¡ $\sigma$ì˜ í¬ê¸°ê°€ forward processê°€ ì–¼ë§ˆë‚˜ stochasticí•œì§€ë¥¼ ê²°ì •í•œë‹¤.
`$\sigma$ â†’ 0`ì´ë©´ $x_0$ ì™€ $x_t$ê°€ ì£¼ì–´ì§€ë©´ ë°”ë¡œ `$x_{t-1}$ì´ determine` ëœë‹¤. 
ì¦‰ $\sigma$ê°€ 0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ deterministicí•´ì§

</aside>

ì •ë¦¬í•˜ë©´ DDPMê³¼ DDIMì€ forward processì—ì„œ  `$q(x_t|x_0)$ ëŠ” ê°™ê²Œ` ë‘ê³  ë‹¤ë§Œ  `joint distributionì„ ë‹¤ë¥´ê²Œ` ê°€ì ¸ê°”ë‹¤.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8bacd29c-2ea0-40d4-b082-4ad1a808e100/Untitled.png)

1.  **Generative Process And Unified Variational Inference Objective**

**Goal** $p_\theta (x_{0:T})$

Generation ì¸¡ë©´ì—ì„œ `$x_t$â†’ $x_{t-1}$`ë¡œ ê°€ëŠ” processê°€ ê¶ê¸ˆí•˜ê³ , $q_{\sigma}(x_{t-1}|x_t,x_0)$ë¥¼ ì´ìš©í•´ $p_\theta ^t (x_{t-1}|x_t)$ë¥¼ defineí•´ë³´ì

<aside>
ğŸª§ 1. $x_t$ ê°€ ì£¼ì–´ì§€ë©´ $x_0$ë¥¼ ì˜ˆì¸¡  by $f_\theta$
2. $q_{\sigma}(x_{t-1}|x_t,x_0)$ë¥¼ ì´ìš©í•´ $x_{t-1}$ obtain

</aside>

$x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$ ë¥¼ ì´ìš©í•´ ëª¨ë¸ì´ epsilon `noise`ì„ ì˜ˆì¸¡í•´, `$x_0$` ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ì˜ˆì¸¡í•˜ëŠ” f ë„ì…

$$
f_{\theta}^{(t)}(x_t) = (x_t -\sqrt{1-\bar{\alpha_t}}\epsilon_{\theta}^{(t)}(x_t))/\sqrt {\alpha_t} \approx x_0
$$

ê°€ ë˜ê³  ìš°ë¦¬ ëª¨ë¸ì€

â­â­â­

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/49d32062-0fc9-468e-b667-9c6cc2c084c2/Untitled.png)

â­â­â­

ì´ê³  objective $J_\sigma(\epsilon_\theta)$ ëŠ” $\epsilon_{\theta}$ì˜ í•¨ìˆ˜ê°€ ëœë‹¤. ë˜í•œ objectiveê°€ $\sigma$ì— ëŒ€í•œ dependencyê°€ ìˆìœ¼ë¯€ë¡œ ê° $\sigma$ì—ëŒ€í•´ ë”°ë¡œ í•™ìŠµì„ í•´ì£¼ì–´ì•¼í•œë‹¤. ê·¸ëŸ°ë° $J_\sigma$ëŠ” ì–´ë–¤ $\gamma$ì—ëŒ€í•´ $L_\gamma$ì™€ ê°™ë‹¤ê³  í•œë‹¤.

$$
\text{Theorem 1)} \forall \sigma>0, there \; exists \; \gamma \in \mathbb R^T_{>0} \; and \; C \in \mathbb R \quad s.t.\; J_\sigma = L_\gamma + C
$$

ì—¬ê¸°ì„œ $L_\gamma (\epsilon_{\theta}) = \sum_{t=1}^T \gamma_t \mathbb E_{x_0 \sim q(x_0), \epsilon_t \sim N(0,I) }[||\epsilon_\theta ^{(t)}(\sqrt{\bar \alpha_t}x_0 +\sqrt{1-\bar{\alpha_t}}\epsilon_t)-\epsilon_t||^2_2]$ë¥¼ ë‹¤ì‹œ ë³´ì. 

ë§Œì•½ $\epsilon_{\theta}^t$ê°€ ì„œë¡œë‹¤ë¥¸ t ë¼ë¦¬ parameterë¥¼ ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´!! , ì „ì²´ë¥¼ maximizeí•˜ê¸° ìœ„í•´ì„  ìš°ë¦¬ëŠ” ê° tì— ëŒ€í•œ termë“¤ì„ ê°ê° maximizeí•´ì•¼ë˜ê³  ê·¸ ì–˜ê¸°ëŠ” weight factor `$\gamma$ì™€ ë¬´ê´€`í•˜ê²Œ optimizationì´ ì§„í–‰ëœë‹¤ëŠ” ê²ƒ. ê·¸ëŸ¬ë¯€ë¡œ optimize(objective)ê´€ì ì—ì„œ $\gamma$ëŠ” arbitraryí•˜ê²Œ ì¡ì•„ë„ ë˜ê³ , ì´ë¥¼ 1ë¡œ ì¡ì•„ë„ Okì´ë‹¤. ê·¸ëŸ°ë° theorem 1ì— ì˜í•˜ë©´ ì–´ë–¤ $L_\gamma$ ëŠ” $J_\sigma$ì™€ ê°™ì€ objectiveë¥¼ ê°–ìœ¼ë¯€ë¡œ $L_1$ì„  $J_\sigma$ ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

<aside>
ğŸ’¡ ë§Œì•½ ëª¨ë¸ $\epsilon_\theta$ì˜ paramterê°€ ì„œë¡œë‹¤ë¥¸ të¼ë¦¬ ê³µìœ í•˜ì§€ ì•ŠëŠ” êµ¬ì¡°ë©´
 `$J_\sigma$ì˜ objectiveë¡œ $L_1$ì„ ì¨ë„ Ok`.

</aside>

### Sampling From Generalized Generative Processes

ìš°ë¦¬ëŠ” generalí•œ objectiveë¥¼ $L_1$ì´ ëŒ€ì²´ê°€ëŠ¥í•¨ì„ ë³´ì˜€ê³ , ê·¸ë˜ì„œ markovianì˜ forward processì™€ non markovianì˜ forward process ëª¨ë‘ì˜ objectiveì´ë¯€ë¡œ pretrained DDPMì„ ì‚¬ìš©í•´ë„ ì¢‹ë‹¤.

ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” $\sigma$ì— ë”°ë¥¸ sampling ì— ì§‘ì¤‘í•´ë³¼ê²ƒ 

1.  **Denoising Diffusion Implicit Models**

ìš°ë¦¬ëŠ” ìœ„ì˜ $p_\theta$ë¡œ $x_t$ë¡œë¶€í„° $x_{t-1}$ì„ generateí•  ìˆ˜ ìˆê²Œ ëë‹¤.

â­â­â­â­â­

$$
x_{t-1} = \sqrt{\bar \alpha_{t-1}}(\frac{x_t-\sqrt{1-\bar \alpha_{t}}\epsilon_{\theta}^{(t)}(x_t)}{\sqrt{\bar \alpha_{t}}}) +\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot \epsilon_{\theta}^{(t)}(x_t) + \sigma_t\epsilon_t
$$

1st term

$(\frac{x_t-\sqrt{1-\bar \alpha_{t}}\epsilon_{\theta}^{(t)}(x_t)}{\sqrt{\bar \alpha_{t}}})$  : predicted $x_0$ 

2nd term

$\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot \epsilon_{\theta}^{(t)}(x_t)$ : direction pointing to $x_t$

3rd term

random noise independent of $x_t$

â­â­â­â­â­

ì—¬ê¸°ì„œ $\sigma_t$ë¥¼ ì–´ë–»ê²Œ ì¡ëŠëƒì— ë”°ë¼ ë§¤ìš° í¥ë¯¸ë¡œìš´ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.

case1) $\sigma_t = \sqrt{(1-\bar\alpha_{t-1})/(1-\bar\alpha_t)}\sqrt{1-\bar\alpha_t/\alpha_{t-1}}$

 forward processê°€ Markovianì´ ë˜ê³   DDPMìœ¼ë¡œ reduceëœë‹¤.

case2) `$\sigma_t$ = 0 for all t`

forward processê°€ `deterministic`í•´ì§„ë‹¤(t=1 ì¼ë•Œ ì œì™¸), ë˜í•œ generative processì—ì„œë„ noiseì˜ ê³„ìˆ˜ê°€ 0ì´ ë˜ì–´ë²„ë ¤ ë§ˆì°¬ê°€ì§€

â‡’ `DDIM`

forward processê°€ ë”ì´ìƒ diffusionì´ ì•„ë‹ˆì§€ë§Œ, `DDPMì˜ objectiveë¡œ í•™ìŠµì´ëœ implicit model`

implicit probablisticì´ë¼ í•˜ëŠ” ì´ìœ ëŠ” sampleë“¤ì´ latent variableì¸ $x_t$ì— ì˜í•´ ìƒì„±ë˜ì—ˆê¸° ë•Œë¬¸

1.  **Accelerated Generation Processes**

ë‹¤ì‹œ ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ë…¼ë¦¬ë¥¼ ì´ì•¼ê¸°í•˜ë©´ $L_1$ì˜ objectiveê°€ ì–´ë– í•œ íŠ¹ì • forward process(joint distb)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  â­â­ `$q(x_t|x_0)$ë§Œ ë§Œì¡±`â­â­í•˜ë©´ ëë‹¤.

ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ê¸°ì¡´ ì „ì²´ forward processì˜ length of time step Të³´ë‹¤ ë” ì‘ê²Œ  ì¼ë¶€ ëª‡ ê°œì˜ forward stepì— ëŒ€í•´ì„œë§Œ forward processë¥¼ ì§„í–‰í•˜ê³  ì´ë“¤ì˜ subsetì„ ê°–ê³  generative processë¥¼ ì§„í–‰í•´ë„ ì¢‹ë‹¤.

â‡’ pretrainëœ DDPMì„ í™œìš©í•´ generative processë¥¼ ëŒë ¤ë„ ë˜ê³  ì˜¤íˆë ¤ ì¢‹ë‹¤ëŠ” ì´ì•¼ê¸°

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/190ddad9-a7bc-452c-bd2e-a1b005402c8a/Untitled.png)

$\tau = [1,3]$, $\tau$ëŠ” (sampling) trajectoryì´ê³  [1,2,3,â€¦,T]ì˜ subsequenceì´ë‹¤. lengthë¥¼ Së¼ê³  denoteí•˜ê³ , ì´ëŠ” DDIMì˜ sampling stepë“¤ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸

1.  **Relevance to Nueral ODE**

DDIM(with sigma = 0)ì„ ODEë¡œ rewriteí•´ë³´ì.

$$
d\bar x(t) = \epsilon_\theta^{(t)}(\frac{\bar x(t)}{\sqrt{\sigma^2+1}})d\sigma(t)
$$

where I.C : $x(T) \sim N(0,\sigma (T))$

 ì¶©ë¶„íˆ discretization stepì„ ê±°ì¹˜ë©´ ì´ ODEë¥¼ reverseí•´ì„œ generation processì˜ reverse, `encoding`ì´ ê°€ëŠ¥í•´ì§„ë‹¤.

<aside>
â­ **`DDIM** sampleì˜ high level featureë“¤ì€ $x_T$ì— encodingëœë‹¤.`

</aside>

### Experiments

DDIMì€ DDPMë³´ë‹¤ í›¨ì”¬ ë” ì ì€ iterationìœ¼ë¡œ image generationì´ ê°€ëŠ¥í•˜ê³ , DDPMê³¼ëŠ” ë‹¬ë¦¬ `initial latent $x_T$ê°€ fixë˜ë©´ generation trajectoryì™€ ë¬´ê´€í•˜ê²Œ high level image featuresë“¤ì´ ìœ ì§€`ëœë‹¤.ê·¸ë˜ì„œ latent spaceìƒì—ì„œ ë°”ë¡œ interpolationì´ ê°€ëŠ¥í•˜ë‹¤. ë˜í•œ sampleë“¤ì„ encodingí•  ìˆ˜ ìˆì–´ latent codeì—ì„œ  sampleì„ reconstructí•  ìˆ˜ ìˆë‹¤.(DDIMì˜ deterministicí•œ ì„±ì§ˆ)

ë‹¤ë¥¸ ì¡°ê±´ì€ ë‹¤ ê°™ê²Œ ë‘ê³  $\tau$( (sampling) trajectory â†’ how fast samples are obtained) ì™€  $\sigma$ (DDIM = 0)ë§Œì„ ì¡°ì ˆí•˜ë©° samplingì— ì§‘ì¤‘í–ˆê³ ,  $\sigma$ë¥¼ í¸í•˜ê²Œ controlí•˜ê¸° ìœ„í•´ $\eta$ ë„ì…

â­â­â­

$\eta$ = 1.0 DDPM

$\eta$ = 0.0 DDIM

DDPMê³¼ DDIMì„ interpolate

$$
\sigma_{\tau_i} =\eta \sqrt{\frac{1-\bar\alpha_{\tau_{i-1}}}{1-\bar\alpha_{\tau_{i}}}}\sqrt{1-\frac{\bar\alpha_{\tau_i}}{\bar\alpha_{\tau_{i-1}}}}
$$

â­â­â­

1.  **Sample Quality and Efficiency**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3da138f4-0ecc-4d9a-838f-16719c426d44/Untitled.png)

1.  **Sample `Consistency` in DDIMs**

â­â­â­DDIMì˜ generative processëŠ” deterministicí•˜ê³  $x_0$ëŠ” ì˜¤ì§ initial state $x_T$ì—ë§Œ ì˜ì¡´í•œë‹¤â­â­â­

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2dedb5eb-1aa6-4fe1-b6a9-44b84708ec15/Untitled.png)

1.  **Interpolation in Deterministic Generative Processes**

$x_0$ì˜ high level featureê°€ $x_T$ë¡œ encodingì´ ë˜ì–´ interpolationë„ ê°€ëŠ¥

1.  **Reconstruction From Latent Space**

---