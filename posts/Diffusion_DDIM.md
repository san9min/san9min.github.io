---
title: "Diffusion : DDIM"
date: 2023-01-31
readingTime: 20 
thumbnail: images/diffusion_ddim/thumbnail.png
tags: [Generative AI, Diffusion, DDPM]
category : [Tech Review]
---

## ğŸš€ **Denoising Diffusion Implicit Models**


[Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)

- Deterministic sampling
- `Fast sampling`

<aside>
DDPMìœ¼ë¡œ ì—¬ëŸ¬ stepí•˜ê³ , DDIMìœ¼ë¡œ ë¹ ë¥´ê²Œ ìƒ˜í”Œë§í•œë‹¤.
â‡’ ë†’ì€ í’ˆì§ˆê³¼ ë¹ ë¥¸ ìƒ˜í”Œë§
</aside>

DDPMì˜ Markovianì´ì—ˆë˜ forward diffusion processë¥¼ `non-Markcovian` formìœ¼ë¡œ ì¼ë°˜í™”í•˜ê³ , reverese processëŠ” ì§§ì€ Markov chainìœ¼ë¡œ ì„¤ê³„í•´ì„œ ë” ë¹ ë¥´ê²Œ samplingì„ í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤. 

<aside>
DDIMì—ì„œëŠ” DDPMì˜ $\bar\alpha$ë¥¼ $\alpha$ë¼ê³  ì¼ë‹¤.  

ë‚œ DDPMì˜ notationì„ ë”°ë¥¸ë‹¤.
</aside>

### ğŸ“ DDPM Remind

DDPMì„ remindí•´ë³´ë©´ variational lower boundë¥¼ maximizeí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.  

Gaussian transitionì„ í•˜ëŠ” Markov chainì„ ìƒê°í•´ forward processë¥¼ formulateí–ˆê³  ì´ ì—­ê³¼ì •ì€ intractableí•´ì„œ neural networkë¥¼ ì‚¬ìš©í–ˆì—ˆë‹¤.

ê·¸ë¦¬ê³  ì‹ì„ Reparmeterizeí•´ì„œ $x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$ ë¡œ $x_t$ë¥¼ $x_0$ ì™€ noiseì˜ linear combinationìœ¼ë¡œ ì“¸ ìˆ˜ ìˆì—ˆë‹¤. 

ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ëŠ” noiseë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí–ˆì—ˆë‹¤.

$$
L_\gamma (\epsilon_{\theta}) = \sum_{t=1}^T \gamma_t  \mathbb E_{x_0 \sim q(x_0), \epsilon_t \sim N(0,I) }[||\epsilon_\theta ^{(t)}(\sqrt{\bar \alpha_t}x_0 +\sqrt{1-\bar{\alpha_t}}\epsilon_t)-\epsilon_t||^2_2]
$$

ì´ ì‹ì˜ $\gamma$ =1 ì¼ ë•Œì˜€ë‹¤.( *where* $\epsilon_\theta = {\epsilon_\theta^{(t)}}^{T}_{t=1}$ , $\gamma = [\gamma _1,\cdots,\gamma _T]$ )

ê·¸ëŸ° T ê°€ ì¶©ë¶„íˆ ì»¤ì•¼í•˜ê³  sequentialí•˜ê²Œ ê³„ì† ê³„ì‚°(iterations)ì„ ë–„ë ¤ì•¼í•˜ê¸° ë•Œë¬¸ì— computaitional costê°€ ë§¤ìš° ë†’ë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤.


### ğŸ§© Variational inference for Non-Markovian Forward Processes

ê²°êµ­ ìš°ë¦¬ì˜ generative modelì€ reverse ë¥¼ approximateì„ ì˜í•´ë³´ìë¼ëŠ” ê²ƒ.

ê·¸ë˜ì„œ iterationì˜ ìˆ˜ë¥¼ ì¤„ì´ê³ ìí•˜ëŠ” ì˜ì§€ì™€ í•¨ê»˜ ì‹ì„ ëœ¯ì–´ ê³ ì³ë³´ì. ìœ„ì˜ objective functionì„ ë³´ë©´ ìš°ë¦¬ê°€ ddpmì—ì„œ reparametrizationì„ í†µí•´ ë³¼ ìˆ˜ ìˆë“¯ joint distribution $q(x_{1:T}|x_0)$ ê°€ ì•„ë‹ˆë¼ $q(x_t|x_0)$ì—ë§Œ ì§ì ‘ì ì¸ dependencyê°€ ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤.  

ê°™ì€ obejctiveë¥¼ ê°–ê²Œ í•˜ê¸° ìœ„í•´,$q(x_t|x_0)$ë§Œ ë§Œì¡±í•˜ë©´ë˜ê³  ì´ë¥¼ ë§Œì¡±í•˜ëŠ” jointëŠ” ë§ê¸°ì— forward processë¥¼ non-Markovian ìœ¼ë¡œ ë°”ê¿” ì¼ë°˜í™”í•´ë³´ì.

#### 1ï¸âƒ£ **Non-Markovian Forward Processes**

inference distributionë“¤ì„ ëª¨ì•„ë…¼ Që¥¼ ìƒê°í•´ë³´ì. ì´ë¥¼ real vector $\sigma$ë¡œ inference distbì„ indexingì„ í•´ì„œ forward processë¥¼ inference distbë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$q_\sigma (x_{1:T}|x_0)=q_{\sigma}(x_T|x_0) \Pi_{t=2}^Tq_{\sigma}(x_{t-1}|x_t,x_0)$$

ì—¬ê¸°ì„œ $q(x_T|x_0) = N(\sqrt{\bar \alpha_T}x_0,(1-\bar{\alpha_T})I)$ë¼í•˜ê³  t > 1ë³´ë‹¤ í´ ë•Œë¥¼ ìƒê°í•´ë³´ì.


**Reverse Conditional Distribution**
<figure class="eq">
$$
q_\sigma\!\left(x_{t-1}\,\middle|\,x_t, x_0\right)
  = \mathcal{N}\!\Bigl(
      \sqrt{\alpha_{t-1}}\,x_0
      + \sqrt{\,1-\alpha_{t-1}-\sigma_t^{2}}\;
        \frac{x_t-\sqrt{\alpha_t}\,x_0}{\sqrt{1-\alpha_t}},
      \;\sigma_t^{2}\mathbf I
    \Bigr)
$$
</figure>

ë¡œ ì¨ì„œ  ëª¨ë“  tì— ëŒ€í•´ì„œ $q(x_t|x_0) = N(\sqrt{\bar \alpha_t}x_0, (1-\bar{\alpha_t})I)$ë¥¼ ë§Œì¡±í•˜ë„ë¡ (DDPMê³¼ ê°™ë„ë¡) formulateí–ˆë‹¤ëŠ” ê²ƒ.

ì´ forward processë¥¼ Bayes ruleë¡œ ë‹¤ì‹œ ì“°ë©´

$$
q_{\sigma}(x_t|x_{t-1},x_0) = \frac {q_{\sigma}(x_t,x_0)q_\sigma(x_t|x_0)}{q_{\sigma}(x_{t-1}|x_0)}
$$

ì´ ì‹ì„ ë³´ë©´  $x_t$ê°€ $x_{t-1}$ ë¿ë§Œì•„ë‹ˆë¼ $x_0$ì—ë„ ì˜ì¡´í•˜ë¯€ë¡œ ë”ì´ìƒ Markovianì´ ì•„ë‹ˆë‹¤. 

<aside>
$\sigma$ì˜ í¬ê¸°ê°€ forward processê°€ ì–¼ë§ˆë‚˜ stochasticí•œì§€ë¥¼ ê²°ì •í•œë‹¤.
`$\sigma$ â†’ 0`ì´ë©´ $x_0$ ì™€ $x_t$ê°€ ì£¼ì–´ì§€ë©´ ë°”ë¡œ `$x_{t-1}$ì´ determine` ëœë‹¤. 
ì¦‰ $\sigma$ê°€ 0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ deterministicí•´ì§
</aside>

ì •ë¦¬í•˜ë©´ DDPMê³¼ DDIMì€ forward processì—ì„œ  `$q(x_t|x_0)$ ëŠ” ê°™ê²Œ` ë‘ê³  ë‹¤ë§Œ  `joint distributionì„ ë‹¤ë¥´ê²Œ` ê°€ì ¸ê°”ë‹¤.

![Untitled](/images/diffusion_ddim/01.png)

#### 2ï¸âƒ£ **Generative Process && Unified Variational Inference Objective**

> **Goal** $p_\theta (x_{0:T})$

Generation ì¸¡ë©´ì—ì„œ `$x_t$â†’ $x_{t-1}$`ë¡œ ê°€ëŠ” processê°€ ê¶ê¸ˆí•˜ê³ , $q_{\sigma}(x_{t-1}|x_t,x_0)$ë¥¼ ì´ìš©í•´ $p_\theta ^t (x_{t-1}|x_t)$ë¥¼ defineí•´ë³´ì

<aside>

1. $x_t$ ê°€ ì£¼ì–´ì§€ë©´ $x_0$ë¥¼ ì˜ˆì¸¡  by $f_\theta$  

2. $q_{\sigma}(x_{t-1}|x_t,x_0)$ë¥¼ ì´ìš©í•´ $x_{t-1}$ obtain
</aside>

$x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon$ ë¥¼ ì´ìš©í•´ ëª¨ë¸ì´ epsilon `noise`ì„ ì˜ˆì¸¡í•´, `$x_0$` ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ì˜ˆì¸¡í•˜ëŠ” f ë„ì…

$$
f_{\theta}^{(t)}(x_t) = (x_t -\sqrt{1-\bar{\alpha_t}}\epsilon_{\theta}^{(t)}(x_t))/\sqrt {\alpha_t} \approx x_0
$$

ê°€ ë˜ê³  ìš°ë¦¬ ëª¨ë¸ì€
<figure class="eq">
$$
p^{(t)}_{\theta}\!\bigl(x_{t-1}\mid x_t\bigr)
  = 
  \begin{cases}
    \mathcal{N}\!\bigl(f^{(1)}_{\theta}(x_1),\,\sigma_1^{2}\mathbf I\bigr), & \text{if } t = 1,\\[6pt]
    q_{\sigma}\!\bigl(x_{t-1}\mid x_t,\,f^{(t)}_{\theta}(x_t)\bigr), & \text{otherwise}.
  \end{cases}
$$
</figure>

ì´ê³  objective $J_\sigma(\epsilon_\theta)$ ëŠ” $\epsilon_{\theta}$ì˜ í•¨ìˆ˜ê°€ ëœë‹¤. ë˜í•œ objectiveê°€ $\sigma$ì— ëŒ€í•œ dependencyê°€ ìˆìœ¼ë¯€ë¡œ ê° $\sigma$ì—ëŒ€í•´ ë”°ë¡œ í•™ìŠµì„ í•´ì£¼ì–´ì•¼í•œë‹¤. ê·¸ëŸ°ë° $J_\sigma$ëŠ” ì–´ë–¤ $\gamma$ì—ëŒ€í•´ $L_\gamma$ì™€ ê°™ë‹¤ê³  í•œë‹¤.

$$
\text{Theorem 1)} \forall \sigma>0, there \; exists \; \gamma \in \mathbb R^T_{>0} \; and \; C \in \mathbb R \quad s.t.\; J_\sigma = L_\gamma + C
$$

ì—¬ê¸°ì„œ $L_\gamma (\epsilon_{\theta}) = \sum_{t=1}^T \gamma_t \mathbb E_{x_0 \sim q(x_0), \epsilon_t \sim N(0,I) }[||\epsilon_\theta ^{(t)}(\sqrt{\bar \alpha_t}x_0 +\sqrt{1-\bar{\alpha_t}}\epsilon_t)-\epsilon_t||^2_2]$ë¥¼ ë‹¤ì‹œ ë³´ì. 

ë§Œì•½ $\epsilon_{\theta}^t$ê°€ ì„œë¡œë‹¤ë¥¸ t ë¼ë¦¬ parameterë¥¼ ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ì „ì²´ë¥¼ maximizeí•˜ê¸° ìœ„í•´ì„  ìš°ë¦¬ëŠ” ê° tì— ëŒ€í•œ termë“¤ì„ ê°ê° maximizeí•´ì•¼ëœë‹¤, ì¦‰ weight factor $\gamma$ì™€ ë¬´ê´€í•˜ê²Œ optimizationì´ ì§„í–‰ëœë‹¤ëŠ” ê²ƒì´ë‹¤. 

ê·¸ëŸ¬ë¯€ë¡œ objectiveê´€ì ì—ì„œ $\gamma$ëŠ” arbitraryí•˜ê²Œ ì¡ì•„ë„ ë˜ê³ , ì´ë¥¼ 1ë¡œ ì¡ì•„ë„ Okì´ë‹¤. ê·¸ëŸ°ë° theorem 1ì— ì˜í•˜ë©´ ì–´ë–¤ $L_\gamma$ ëŠ” $J_\sigma$ì™€ ê°™ì€ objectiveë¥¼ ê°–ìœ¼ë¯€ë¡œ $L_1$ì„  $J_\sigma$ ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

<aside>
ë§Œì•½ ëª¨ë¸ $\epsilon_\theta$ì˜ paramterê°€ ì„œë¡œë‹¤ë¥¸ të¼ë¦¬ ê³µìœ í•˜ì§€ ì•ŠëŠ” êµ¬ì¡°ë©´
 `$J_\sigma$ì˜ objectiveë¡œ $L_1$ì„ ì¨ë„ Ok`.
</aside>

### âš™ï¸ Sampling From Generalized Generative Processes

ìš°ë¦¬ëŠ” generalí•œ objectiveë¥¼ $L_1$ì´ ëŒ€ì²´ê°€ëŠ¥í•¨ì„ ë³´ì˜€ê³ , ê·¸ë˜ì„œ markovianì˜ forward processì™€ non markovianì˜ forward process ëª¨ë‘ì˜ objectiveì´ë¯€ë¡œ pretrained DDPMì„ ì‚¬ìš©í•´ë„ ì¢‹ë‹¤.

ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” $\sigma$ì— ë”°ë¥¸ sampling ì— ì§‘ì¤‘í•´ë³¼ê²ƒ 

#### 1ï¸âƒ£ **Denoising Diffusion Implicit Models (DDIM ì—…ë°ì´íŠ¸ ì‹)**

ìš°ë¦¬ëŠ” ìœ„ì˜ $p_\theta$ë¡œ $x_t$ë¡œë¶€í„° $x_{t-1}$ì„ generateí•  ìˆ˜ ìˆê²Œ ëë‹¤.

<figure class="eq">

$$
x_{t-1} = \sqrt{\bar \alpha_{t-1}}(\frac{x_t-\sqrt{1-\bar \alpha_{t}}\epsilon_{\theta}^{(t)}(x_t)}{\sqrt{\bar \alpha_{t}}}) +\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot \epsilon_{\theta}^{(t)}(x_t) + \sigma_t\epsilon_t
$$
</figure>

**1st term**  

$(\frac{x_t-\sqrt{1-\bar \alpha_{t}}\epsilon_{\theta}^{(t)}(x_t)}{\sqrt{\bar \alpha_{t}}})$  : predicted $x_0$ 

**2nd term**  

$\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot \epsilon_{\theta}^{(t)}(x_t)$ : direction pointing to $x_t$

**3rd term**

random noise independent of $x_t$


ì—¬ê¸°ì„œ $\sigma_t$ë¥¼ ì–´ë–»ê²Œ ì¡ëŠëƒì— ë”°ë¼ ë§¤ìš° í¥ë¯¸ë¡œìš´ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.

**case1**  

$\sigma_t = \sqrt{(1-\bar\alpha_{t-1})/(1-\bar\alpha_t)}\sqrt{1-\bar\alpha_t/\alpha_{t-1}}$

 forward processê°€ Markovianì´ ë˜ê³   DDPMìœ¼ë¡œ reduceëœë‹¤.

**case2**  

$\sigma _t$ = 0 for all t

forward processê°€ `deterministic`í•´ì§„ë‹¤(t=1 ì¼ë•Œ ì œì™¸), ë˜í•œ generative processì—ì„œë„ noiseì˜ ê³„ìˆ˜ê°€ 0ì´ ë˜ì–´ë²„ë ¤ ë§ˆì°¬ê°€ì§€

â‡’ `DDIM`

forward processê°€ ë”ì´ìƒ diffusionì´ ì•„ë‹ˆì§€ë§Œ, `DDPMì˜ objectiveë¡œ í•™ìŠµì´ëœ implicit model`

implicit probablisticì´ë¼ í•˜ëŠ” ì´ìœ ëŠ” sampleë“¤ì´ latent variableì¸ $x_t$ì— ì˜í•´ ìƒì„±ë˜ì—ˆê¸° ë•Œë¬¸

#### 2ï¸âƒ£ **Accelerated Generation Processes**

ë‹¤ì‹œ ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ë…¼ë¦¬ë¥¼ ì´ì•¼ê¸°í•˜ë©´ $L_1$ì˜ objectiveê°€ ì–´ë– í•œ íŠ¹ì • forward process(joint distb)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  â­â­ `$q(x_t|x_0)$ë§Œ ë§Œì¡±`â­â­í•˜ë©´ ëë‹¤.

ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ê¸°ì¡´ ì „ì²´ forward processì˜ length of time step Të³´ë‹¤ ë” ì‘ê²Œ  ì¼ë¶€ ëª‡ ê°œì˜ forward stepì— ëŒ€í•´ì„œë§Œ forward processë¥¼ ì§„í–‰í•˜ê³  ì´ë“¤ì˜ subsetì„ ê°–ê³  generative processë¥¼ ì§„í–‰í•´ë„ ì¢‹ë‹¤.

â‡’ pretrainëœ DDPMì„ í™œìš©í•´ generative processë¥¼ ëŒë ¤ë„ ë˜ê³  ì˜¤íˆë ¤ ì¢‹ë‹¤ëŠ” ì´ì•¼ê¸°

![Untitled](/images/diffusion_ddim/02.webp)

$\tau = [1,3]$, $\tau$ëŠ” (sampling) trajectoryì´ê³  [1,2,3,â€¦,T]ì˜ subsequenceì´ë‹¤. lengthë¥¼ Së¼ê³  denoteí•˜ê³ , ì´ëŠ” DDIMì˜ sampling stepë“¤ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸

1.  **Relevance to Nueral ODE**

DDIM(with sigma = 0)ì„ ODEë¡œ rewriteí•´ë³´ì.

$$
d\bar x(t) = \epsilon_\theta^{(t)}(\frac{\bar x(t)}{\sqrt{\sigma^2+1}})d\sigma(t)
$$

where I.C : $x(T) \sim N(0,\sigma (T))$

 ì¶©ë¶„íˆ discretization stepì„ ê±°ì¹˜ë©´ ì´ ODEë¥¼ reverseí•´ì„œ generation processì˜ reverse, `encoding`ì´ ê°€ëŠ¥í•´ì§„ë‹¤.

<aside>

**DDIM** sampleì˜ high level featureë“¤ì€ $x_T$ì— encodingëœë‹¤.

</aside>

### ğŸ§ª ì‹¤í—˜ ê²°ê³¼ ìš”ì•½


DDIMì€ DDPMë³´ë‹¤ í›¨ì”¬ ë” ì ì€ iterationìœ¼ë¡œ image generationì´ ê°€ëŠ¥í•˜ê³ , DDPMê³¼ëŠ” ë‹¬ë¦¬ initial latent $x_T$ê°€ fixë˜ë©´ generation trajectoryì™€ ë¬´ê´€í•˜ê²Œ high level image featuresë“¤ì´ ìœ ì§€ëœë‹¤.  

ê·¸ë˜ì„œ latent spaceìƒì—ì„œ ë°”ë¡œ interpolationì´ ê°€ëŠ¥í•˜ë‹¤. 

ë˜í•œ sampleë“¤ì„ encodingí•  ìˆ˜ ìˆì–´ latent codeì—ì„œ  sampleì„ reconstructí•  ìˆ˜ ìˆë‹¤.(DDIMì˜ deterministicí•œ ì„±ì§ˆ)

ë‹¤ë¥¸ ì¡°ê±´ì€ ë‹¤ ê°™ê²Œ ë‘ê³  $\tau$( (sampling) trajectory â†’ how fast samples are obtained) ì™€  $\sigma$ (DDIM = 0)ë§Œì„ ì¡°ì ˆí•˜ë©° samplingì— ì§‘ì¤‘í–ˆê³ ,  $\sigma$ë¥¼ í¸í•˜ê²Œ controlí•˜ê¸° ìœ„í•´ $\eta$ ë„ì…

<figure class='eq'>

$\eta$ = 1.0 DDPM

$\eta$ = 0.0 DDIM

DDPMê³¼ DDIMì„ interpolate

$$
\sigma_{\tau_i} =\eta \sqrt{\frac{1-\bar\alpha_{\tau_{i-1}}}{1-\bar\alpha_{\tau_{i}}}}\sqrt{1-\frac{\bar\alpha_{\tau_i}}{\bar\alpha_{\tau_{i-1}}}}
$$

</figure>

1.  **Sample Quality and Efficiency**

![Untitled](/images/diffusion_ddim/03.png)

2.  **Sample `Consistency` in DDIMs**
<aside>
DDIMì˜ generative processëŠ” deterministicí•˜ê³  $x_0$ëŠ” ì˜¤ì§ initial state $x_T$ì—ë§Œ ì˜ì¡´í•œë‹¤
</aside>

![Untitled](/images/diffusion_ddim/04.png)

3.  **Interpolation in Deterministic Generative Processes**

$x_0$ì˜ high level featureê°€ $x_T$ë¡œ encodingì´ ë˜ì–´ interpolationë„ ê°€ëŠ¥

4.  **Reconstruction From Latent Space**

### ğŸŒ í•œëˆˆì— ì •ë¦¬

| ë°©ë²• | ìŠ¤í… ìˆ˜ | ê²°ì •ì„± | íŠ¹ì§• |
|------|--------:|-------:|------|
| DDPM | ë§ìŒ | í™•ë¥ ì  | ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦¼ |
| DDIM | ì ìŒ | ê²°ì •ì  | ë¹ ë¥´ê³  ì¼ê´€ì„± ìš°ìˆ˜ |
| í˜¼í•© | ê°€ë³€ | ë¶€ë¶„ ê²°ì • | í’ˆì§ˆ-ì†ë„ ì ˆì¶© |

> DDIMì€ **í•™ìŠµì€ ê·¸ëŒ€ë¡œ** ë‘ê³  **ìƒ˜í”Œë§ë§Œ** êµì²´í•´,  
> í’ˆì§ˆ Ã— ì†ë„ë¥¼ ëª¨ë‘ ì±™ê¸¸ ìˆ˜ ìˆëŠ” ì‹¤ìš©ì  ì ‘ê·¼ì´ë‹¤.