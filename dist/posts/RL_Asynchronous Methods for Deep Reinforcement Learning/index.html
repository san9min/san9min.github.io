<!DOCTYPE html>
<html lang="ko"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Sangmin Blog | Reinforcement Learning : A3C</title>
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css">
  <link rel="stylesheet" href="../../assets/styles/main.css">
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <link rel="stylesheet"
     href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

</head><body>
  <main class="article">
    <h1>Reinforcement Learning : A3C</h1>
    <div class="meta">Aug 17, 2022 · 20 min read</div>
    <img class="hero" src="../../images/RL_Asynchronous_Methods_for_Deep_Reinforcement_Learning/thumb.png" alt="cover image">
    <h3>Asynchronous Methods for Deep Reinforcement Learning</h3>
<hr>
<h3>논문소개</h3>
<p><a href="https://arxiv.org/abs/1602.01783">Asynchronous Methods for Deep Reinforcement Learning</a></p>
<p><strong>Simple and lightweight Deep Reinforcement Learning framework</strong></p>
<ul>
<li><strong>multi core CPU instead of GPU</strong>.</li>
<li><strong>Asynchronous gradient descent</strong></li>
<li><strong>Parallel</strong></li>
<li><strong>A3C</strong></li>
</ul>
<hr>
<h3>Introduction</h3>
<p>Deep nueral networks가 Reinforcement Learning(이하 RL)에 들어오면서 RL은 많이 유용해졌다. 그러나 단순히 On-line RL에 neural networks을 사용하는 것은 fundamentally unstable하다. on line RL agent가 observe하는 data sequence는 non-stationary하고 on-line RL updates는 high correlated 되어있기 때문이다. 이 문제는 experience replay memory를 활용하면 memory에 experiences를 저장하고 batch단위로(!non-stationary) randomly sampling(!high correlation)할 수있어 어느정도 해소가 된다. 그러나 이는 off-policy에 국한된 얘기이다. on-policy에는 여전히 문제가 남아있다.</p>
<p>본 논문에선 experience replay가 아닌 비동기 방식으로 multiple instances of environments에서 여러 agent들을 parallel하게  사용해서 agent’s data를 non-stationary로 decorrelate한다. 어떤 특정 time step t에서 parallel agent들은 (대부분) 다른 state들을 경험할 것이기 때문이다.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4eb16007-516d-4a29-b1e0-888131c24a4d/Untitled.png" alt="Untitled"></p>
<p>이렇게 비동기 방식으로 처리를 하면 on-policy에서도 위의 문제를 해결할 수 있다. 또한 이 비동기 방식은 Q-learning같은 off-policy 에도 적용가능하다. 그래서 RL에 neural networks를 안정하게 더욱 넓고 효과적으로 사용할 수 있게 되었다.</p>
<p>또한 위 방식은 GPU를 사용하는 대신에 multicore CPU를 사용했는데, 속도 또한 빨랐다고 한다.</p>
<hr>
<h3>Related Work</h3>
<p>distributed setting과 parallism에 관한 연구들을 소개하고있다.</p>
<ol>
<li><p><strong>Gorila</strong>
distributed setting에서 <strong>agent</strong>들을 비동기 방식으로 학습시켰다. 각각의 process는 하나의 <strong>acotr,</strong> 하나의 <strong>replay memory,</strong> 하나의 <strong>learner</strong>를 갖는다**.** </p>
<p> actor는 environment를 copy한 자신만의 environment와 interaction한 후 replay memory에 저장한다. <strong>learner</strong>는 replay memory에서 data를 뽑아 policy parameter에 대한 DQN loss의 gradient를 계산한다. 이 gradient들은 비동기적으로 <strong>central parameter sever</strong>에 보내져 central copy of the model을 update한다. 그리고 그 update된 parameter는 일정한 간격으로 actor-learner들에 보내진다.</p>
</li>
</ol>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8c86cdde-e63e-45d7-be2a-79d4ab1d5e34/Untitled.png" alt="Untitled"></p>
<ol>
<li><p><strong>Li &amp; Schuurmans, 2011</strong></p>
<p> Map Reduce framework을 Batch reinforcement learning methods에 적용했다. Neural network이 아닌 linear function approximator를 사용했다. </p>
<p> Parallelism이 large matrix operation을 빠르게 처리하도록 사용되었지만</p>
<p> Experiences을 모으는것을 병렬화 하거나 학습을 안정적으 진행되로록 만드는데는 사용하지 않았다.</p>
</li>
<li><p><strong>Grounds &amp; Kudenko, 2008</strong></p>
<p> 여러  separate된 actor-leaner들을 사용해서 학습을 가속화할 수 있도록 Sarsa에 parallelism을 적용했다. 각각의 actor-learner은 각자 학습을 한 후 주기적으로 update된 내용을 보내고  이 내용들을 다른 러너들이 peer-to-peer communication을 사용한다.</p>
</li>
<li><p><strong>Tsitsiklis, 1994</strong></p>
<p> 비동기로 최적화시키는 setting에대해 Q-learning의 수렴성을 연구했다. Q learning은 몇개의 정보가 오래됐을 때, 오래된 정보가 버려지고 몇가지 기술적인 가정이 충족되면 수렴된다고 한다. distributed dynamic programming 문제에 대한 연구도 했다고 한다.</p>
</li>
<li><p><strong>Others</strong></p>
<p> Parallizing에 대해 굉장히 직관적으로 받아들일 수 있는 Evolutionary methods(진화 알고리즘)을 이야기한다. 이는 multiple machine이나 threads에서  fitness evaluation을 해서 parallelize하는 방식인데,  visual reinforcement learning task에 적용된 사례도 있다고 한다.</p>
</li>
</ol>
<hr>
<h3>Reinforcement Background</h3>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fd7a366c-d6a3-48a7-8662-548b9768298c/Untitled.png" alt="Untitled"></p>
<p>agent는 environment와 interaction한다. </p>
<p>time step t에서 state를 받고 apolicy를 따라 action을 선택하면 envirionment가 next state과 reward를 agent에게 넘겨준다. 이 과정의 반복일 뿐이다. agent는 각 state에서 expected return을 최대화하는 것을 목적으로 한다. 여기서 return은 $R_t  = \sum <em>{k=0} \gamma^k r</em>{t+k}$ 으로 formulate된다. </p>
<p>action value function은 $Q^\pi (s,a) = \mathbb E [R_t |s_t = s,a]$ 정의된다. </p>
<p>즉 state s에서 action a를 취하고 이후에 policy $\pi$를 따랐을 때 받는 expected return이다. 여기서 optimal action value functinon은 $Q^*(s,a) = max_\pi Q^\pi (s,a)$ (achievable by any policy)로 정의되고 maximum action value를 주는 것을 말한다. </p>
<p>value function은 policy $\pi$를 따를 때, state s의 value,즉 expected return을 나타낸다.</p>
<p> $V^\pi (s) = \mathbb E [R_t |s_t = s]$</p>
<p><strong>Value based</strong></p>
<p>Q를 neural network로 근사시키는 것을 말한다. Q를 어떻게 학습시킬지에 따라 다양한 알고리즘이 있고, 대표적인게 Q-learning이다.</p>
<p>One-step Q-learning에선 Q-function을 loss function들의 sequence를 반복적으로 minimize하는 방식으로 학습시킨다. i번째 loss function은 </p>
<p>$$
L_i(\theta_i) = \mathbb E (r + \gamma max_{a&#39;} Q(s&#39;,a&#39;;\theta_{i-1}) - Q(s,a;\theta_i))^2
$$</p>
<p>으로 정의되고 여기서 prime ‘ 은 next를 의미한다. 여기서 reward r은 오직 이 reward r을 만들어낸  (s,a) pair의 value에만 직접적으로  영향을 미치고 다른 state action pair는 간접적으로만 영향을 받는다. 그래서 이 reward r이 다음 state action pair들에 영향을 미치려면 update를 많이 해나가야돼서 학습 속도가 느리다. </p>
<p> n-step Q-learning은 이 문제를 해소한다.  여기선 Q(s,a)가 n-step return이 되도록 학습이 된다. </p>
<p>$$
Q(s,a) \leftarrow r_t + \gamma r_{t+1} + \cdots + \gamma^{n-1}r_{t+n-1} + max_a \gamma^n Q(s_{t+n},a) 
$$</p>
<p><strong>Policy-based</strong></p>
<p>poilcy를 바로 parameterized하고 gradient ascent 방식으로 $\mathbb E [R_t]$를 최대화 하도록 학습시킨다.  REINFORCE가 대표적인 예시로 $\nabla_\theta ;log\pi(a_t|s_t;\theta)R_t$ 의 방향으로 theta를 update를 한다. 이는 $\nabla_\theta\mathbb E [R_t]$의 unbiased estimate인데 이를 unbiased를 유지하면서 variance를 줄이려면, return에서 baseline을 빼면된다. 여기서 baseline은 state의 learned function $b_t(s_t)$이고 이를 적용해 gradient를 때리면</p>
<p>$$
\nabla_\theta ;log\pi(a_t|s_t;\theta)(R_t -b_t(s_t))
$$</p>
<p>이 된다.</p>
<p>보통  $b_t \approx V^\pi(s_t)$로 근사시켜 policy gradient 값의 variance값을 줄인다. 이런 근사를 하고나면 $R_t -b_t$는 $R_t$는 $Q^\pi(a_t,s_t)$ 의 estimate이고 $b_t$는 $V^\pi(s_t)$의 estimate이므로, state s에서 action의 advantage가 된다.</p>
<p>$$
A(a_t,s_t)= Q(a_t,s_t) -V(s_t)
$$</p>
<p>Policy $\pi$를 actor, baseline $b_t$ 를 critic이라 하면 actor-critic architecture가 된다.</p>
<hr>
<h3>Main</h3>
<ol>
<li><p><strong>Asynchronous actor-learners</strong></p>
<p> multiple CPU threads on a single machine, instead of using separate machines and a parameter server(Gorila)</p>
<p> → Gorila에비해 machines과 server간의 communication 비용을 줄임</p>
</li>
<li><p><strong>Using different exploration policies in each actor-learner</strong></p>
<p> less correlated and stable</p>
<p> reduction in training time</p>
<p> on-policy as well as off-policy</p>
</li>
</ol>
<p><strong>Asynchronous one-step Q-learning</strong></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/00d63441-b61f-4ffa-b1c5-7f8962b6cba2/Untitled.png" alt="Untitled"></p>
<p>각각의 thread에서 environment를 복사해 그 environment와 interaction시킨다.  그리고 각 스텝에서 Q-learnig loss의 gradient를 계산한다. DQN처럼 Q-learning loss를 계산할 때 shared and slowly changing <em>target network</em>를 도입했다.  그리고 <strong>여러 time step에서 계산된 gradient들을 모은다</strong>. 이 과정은 서로의 update된 내용들을 overwrite 할 가능성을 줄이고 data efficiency와 computational cost간의 trade off 를 조절할 수 있게한다.</p>
<p>또한 각각의 thread에 다른 exploration policy를 줘서 robustness를 향상시켰다. exploraiton의 다양성을 추가하는건 exploration을 더 잘해서 더 좋은 성능을 내기 때문이라 생각하면 된다. 여기선 epsilon greedy 를 사용했다.</p>
<p><strong>Asynchronous one-step Sarsa</strong></p>
<p>위의 방식과 동일한데 Q function의 target value만 다르다. target value가 $r + \gamma Q(s&#39;,a&#39;;\theta^-)$이다.  target network와 update를 gradients을 multiple timesteps들에서 accumulate해서 사용한는 방식도 동일하다.</p>
<p><strong>Asynchronous n-step Q-learning</strong></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2a9da19c-cfa3-4b48-86ba-543f64e77a90/Untitled.png" alt="Untitled"></p>
<p>n-step returns을 backward view방식이 아닌 forward view로 얻는다. 이는 neural networks을 momentum과 bpp로 학습시키는 것을 더 편하게 한다고한다. </p>
<p>update를 시키는 것을 보자. 한번의 update를 위해선 exploration policy(여기선 epsilon greedy)를 사용해 $t_{max}$ step 혹은 episode가 중간에 끝난다면 그 시점까지 action을 취한다. 즉 한번의 업데이트 이후 다음번의 업데이트가 일어나기 전까지(중간에 episode가 끝나지 않는다면) agent는 총 $t_{max}$개 만큼의 reward를 받는다. </p>
<p>그 후 gradient를 계산한다. 가능한 가장 긴 n-step return을 사용하는데, 예를들어 마지막 state는 one-step update를 , 뒤에서 두번째 state는 two-step update를, 이렇게 하다 마지막엔 전체에대해선 $t_{max}$ update를 사용한다. 이 update(or gradient)들을 accumulate하고, 이를 single gradient step에서 사용한다.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/39b3cacc-c339-4fda-9bca-c68ddea0d256/Untitled.png" alt="forward view. We decide how to update each state by looking forward to
future rewards and states."></p>
<p>forward view. We decide how to update each state by looking forward to
future rewards and states.</p>
<p><strong>Asynchronous adavantage actor-critic ⇒ A3C</strong></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/25cef61b-eab6-415c-902f-efd4d5c57f26/Untitled.png" alt="Untitled"></p>
<ul>
<li>Policy $\pi(a_t|s_t;\theta)$</li>
<li>Estimate of the value function $V(s_t;\theta_v)$</li>
</ul>
<p>n-step Q-learning과 마찬가지로 forward view 방식으로 작동하고, update를 위해 n-step returns을 활용할 것이다. policy와 value function은 $t_{max}$개의 action을 취한 후 update가 될 것이다. 만약 중간에 episode가 끝나면 그 때 update 시키면 된다.</p>
<p>update rule은 다음과 같다.</p>
<p>$$
\nabla _{\theta&#39;} log \pi(a_t|s_t;\theta&#39;)A(s_t,a_t;\theta,\theta_v)
$$</p>
<p>여기서 A는 advantage function의 estimation값이고 식으로 표현하면</p>
<p>$$
\sum^{k-1}<em>{i=0} \gamma^ir</em>{t+i} + \gamma^kV(s_{t_k};\theta_v) - V(s_t;\theta_v)
$$</p>
<p>이고 k는 state마다 다르고 $t_{max}$로 upper bound되어있다. </p>
<p>parallel actor-learners를 사용했고, 학습이 안정하게 진행되도록 updates을 모았다.</p>
<p>Policy의 parameters $\theta$와 Value function의 parameter $\theta_v$는 일반적으로 다른 parameter인데, 편의상 실험에선 일부 공유했다고 한다. CNN을 사용해 parameter를 공유했고 output layer에 Policy에는 softmax를 value fucntion엔  linear model을 사용했다고 한다.</p>
<p>또한 objective function에 policy의 entropy term을 추가해 exploration을 향상시켜 globally 최적이 아닌 곳에 수렴하는 것을 막았다고 한다. 여기선 entropy를 regulartization하는데 이용했고, policy parameters에대한 objective function의 gradient는</p>
<p>$$
\nabla <em>{\theta&#39;}log\pi(a_t|s_t;\theta&#39;)(R_t - V(s_t;\theta_v))+\beta\nabla</em>{\theta&#39;} H (\pi(s_t;\theta&#39;))
$$</p>
<p>이다.</p>
<p><strong>Optimization</strong></p>
<p>asynchronous framework에서의 optimization 3가지</p>
<ol>
<li>SGD with momentum</li>
<li>RMSProp without shared statistics</li>
<li>RMSProp with shared statistics</li>
</ol>
<p>논문에서는 standard한 non-centered RMSProp update를 사용했다고 한다.</p>
<p>$$
g = \alpha g (1-\alpha)\Delta\theta^2 \\text{and} \ \theta \leftarrow \theta - \eta \frac {\Delta \theta}{\sqrt{g + \epsilon}}
$$</p>
<p>모든 operation은 elementwise로 수행된다한다. 여기서 statistics g 를 모든 threads가 공유하는게 학습이 가장 안정했다고 한다.</p>

    <section class="related">
        <div class="related-head">
          <h2>Related posts</h2>
          <a class="all-link" href="../../">Browse all articles&nbsp;→</a>
        </div>
        <div class="related-list">
      <a class="card mini" href="../../posts/Difussion_Latent Diffusion/">
        <img class="thumb" src="../../images/latentdiffusion.webp" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Difussion : Latent Diffusion</h3>
          <span class="arrow">↗</span>
          <div class="meta">Feb 17, 2024 · 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/Difussion_LM_Seq2Seq/">
        <img class="thumb" src="../..//images/Difussion_LM_Seq2Seq/thumb.png" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Difussion : Difussion LM & Seq2Seq</h3>
          <span class="arrow">↗</span>
          <div class="meta">Feb 26, 2023 · 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/Diffusion_DDIM/">
        <img class="thumb" src="../../images/post-002-thumb.jpg" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Diffusion : DDIM</h3>
          <span class="arrow">↗</span>
          <div class="meta">Jan 31, 2023 · 20 min</div>
        </div>
      </a></div>
      </section> 
  </main>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
<script>
window.MathJax = {
  tex: {
    inlineMath:  [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },

  chtml: {
    linebreaks: { automatic: true, width: "match" }  // 또는 width: 80
  },

  options: {
    renderActions: { addMenu: [] }   // 우클릭 메뉴 제거
  }
};
</script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script src="../../assets/js/main.js" defer></script>
</body></html>