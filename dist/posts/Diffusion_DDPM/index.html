<!DOCTYPE html>
<html lang="ko"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Sangmin Blog | Diffusion : DDPM</title>
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css">
  <link rel="stylesheet" href="../../assets/styles/main.css">
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <link rel="stylesheet"
     href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

</head><body>
  <main class="article">
    <h1>Diffusion : DDPM</h1>
    <div class="meta">Jan 22, 2023 Â· 20 min read</div>
    <img class="hero" src="../..//images/diffusion_ddpm/thumbnail.jpg" alt="cover image">
    <h2>Denoising Diffusion Probabilistic Models</h2>
<p><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></p>
<ul>
<li>parameterized Markov chain</li>
<li>trained using variational inference</li>
<li><marck><code>learn to reverse a diffusion process</code> </mark></li>
</ul>
<p><img src="/images/diffusion_ddpm/00.png" alt="Untitled"></p>
<p>Diffusion modelì€ latent variable ëª¨ë¸ì´ë‹¤.
<em>latent : a hidden continuous feature space</em></p>
<p><strong>GOAL</strong></p>
<figure class="eq">
$$
p_{\theta} (x_0) = \int p_{\theta}(x_{0:T})dx_{1:T}
$$
</figure>
$x_1,..x_T$ ëŠ” latentë“¤ì´ê³  data $x_0 \sim q(x_0)$ì™€ ê°™ì€ dimensionì„ ê°–ëŠ”ë‹¤.

<p>modelì€ ì´ reverse processë¥¼ í•™ìŠµí•˜ê³  ìƒˆë¡œìš´ dataë¥¼ generationí•œë‹¤.</p>
<hr>
<h2>&gt;&gt; Forward Process (diffusion process)</h2>
<blockquote>
<p><code>Gaussian noiseë¥¼ ë”í•´ê°€ëŠ” ê³¼ì •</code></p>
</blockquote>
<h3>(1) Markov chain - Noising process</h3>
<p>$$
q(x_{1:T}|x_0) = \Pi_{t=1}^Tq(x_t|x_{t-1}) 
$$</p>
<p>forward ê³¼ì •ì€ Markov chainìœ¼ë¡œ formulateí•  ìˆ˜ ìˆë‹¤.<br>Markov chainì€ ê° stepì€ ì˜¤ì§ ì§ì „ stepì—ë§Œ ì˜ì¡´í•¨ì„ ì˜ë¯¸í•œë‹¤.<br>ì—¬ê¸°ì„œ $q(x_{1:T})$ëŠ” që¥¼ timestep 1ë¶€í„° Tê¹Œì§€ ë°˜ë³µí•´ì„œ ë…¸ì´ì¦ˆë¥¼ ê°€í•¨ì„ ì˜ë¯¸í•˜ëŠ” notationì´ë‹¤.</p>
<p>$$q(x_{t}|x_{t-1}) = N(x_t ; \sqrt {1-\beta_t}x_{t-1},\beta_tI)$$</p>
<p>ê° ìŠ¤í…ì—ì„œ Gaussian noiseë¥¼ ë”í•œë‹¤. í•œ ìŠ¤í… ì „ì´í•  ë•Œë§ˆë‹¤,í‰ê· ì„ ìŠ¤ì¼€ì¼ë§í•˜ê³ , ë¶„ì‚°ë§Œí¼ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œë‹¤.  </p>
<p><strong>$\beta_t$ (ë¶„ì‚° ìŠ¤ì¼€ì¤„)</strong>  </p>
<p>ì—¬ê¸°ì„œ $\beta_t$ëŠ” variance scheduleì´ê³  Iê°€ identityì´ë¯€ë¡œ ê° dimensionì€ ê°™ì€ stdë¥¼ ê°–ëŠ”ë‹¤. 
ìƒìˆ˜ë¡œ ë‘¬ë„ ë˜ê³ , ì‹œê°„ì— ë”°ë¥¸ ë³€ìˆ˜ë¡œ ë‘ì–´ë„ ëœë‹¤. 
ë…¼ë¬¸ì—ì„  higer të¡œ ê°ˆìˆ˜ë¡ ì»¤ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ linearí•˜ê²Œ ë‘ì—ˆëŠ”ë° ë‹¤ë¥¸ ë…¼ë¬¸ì—ì„  cosine shcedule ì´ ì˜ ëë‹¤ê³  í•œë‹¤.</p>
<p>$\beta_t$ë¥¼ ì´ìš©í•´ scalingí•œ í›„ ë”í•´ì£¼ëŠ” ì´ìœ ëŠ” varianceê°€ divergeí•˜ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•¨ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. 
Gaussian noiseë¥¼ ë”í•´ê°€ë©´, ìµœì¢… step (time T)ì—ì„œëŠ” í‘œì¤€ ì •ê·œë¶„í¬ì— $N(x_T;0,I)$ ê°€ê¹ê²Œ ìˆ˜ë ´í•œë‹¤. 
ì´ ë‹¨ìˆœí•œ prior ë•ë¶„ì— Reverse Process ëŠ” â€œì™„ì „í•œ ë…¸ì´ì¦ˆ â†’ ì›ë³¸â€ ë§Œ í•™ìŠµí•˜ë©´ ëœë‹¤</p>
<aside>
$\beta _t$ë¥¼ ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´ ë¶„ì‚°ì´ í­ì£¼í•˜ê³ , ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦¬ê²Œ íŒŒê´´ë˜ë¯€ë¡œ ìŠ¤ì¼€ì¤„ ë””ìì¸ì´ í•µì‹¬ì´ë‹¤.
</aside>

<p>ê·¸ëŸ°ë° ì—¬ê¸°ì„œ ì–´ë–¤ ìˆœê°„ t ( $0 \le t \le T)$ì—ì„œ $x_t$ë¥¼ ì•Œê³  ì‹¶ë‹¤ê³ í•œë‹¤ë©´, ìœ„ì˜ ì‹ì„ ì´ìš©í•´ ë°˜ë³µì ì¸ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ë©´ ëœë‹¤. ê·¸ëŸ¬ë‚˜ t ê°€ í¬ë‹¤ë©´ ì´ëŠ” ì¢‹ì€ ë°©ë²•ì´ ì•„ë‹ ê²ƒì´ë‹¤.</p>
<h3>(2) Reparameterization trick - í•œë²ˆì— $x_t$ ë¥¼ sampling</h3>
<p>ë§Œì•½ ìš°ë¦¬ê°€
$\alpha_t = 1-\beta_t , \bar \alpha_t = \Pi_{s=0}^t \alpha_s$ ë¼ê³  ì¡ëŠ”ë‹¤ë©´, tì—ì„œ $x_t$ë¥¼ samplingí•˜ëŠ” ê²ƒì„ closed formìœ¼ë¡œ ì“¸ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>
<figure class="eq">

<p>$$
q(x_t|x_0) = N(x_t;\sqrt {\bar \alpha_t} x_0, (1- {\bar \alpha_t})I)
$$</p>
<p>$$
\text{sample} =\mu +\sqrt{\sigma^{2}};\epsilon,\qquad\epsilon \sim \mathcal N(0,1)
$$</p>
</figure>
    
<p><strong>Proof</strong>
    $$\text{let  } \epsilon_0, \cdots \epsilon_{t-2},\epsilon_{t-1} \sim N(0,I)$$
    $$x_ t = \sqrt{(1-\beta_t)}x_{t-1} + \sqrt {\beta_t} \epsilon_{t-1} \\ =\cdots  \\=\sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon_0$$</p>
<p>ì¦ëª…ì˜ í•µì‹¬ ë…¼ë¦¬ëŠ” ë‹¤ë¥¸ varianceë¥¼ ê°–ëŠ” ë‘ê°œ($\sigma_1^2,\sigma_2^2$)ì˜ Gaussiansì„ mergeí•´ ìƒˆë¡œìš´ distribution(with variance $\sigma_1^2+\sigma_2^2$ )ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤.</p>
<p>í™•ì¸ì„ í•´ë³´ë©´ t â†’ $\infty$ë¡œ ê°ˆë•Œ $q(x_t|x_0)$ê°€ $N(x_t;0,I)$ë¡œ ê°ë„ ë³¼ ìˆ˜ ìˆë‹¤</p>
<p>ì´ì œ ìš°ë¦¬ëŠ” any timestep tì—ì„œ noiseë¥¼ samplingí•  ìˆ˜ ìˆê²Œ ëë‹¤.
ì´ë¥¼ í†µí•´ $x_t$ë¥¼ $x_0$ì™€ $\epsilon$ì˜ í•¨ìˆ˜ë¡œ ë³¼ìˆ˜ ìˆê²Œ ëœë‹¤.</p>
<p>$$
x_t = \sqrt{\bar \alpha_t}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon_0
$$</p>
<p><span>or</span></p>
<p>$$
x_0 = \frac{1}{\sqrt{\bar \alpha_t}}(x_t - \sqrt{1-\bar \alpha_t}\epsilon)
$$</p>
<aside>
ëª¨ë“  ì¤‘ê°„ ìŠ¤í…ì„ ê±°ì¹˜ì§€ ì•Šì•„ë„, $x_0$ ê³¼ í‘œì¤€ì •ê·œ ë…¸ì´ì¦ˆ $\epsilon$ ë§Œìœ¼ë¡œ ì¦‰ì‹œ $x_t$ ìƒì„± ê°€ëŠ¥í•˜ë‹¤.
ì´ íŠ¸ë¦­ ë•ë¶„ì— ì„ì˜ ì‹œì ì˜ ë…¸ì´ì¦ˆ ìƒíƒœë¥¼ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ìƒ˜í”Œë§í•  ìˆ˜ ìˆê³ , ë¯¸ë¶„ ê°€ëŠ¥í•œ parameterë¡œ ì—…ë°ì´íŠ¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
</aside>


<hr>
<h2>&lt;&lt; Reverse Process (denoising process)</h2>
<blockquote>
<p><code>ëª¨ë¸ì´ í•™ìŠµí•  ê³¼ì •</code></p>
</blockquote>
<p>$q(x_{t-1}|x_{t})$ë¥¼ ì›í•˜ë‚˜ ì–´ë ¤ì›Œì„œ neural networkë¥¼ ì´ìš©</p>
<p>forward processì˜ Gaussian noiseê°€ ì¶©ë¶„íˆì‘ì„ ë•Œ reverse process ë˜í•œ Gaussianì´ ë˜ê³  ì´ëŠ” neural networkë¥¼ ì´ìš©í•´ ê·¼ì‚¬ì‹œì¼œ meanê³¼ varianceë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.</p>
<p>$$
p_{\theta}(x_{t-1}|x_t) = N(x_{t-1};\mu_{\theta}(x_t,t), \Sigma_{\theta}(x_t,t))
$$</p>
<p>Reverse processëŠ” $p(x_T) = N(x_T;0,I)$ë¶€í„° ì¶œë°œí•´ (learned) Gaussian trainsitionì„ í•˜ëŠ” Markov chainì´ë‹¤. </p>
<p>ì¦‰, trajectoryëŠ”</p>
<p>$$
p_{\theta}(x_{0;T}) = p(x_T)\Pi_{t=1}^T p_{\theta}(x_{t-1}|x_t)
$$</p>
<p>ë¡œ fomulateí•  ìˆ˜ ìˆë‹¤.</p>
<p>neural networkì— timestep të¥¼ conditioningí•˜ë©´ modelì€ ê° time stepì˜ Gaussianì˜ meanê³¼ varianceë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œëœë‹¤.</p>
<hr>
<h2>ğŸ‹ï¸ Training</h2>
<blockquote>
<p><code>ELBO</code> on the negative log likelihoodë¥¼ optimize</p>
</blockquote>
<p>$$
E[-\log p_{\theta}(x_0)] \le E_q[-\log \frac {p_\theta (x_{0:T})}{q(x_{1:T}|x_0)}] \ = L
$$</p>
<p>ì´ê³  ìœ„ ì‹ì„ ì •ë¦¬í•˜ë©´</p>
<p>$$
L = E_q[D_{KL}(q(x_T|x_0)||p(x_T))+\sum_{t&gt;1}D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\log p_{\theta}(x_0|x_1)]
$$</p>
<p>ì´ ëœë‹¤.</p>
<aside>
$q(x_{t-1}|x_t)$ëŠ” intractableí•˜ì§€ë§Œ $x_0$ì˜ conditioningì„ ì£¼ë©´ tractableí•˜ë‹¤ê³  í•¨ â†’ generative modelì´ reverse diffusion stepìœ¼ë¡œ generationì„ í•˜ê¸° ìœ„í•´ì„  reference image $x_0$ê°€ í•„ìš”í•˜ë‹¤

</aside>

<p>ê²°êµ­ í•˜ê³  ì‹¶ì€ ê²ƒì€ $p_{\theta}(x_{t-1}|x_t)$ì™€ forward process posteriors $q(x_{t-1}|x_t,x_0)$ë¥¼ ë¹„êµí•˜ëŠ” ê²ƒì´ê³ </p>
<p>$L_T = D_{KL}(q(x_T|x_0)||p(x_T))$ = const</p>
<p>$X_T$ê°€ ì–¼ë§ˆë‚˜ standard Gaussianì¸ì§€, ê·¸ëŸ°ë° ìš°ë¦¬ëŠ” $\beta_t$ë¥¼ ì‹œê°„ì— ë”°ë¥¸ constant ë¡œ ë‘ì—ˆìœ¼ë¯€ë¡œ ì´ termë„ constantì´ê³  í•™ìŠµí•  ë•Œ <code>ë¬´ì‹œ</code>í•´ë„ ëœë‹¤</p>
<p>$L_{T-1} = \sum_{t&gt;1}D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))$</p>
<p>denoising step $p_{\theta}(x_{t-1}|x_t)$ ê³¼ approximated denoising step $q(x_{t-1}|x_t,x_0)$ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•¨ì„ ë³¼ ìˆ˜ ìˆìŒ</p>
<mark>
modelì´ `noiseë¥¼ ì˜ˆì¸¡`í•˜ë„ë¡ `Reparam`
</mark>

<ol>
<li>$\Sigma_{\theta}(x_t,t) = \sigma_t^2I$, $\sigma$ëŠ” $\beta$ì— ê´€í•œ time dependent constants</li>
<li>$\mu_\theta(x_t,t)$ for $p_{\theta}(x_{t-1}|x_t)$ using  $x_t(x_0,\epsilon)$</li>
</ol>
<figure class="eq">

<p>$$
p_\theta(x_{t-1}|x_t) = N(x_{t-1};\mu_{\theta}(x_t,t),\sigma_t^2I)
$$</p>
<p>$$
\mu_{\theta}(x_t,t) = \frac 1 {\sqrt {\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar \alpha_t}}\epsilon_{\theta}(x_t,t))
$$</p>
<p> $sample \ x_{t-1}$ = $\frac 1 {\sqrt {\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar \alpha_t}}\epsilon_{\theta}(x_t,t))$ + $\sigma_t$<strong>z</strong>,      <strong>z</strong> $\sim N(0,I)$</p>
</figure>

<p>$$
\mathbb{E}_{x_0,\epsilon}\left[\frac{\beta_t^{2}}{2\sigma_t^{2}\alpha_t(1-\bar\alpha_t)}\bigl\lVert\epsilon - \epsilon _\theta\bigl(\sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\epsilon,t \bigr) \bigr\rVert^{2}\right]
$$</p>
<p>â‡’$\epsilon_{\theta}$ ëŠ” $x_t$ì™€ të¥¼ ë°›ê³  noiseë¥¼ ì˜ˆì¸¡í•œë‹¤.</p>
<p>$x_t$ëŠ” $x_0$ë¡œ ë¶€í„° samplingì´ ê°€ëŠ¥í•˜ë„ë¡ ìœ„ì—ì„œ reparamí–ˆë‹¤.</p>
<p>Reconstruction term
$L_0 = -\log p_{\theta}(x_0|x_1)$</p>
<p><strong>Simplified Loss</strong></p>
<figure class="eq">
$$
L_{simple}(\theta) = \mathbb E_{t,x_0,\epsilon} [||\epsilon -\epsilon_{\theta}(\sqrt{\bar \alpha_t}x_0  + \sqrt{1-\bar\alpha_t}\epsilon,t) ||^2]
$$
</figure>

<p>t = 1 ì¼ ë•Œ</p>
<p>$L_0$ ì¦‰, $-\log p_{\theta}(x_0|x_1)$ë¥¼ minimize</p>
<p>t&gt; 1ì¼ ë•Œ</p>
<p>$L_{t-1}$ì—ì„œ ì‹ ì •ë¦¬í•˜ë©´ ë‚˜ì˜¤ëŠ” coefficient $\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar\alpha_t)}$ë¥¼ ë²„ë ¤ì„œ higher noise level(higer t)ì— ì „(coefficient ê°€ ìˆì„ ë•Œ)ë³´ë‹¤ ë” í° weightë¥¼ ì£¼ê³  small tì— ëŒ€í•´ì„  ë” ì‘ì€ weightë¥¼ ì¤˜ì„œ ë” ì¢‹ì€ sample qualityë¥¼ ì–»ì—ˆë‹¤. (small tì—ì„  modelì´ ì‘ì€ ì–‘ì˜ noiseë§Œ denoiseí•˜ë„ë¡ í•™ìŠµì„ ì‹œí‚¤ê¸° ë•Œë¬¸, ê·¸ë˜ì„œ ë” ì–´ë ¤ìš´ large tì— ì§‘ì¤‘í•˜ë„ë¡ ë§Œë“¦)</p>
<p><img src="/images/diffusion_ddpm/01.jpg" alt="Untitled"></p>
<aside>
randomí•˜ê²Œ timesteps të¥¼ ë½‘ê³ , $x_0$ì™€ të¥¼ ì´ìš©í•´ $q(x_t|x_0)$ ë¡œë¶€í„° $x_t$ë¥¼ êµ¬í•¨
ì´ $x_t$ì™€ të¥¼ ìš°ë¦¬ ëª¨ë¸ì— ë„£ê³  epsilonì„ ë½‘ìŒ
ì´ epsilonê³¼ ($x_0$ì™€ ì •í™•íˆ ê°™ì€ dimensionì„ ê°–ëŠ”) noiseë¥¼ ë½‘ê³  MSE loss ë•Œë¦¬ë©´ ëœë‹¤.
</aside>

<hr>
<h2>ğŸ› ï¸ Model Architectue</h2>
<p>modelì˜ inputê³¼ outputì˜ dimensionì´ ê°™ì•„ì•¼í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„  U-Netì„ ì‚¬ìš©í–ˆë‹¤. 
U-Netì€ Residual Block, self-attention blockì´ ìˆë‹¤.</p>
<p>diffusionì˜ timestep tê°€ position embeddingì„ í•œ í›„residual blockì— ì „ë‹¬ë˜ëŠ” ì‹ìœ¼ë¡œ ëª¨ë¸ì— tê°€ ì…ë ¥ëœë‹¤.</p>
<p><strong>U-Net</strong></p>
<p><img src="/images/diffusion_ddpm/02.jpg" alt="Untitled"></p>
<p>$\epsilon_{\theta}$ <strong>model using U-Net</strong></p>
<ul>
<li>input : $(x_t, t)$</li>
<li>output :<code>ì œê±°í•´ì•¼í•  noise</code></li>
</ul>
<p><strong>Implementation Code</strong></p>
<pre><code class="language-python">class ResBlock(nn.Module):
    def __init__(self, c, t_emb_dim):
        super().__init__()
        self.time_mlp = nn.Linear(t_emb_dim, c)
        self.block    = nn.Sequential(
            nn.GroupNorm(8, c),
            nn.SiLU(),
            nn.Conv2d(c, c, 3, padding=1)
        )

    def forward(self, x, t_emb):
        h = self.block(x) + self.time_mlp(t_emb)[:, :, None, None]
        return h + x                            # residual

class UNet(nn.Module):
    def __init__(self, ch=64, t_emb_dim=256):
        super().__init__()
        self.t_embed = nn.Sequential(
            nn.Linear(1, 128), nn.SiLU(), nn.Linear(128, t_emb_dim)
        )
        self.down1 = ResBlock(ch, t_emb_dim)
        self.down2 = ResBlock(ch*2, t_emb_dim)
        self.up1   = ResBlock(ch*2, t_emb_dim)
        self.out   = nn.Conv2d(ch, 3, 1)

    def forward(self, x, t):
        t_emb = self.t_embed(t)
        h1 = self.down1(x, t_emb)
        h2 = self.down2(nn.functional.avg_pool2d(h1, 2), t_emb)
        h  = nn.functional.interpolate(h2, scale_factor=2)
        h  = self.up1(torch.cat([h, h1], 1), t_emb)
        return self.out(h)

def beta_schedule(T, start=1e-4, end=0.02):
    return torch.linspace(start, end, T)

def q_sample(x0, t, âˆšabar, âˆš1mabar, noise):
    return âˆšabar[t][:, None, None, None] * x0 + âˆš1mabar[t][:, None, None, None] * noise

def loss_step(model, x0, t, âˆšabar, âˆš1mabar):
    noise  = torch.randn_like(x0)
    xt     = q_sample(x0, t, âˆšabar, âˆš1mabar, noise)
    pred_n = model(xt, t.float().unsqueeze(-1))
    return ((noise - pred_n) ** 2).mean()
</code></pre>

    <section class="related">
        <div class="related-head">
          <h2>Related posts</h2>
          <a class="all-link" href="../../">Browse all articles&nbsp;â†’</a>
        </div>
        <div class="related-list">
      <a class="card mini" href="../../posts/Difussion_Latent Diffusion/">
        <img class="thumb" src="../../images/latentdiffusion.webp" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Difussion : Latent Diffusion</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Feb 17, 2024 Â· 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/Difussion_LM_Seq2Seq/">
        <img class="thumb" src="../..//images/Difussion_LM_Seq2Seq/thumb.png" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Difussion : Difussion LM & Seq2Seq</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Feb 26, 2023 Â· 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/diffusion_ddim/">
        <img class="thumb" src="../../images/diffusion_ddim/thumbnail.png" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Diffusion : DDIM</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Jan 31, 2023 Â· 20 min</div>
        </div>
      </a></div>
      </section> 
  </main>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
<script>
window.MathJax = {
  tex: {
    inlineMath:  [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },

  chtml: {
    linebreaks: { automatic: true, width: "match" }  // ë˜ëŠ” width: 80
  },

  options: {
    renderActions: { addMenu: [] }   // ìš°í´ë¦­ ë©”ë‰´ ì œê±°
  }
};
</script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script src="../../assets/js/main.js" defer></script>
</body></html>