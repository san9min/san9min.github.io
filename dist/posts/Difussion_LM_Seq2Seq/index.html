<!DOCTYPE html>
<html lang="ko"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Sangmin Blog | Difussion : Difussion LM & Seq2Seq</title>
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css">
  <link rel="stylesheet" href="../../assets/styles/main.css">
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <link rel="stylesheet"
     href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

</head><body>
  <main class="article">
    <h1>Difussion : Difussion LM & Seq2Seq</h1>
    <div class="meta">Feb 26, 2023 Â· 20 min read</div>
    <img class="hero" src="../..//images/Difussion_LM_Seq2Seq/thumb.png" alt="cover image">
    <h1>Diffusion LM Improves Controllable Text Generation</h1>
<h2>Diffusion LM</h2>
<p><a href="https://arxiv.org/abs/2205.14217">Diffusion-LM Improves Controllable Text Generation</a></p>
<p><a href="https://github.com/xiangli1999/diffusion-lm">https://github.com/xiangli1999/diffusion-lm</a></p>
<ul>
<li>non-autoregressive language model based on continuous diffusions</li>
<li>embedding from discrete space to continuous space</li>
<li>continuous diffusion model to discrete text</li>
<li>Transformer based</li>
</ul>
<p><code>Gaussian noise ë²¡í„°ë“¤ì˜ sequenceì—ì„œ ì‹œì‘</code>í•´ (inference) denoisingì„í•´ì„œ wrodsì— correspondingí•˜ëŠ” vectorsë“¤ì„ ì–»ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤.</p>
<p><em>a sequence of Gaussian noise vecotrs -- <code>denoise</code> â€”&gt; vectors corresponding to words</em></p>
<p>ì´ëŸ° ì‹ìœ¼ë¡œ ì§„í–‰í•˜ë©´ ì¤‘ê°„ì— hierarchyê°€ ìˆëŠ” contiuous latent variableë“¤ì´ ìƒê¸´ë‹¤. ë•ë¶„ì— ìš°ë¦¬ê°€ í•˜ë“¯ gradient ë¥¼ ë•Œë ¤ì„œ ìš°ë¦¬ê°€ í•˜ë˜ ëŒ€ë¡œ í•˜ë©´ ëœë‹¤.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/269add55-95f8-4bed-9ca2-e838de76ebb4/Untitled.png" alt="Untitled"></p>
<p>diffusion ëª¨ë¸ì´ textì— ì ìš©í•˜ê¸° í˜ë“ ì´ìœ ëŠ” diffusion modelì€ continuous domainì—ì„œ ë§ì€ ë°œì „ì„ ì´ë¤„ì™”ëŠ”ë° ë¹„í•´ textê°€ ìì²´ì ìœ¼ë¡œ dicreteí•œ íŠ¹ì„±ì„ ê°–ê³  ìˆê¸° ë•Œë¬¸. ê·¸ë˜ì„œ ì´ëŸ° standard diffusionì— textë¥¼ ì ìš©ì‹œí‚¤ê¸° ìœ„í•´ discreteí•œ textë¥¼ continuousí•œ domainìœ¼ë¡œ ì˜ mappingì„ í•´ì•¼í•œë‹¤. NLPì— word to vectorê°€ ë– ì˜¤ë¥¸ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ìœ„í•´ 2ê°€ì§€ stepì„ ì¶”ê°€ì ìœ¼ë¡œ ì ìš©í•œë‹¤.</p>
<ol>
<li><strong><code>embedding step</code></strong> : ê°™ì´ í•™ìŠµì‹œì¼œë²„ë¦¼</li>
<li><strong><code>rounding step</code></strong> with softmax</li>
</ol>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e320eedb-ae89-41f4-964c-36be65ca8e6c/Untitled.png" alt="Untitled"></p>
<p><strong>Diffusion Models for Text</strong> ëŠ” í¬ê²Œ ë‘ê°€ì§€ê°€ ìˆë‹¤ ë³¼ ìˆ˜ ìˆë‹¤.</p>
<ol>
<li><p>text diffusion models on discrete state spaces</p>
<p> discrete data(ê° í† í°)ì— corruptionì„ ì£¼ëŠ” ì‹ìœ¼ë¡œ forward process ì§„í–‰</p>
</li>
<li><p><code>continuousí•œ latent</code> variablesì„ ë½‘ì•„ì„œ ì§„í–‰</p>
</li>
</ol>
<p>ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ atuoregressiveí•˜ë„ë¡ generationì„ í•˜ëŠ” transformer ê°™ì€ ë°©ì‹ì˜  language ëª¨ë¸ë“¤ì´ ë§ë‹¤.</p>
<p>$p_{lm}(w) = p_{lm}(w_1)\Pi_{i=2}^np_{lm}(x_i|x_{&lt;i})$ â‡’ ì§€ê¸ˆê¹Œì§€ ë§Œë“¤ì–´ë‚¸ tokenë“¤ì˜ sequenceë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ tokenì„ ì˜ˆì¸¡í•˜ê³  ì´ë¥¼ eosê¹Œì§€ ë°˜ë³µí•œë‹¤. ì´ë ‡ê²Œ generationì˜ ìˆœì„œë¥¼ ê³ ì •ì‹œí‚¤ëŠ” ê²ƒì€ ëª¨ë¸ì„ ì»¨íŠ¸ë¡¤í•˜ê¸°ê°€ ì–´ë µë‹¤. </p>
<p> Plug and Play Controllable Generation ì€ ëª¨ë¸ ìì²´ëŠ” frozenì‹œí‚¤ê³  ì™¸ë¶€ì— classifierë¥¼ ë”°ë¡œë‘ëŠ” ë°©ì‹ì´ë¼ ë³´ë©´ëœë‹¤.</p>
<p>Text Generationì€ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ëª¨ë¸ì´ discrete words $w_i$ë“¤ì˜ sequence <em><strong>w</strong></em>ë¥¼ ë½‘ì•„ë‚´ëŠ” ê²ƒ $p_{lm}(w)$ ì´ê³  ì´ê²ƒì´ controllableí•˜ë‹¤ëŠ” ê²ƒì€ ê²°êµ­ conditional distribution $p(w|c)$ ì„ ì˜ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì´ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„  bayes ruleë¡œ ì‹ì„ ë‹¤ì‹œì¨ ì™¸ë¶€ì— classifier $p(c|w)$ ë¥¼ ë”°ë¡œë‘ëŠ” ë°©ì‹ì„ íƒí–ˆë‹¤.</p>
<h3>Continuous Diffusion Langauge Modeling</h3>
<ol>
<li><strong>End-To-End Training</strong></li>
</ol>
<p>ìš°ì„  ê°€ì¥ ë¨¼ì €í•´ì•¼ë  ê²ƒì€ ê°ê°ì˜ <code>ë‹¨ì–´ë“¤ì„ dì°¨ì›ì˜ ë²¡í„°ë¡œ embedding</code>ì„ ì‹œì¼œì•¼í•œë‹¤. ê·¸ë˜ì„œ d x nì˜ í…ì„œë¥¼ ì–»ëŠ”ë‹¤.</p>
<p>$EMB(w_i)$ â†’ $EMB(w) = [EMB(w_1),\cdots, EMB(w_n)] \in \mathbb R^{nd}$ </p>
<p>ì—¬ê¸°ì„œ diffusion modelì˜ parameterë“¤ê³¼ word embeddingì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” full objectiveë¥¼ ì œì‹œí•œë‹¤.( Gaussian embeddingì´ë‚˜ pre-trained word embeddingë„ ì‹¤í—˜ì„ í•´ë´¤ëŠ”ë°, Diffusion LMì— ëŒ€í•´ì„œ <code>fixed embeddingsì€ end-to-end trainingì— ë¹„í•´ suboptimal</code>ì— ë„ë‹¬í–ˆë‹¤ê³  í•œë‹¤.)</p>
<p>ì´ë¥¼ ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ $q_{\phi}(x_0|w) = N(EMB(w),\sigma_0 I)$ì´ê³  standard diffusion processë¥¼ ëŒë¦¬ê¸° ì „ì— Markov chainí•˜ë‚˜ë¥¼ ì¶”ê°€í•´ $wâ†’ x_0$ ë¡œ ê°€ê²Œ ë§Œë“ ë‹¤. ê·¸ëŸ¼ ì—­ìœ¼ë¡œ $x_0 â†’ w$ ë¡œ ê°€ëŠ” stepë„ í•„ìš”í• í…Œê³  ì´ë¥¼ <code>rounding step</code>ì´ë¼ í•˜ê³  ì‹ìœ¼ë¡œ ì“°ë©´ $p_{\theta} (w|x_0) = \Pi_{i = 1}^n p_\theta (w_i|x_i)$ ì´ê³  ê° ë‹¨ì–´ì˜ ëŒ€ì‘í•˜ëŠ” ë²¡í„°ë“¤ì˜ <code>softmaxë¥¼ ì´ìš©</code>í•´ ì ì ˆí•œ í† í°ì„ ë³µêµ¬í•˜ëŠ” ì‹ì´ë‹¤.</p>
<p>ì´ ê³¼ì •ì˜ objectiveë¥¼ ì‹ìœ¼ë¡œì“°ê³  simple formìœ¼ë¡œ ì“°ë©´</p>
<p>$$
L^{e2e}<em>{simple}(w) = \mathbb E</em>{q_{\phi}(x_{0:T}|w)} [L_{simple}(x_0) + ||EMB(w) - \mu_\theta(x_1,1)||^2 - \log p_\theta(w|x_0)] 
$$</p>
<p>ì´ ëœë‹¤.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e6f4dc33-e9f4-4f1c-9c52-ac01d7c0a573/Untitled.png" alt="Untitled"></p>
<ol>
<li><strong>Reducing Rounding Errors</strong></li>
</ol>
<p>$x_0 â†’ discrete ;;text$ ì€ $x_0$ê°€ wordë“¤ì˜ embeddingì— ì •í™•íˆ ìˆë‹¤ë©´ softmaxë¥¼ ì‚¬ìš©í•´ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í°ìœ¼ë¡œ ë³µêµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° ì‹¤ì œ í•´ë³´ë‹ˆ ì˜ ì•ˆëê³ , ì´ìœ ëŠ” $L_{simple}(x_0)$ê°€  $x_0$ì™€ wordì‚¬ì´ì˜ ê´€ê³„ë¥¼ tê°€ 0ê·¼ì²˜ì¼ë•Œë§Œ ê´€ì‹¬ì„ ê°€ì ¸ì„œ $x_0$ì˜ structureë¥¼ ì˜ ëª¨ë¸ë§í•˜ëŠ”ë° ì¶©ë¶„í•˜ì§€ ì•Šì•„ì„œ ì´ë‹¤.ê·¸ë˜ì„œ ëª¨ë“  tì—ëŒ€í•´ì„œ  $x_0$ë¥¼ modelingí•  ìˆ˜ ìˆë„ë¡ lossë¥¼ ìˆ˜ì •í–ˆë‹¤.</p>
<p>$$
L_{x_0 simple}^{e2e} (x_0) = \sum_{t=1}^T \mathbb E <em>{x_t}||f</em>{\theta}(x_t,t)-x_0||^2
$$</p>
<p><code>$f_\theta(x,t)$  predicts $x_0$ directly</code></p>
<p>ê·¸ë¦¬ê³  decodingí•  ë•Œë„ ì´ë¥¼ ì‚¬ìš© ( clamping trick : fê°€ ì˜ˆì¸¡í•œ $x_0$ë¥¼ $x_0$ì™€ ê°€ì¥ ê°€ê¹Œìš´ word embedding sequenceë¡œ mapping, ì¦‰ w_pred)</p>
<p>$x_{t-1} = \sqrt {\bar \alpha}f_\theta (x_t,t) + \sqrt{(1-\bar \alpha)}\epsilon$ â€” <code>clamping trick</code> -â†’ $x_{t-1} = \sqrt {\bar \alpha}Clamp(f_\theta (x_t,t)) + \sqrt{(1-\bar \alpha)}\epsilon$</p>
<h3>Decoding and Controllable Generation with Diffusion-LM</h3>
<ol>
<li>Controllable Text <strong>Generation</strong></li>
</ol>
<p>$$
\nabla <em>{x</em>{t-1}} \log p(x_{t-1}|x_-t,c) = \nabla <em>{x</em>{t-1}} \log p (x_{t-1}|x_t) + \nabla <em>{x</em>{t-1}} \log p(c|x_{t-1})
$$</p>
<p>ì¦‰ ì™¸ë¶€ì— classifierë¥¼ ë”°ë¡œ ë‘”ì±„ë¡œ í•™ìŠµí•˜ê² ë‹¨ ì´ì•¼ê¸°ì´ê³ , ê°ê°ì˜ stepë§ˆë‹¤ gradient stepì„ ë°Ÿê² ë‹¨ ì–˜ê¸°. performanceì™€ speedë¥¼ ìœ„í•´ ë‘ê°€ì§€ë¥¼ ìˆ˜ì •í–ˆëŠ”ë°</p>
<ol>
<li><p>Fluency Regularization</p>
<p> $\lambda$ë¼ëŠ” hyperparamì„ ë„ì…í•´ fluencyì™€ controlê°„ì˜ tradeoffë¥¼ ì¡°ì ˆ</p>
</li>
<li><p>multiple gradient steps</p>
<p> control qualityë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í•œë²ˆì˜ diffusion stepë‹¹ 3ë²ˆì˜ Adagrad updateë¥¼ ì ìš©</p>
</li>
<li><p>Minimum Bayes Risk <strong>Decoding</strong></p>
<p> negative BELU (Bilingual Evaluation Understudy)ë¥¼ ì´ìš©í•´ ê°€ì¥ ë‚®ì€ BELU scoreë¥¼ ê¸°ë¡í•œ sampleì„ íƒí•œë‹¤.</p>
</li>
</ol>
<hr>
<h1>DiffuSeq <strong>: Sequence to Sequence Text Generation with Diffusion Models</strong></h1>
<h2>DiffuSeq</h2>
<p><a href="https://arxiv.org/abs/2210.08933">DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models</a></p>
<p><a href="https://github.com/Shark-NLP/DiffuSeq">https://github.com/Shark-NLP/DiffuSeq</a></p>
<ul>
<li>Conditioning</li>
<li>Classifier free guidance</li>
<li>Seq2Seq</li>
</ul>
<p>Transformerë¥¼ modelë¡œ ì‚¬ìš© â‡’ <code>Attention</code>ì„ í†µí•´ classifier free guide ë°©ì‹ì„ ì œì‹œ</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/35ff4bdc-8eee-4374-8755-3694e88498df/Untitled.png" alt="Untitled"></p>
<p>Notation)</p>
<p><strong>source sequence $w^x =[w^x_1,w^x_2,...w_m^x]$</strong></p>
<p><strong>target sequence $w^y = [w_1^y,w_2^y,...,w_n^y]$</strong></p>
<p>ëª¨ë¸ì€ source sequenceì˜ conditioningìœ¼ë¡œ target seqeunceë¥¼ ìƒì„±</p>
<h3>Forward Process with Partially Noising</h3>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/314b0108-6a1c-42f1-9666-7a285cbadda5/Untitled.png" alt="Untitled"></p>
<p>Diffusion-LMì—ì„œ ì²˜ëŸ¼ $EMB(w)$ì„ ì´ìš©í•´ discrete text w ë¥¼ continuous spaceë¡œ mapping ( <code>embedding</code>, <code>concatenation</code>)</p>
<ul>
<li>sequence $w^x$ ì™€ $w^y$ì˜ pairê°€ ì£¼ì–´ì§€ê³ , DiffuSeqëŠ” unified feature spaceë¥¼ í•™ìŠµí•¨</li>
</ul>
<p>original Markov chainì— $q_{\phi}(z_0|w^{x\bigoplus y})= N(EMB(w^{x\bigoplus y}),\beta_0I)$ë¥¼ ì¶”ê°€í•œ í›„ diffusion process ì§„í–‰</p>
<p>ê·¸ëŸ°ë° ì „ì²´ $z_t$ ($x_t$ì™€ $y_t$ ë‘˜ë‹¤) ê°€ ì•„ë‹Œ íƒ€ê²Ÿ ì†ŒìŠ¤ì¸ $y_t$ì—ë§Œ noiseë¥¼ ì¶”ê°€í•˜ëŠ” ì‹ìœ¼ë¡œ forward process â†’ <code>partially noising</code></p>
<aside>
ğŸª§ implementation ) $x_t$ê¹Œì§€ ê°™ì´ corruput ì‹œí‚¤ê³  ì´ë¥¼ $x_0$ë¡œ replace
trainingê³¼ inferenceí•  ë•Œ ë‘˜ë‹¤ ì´ ë°©ì‹ì„ ì‚¬ìš©

</aside>

<h3>Reverse Process with Conditional Denoising</h3>
<p><strong>Goal :</strong> $z_t$ â€” denoise â€”&gt; $z_0$</p>
<p>$p_{\theta}(z_{0:T}) = p(z_T)\Pi_{t=1}^T p_\theta(z_{t-1}|z_t)$ </p>
<p><strong>model :</strong> $f_\theta (z_t,t)$ ; Transformer â‡’ $x_t$ì™€ $y_t$ ê°„ì˜ semantic relationì„ í•™ìŠµ</p>
<p>$p_{\theta}(z_{t-1}|z_t) = N(z_{t-1};\mu_\theta(z_t,t),\sigma_\theta(z_t,t))$</p>
<p><strong>Loss</strong></p>
<p>$$
min_\theta[\sum_{t=2}^T||y_0-\tilde f_\theta(z_t,t)||^2 + ||EMB(w^y)-\tilde f_{\theta}(z_1,1)||^2 + R(||z_0||^2)]
$$</p>
<p>í‹¸ë‹¤ëŠ” f ê°€ ì¶”ë¡ í•œ zì¤‘ yë§Œ ì“°ê² ë‹¤ëŠ” í‘œê¸°</p>
<p><strong>First term</strong></p>
<p>$y_0$ì— ê´€í•´ì„œ lossë¥¼ ê³„ì‚°í–ˆê¸°ì— $x_0$ì™€ ë¬´ê´€í•´ ë³´ì´ëŠ”ë° transformerì˜ attention mechanism ë•Œë¬¸ì— $x_0$ë„ ê³ ë ¤ëœ termì„</p>
<p><strong>Third term</strong></p>
<p>embedding learningì„ regularize</p>
<aside>
ğŸ’¡ embedding functionì„ sourceì™€ targetì´ ê³µìœ í•˜ë‹ˆê¹Œ ë‘ê°œì˜ feature spaceë¥¼ í•¨ê»˜ ë°°ì›€

</aside>

<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/238b4eab-8c28-4b55-bd3c-d14d78cc6ce3/Untitled.png" alt="Untitled"></p>
<p>decodingí•  ë•Œ Diffusion LM ì²˜ëŸ¼ BELU score ì‚¬ìš©</p>

    <section class="related">
        <div class="related-head">
          <h2>Related posts</h2>
          <a class="all-link" href="../../">Browse all articles&nbsp;â†’</a>
        </div>
        <div class="related-list">
      <a class="card mini" href="../../posts/Difussion_Latent Diffusion/">
        <img class="thumb" src="../../images/latentdiffusion.webp" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Difussion : Latent Diffusion</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Feb 17, 2024 Â· 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/diffusion_ddim/">
        <img class="thumb" src="../../images/diffusion_ddim/thumbnail.png" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Diffusion : DDIM</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Jan 31, 2023 Â· 20 min</div>
        </div>
      </a>
      <a class="card mini" href="../../posts/diffusion_ddpm/">
        <img class="thumb" src="../..//images/diffusion_ddpm/thumbnail.jpg" alt="">
        <div class="card-body">
        
          <h3 class="card-title">Diffusion : DDPM</h3>
          <span class="arrow">â†—</span>
          <div class="meta">Jan 22, 2023 Â· 20 min</div>
        </div>
      </a></div>
      </section> 
  </main>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
<script>
window.MathJax = {
  tex: {
    inlineMath:  [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
  },

  chtml: {
    linebreaks: { automatic: true, width: "match" }  // ë˜ëŠ” width: 80
  },

  options: {
    renderActions: { addMenu: [] }   // ìš°í´ë¦­ ë©”ë‰´ ì œê±°
  }
};
</script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script src="../../assets/js/main.js" defer></script>
</body></html>